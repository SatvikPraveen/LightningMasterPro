{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35facd8e",
   "metadata": {},
   "source": [
    "# File Location: notebooks/06_advanced_mechanics/15_curriculum_batchloop.ipynb\n",
    "\n",
    "# Curriculum Learning with Custom Batch Loops\n",
    "\n",
    "This notebook explores curriculum learning implementation using custom batch loops in PyTorch Lightning. We'll learn to progressively increase training difficulty, implement dynamic batch sampling, and create adaptive learning strategies.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand curriculum learning concepts and benefits\n",
    "- Implement custom batch loops for progressive training\n",
    "- Build difficulty-aware data sampling strategies\n",
    "- Create adaptive curriculum schedulers\n",
    "- Monitor and visualize curriculum progression\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, Sampler\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any, Optional, Iterator\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from pytorch_lightning.loops.base import Loop\n",
    "from pytorch_lightning.loops import TrainingEpochLoop\n",
    "from pytorch_lightning.trainer.states import TrainerFn\n",
    "import math\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Lightning version: {pl.__version__}\")\n",
    "```\n",
    "\n",
    "## 1. Curriculum Learning Fundamentals\n",
    "\n",
    "```python\n",
    "class CurriculumLearningConcepts:\n",
    "    \"\"\"\n",
    "    Curriculum Learning Concepts:\n",
    "    \n",
    "    1. Progressive Difficulty: Start with easy examples, gradually increase difficulty\n",
    "    2. Sample Ordering: Strategic ordering of training examples\n",
    "    3. Difficulty Metrics: Automatic or manual difficulty assessment\n",
    "    4. Pacing Functions: Control the rate of curriculum progression\n",
    "    5. Adaptive Strategies: Dynamic adjustment based on model performance\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def explain_benefits():\n",
    "        benefits = {\n",
    "            \"Faster Convergence\": \"Models learn basic patterns first, then complex ones\",\n",
    "            \"Better Generalization\": \"Systematic learning reduces overfitting\",\n",
    "            \"Improved Stability\": \"Gradual complexity prevents training collapse\", \n",
    "            \"Transfer Learning\": \"Foundation knowledge transfers to harder tasks\",\n",
    "            \"Resource Efficiency\": \"Smarter training requires fewer epochs\"\n",
    "        }\n",
    "        \n",
    "        print(\"Curriculum Learning Benefits:\")\n",
    "        for benefit, explanation in benefits.items():\n",
    "            print(f\"  {benefit}: {explanation}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def common_strategies():\n",
    "        strategies = {\n",
    "            \"Self-Paced Learning\": \"Automatically select samples based on loss\",\n",
    "            \"Pre-defined Curriculum\": \"Manual difficulty ordering\",\n",
    "            \"Teacher-Student\": \"Use teacher model to guide sample selection\",\n",
    "            \"Anti-Curriculum\": \"Start with hard examples (contrarian approach)\",\n",
    "            \"Mixed Curriculum\": \"Combine multiple curriculum strategies\"\n",
    "        }\n",
    "        \n",
    "        print(\"\\nCommon Curriculum Strategies:\")\n",
    "        for strategy, description in strategies.items():\n",
    "            print(f\"  {strategy}: {description}\")\n",
    "\n",
    "CurriculumLearningConcepts.explain_benefits()\n",
    "CurriculumLearningConcepts.common_strategies()\n",
    "```\n",
    "\n",
    "## 2. Difficulty-Aware Dataset\n",
    "\n",
    "```python\n",
    "class DifficultyAwareMNIST(Dataset):\n",
    "    \"\"\"MNIST dataset with difficulty annotations\"\"\"\n",
    "    \n",
    "    def __init__(self, train=True, transform=None, difficulty_type='manual'):\n",
    "        self.dataset = torchvision.datasets.MNIST('./data', train=train, download=True, transform=transform)\n",
    "        self.difficulty_type = difficulty_type\n",
    "        self.difficulty_scores = self._compute_difficulty_scores()\n",
    "        \n",
    "        # Create difficulty-based groups\n",
    "        self.easy_indices = []\n",
    "        self.medium_indices = []\n",
    "        self.hard_indices = []\n",
    "        self._create_difficulty_groups()\n",
    "    \n",
    "    def _compute_difficulty_scores(self):\n",
    "        \"\"\"Compute difficulty scores for each sample\"\"\"\n",
    "        scores = np.zeros(len(self.dataset))\n",
    "        \n",
    "        if self.difficulty_type == 'manual':\n",
    "            # Manual difficulty based on digit characteristics\n",
    "            for idx in range(len(self.dataset)):\n",
    "                _, label = self.dataset[idx]\n",
    "                \n",
    "                # Define difficulty based on digit complexity\n",
    "                difficulty_map = {\n",
    "                    0: 0.1, 1: 0.2, 2: 0.7, 3: 0.8, 4: 0.6,\n",
    "                    5: 0.9, 6: 0.5, 7: 0.3, 8: 1.0, 9: 0.4\n",
    "                }\n",
    "                scores[idx] = difficulty_map[label] + np.random.normal(0, 0.1)\n",
    "        \n",
    "        elif self.difficulty_type == 'variance':\n",
    "            # Difficulty based on pixel variance\n",
    "            for idx in range(len(self.dataset)):\n",
    "                image, _ = self.dataset[idx]\n",
    "                if hasattr(image, 'numpy'):\n",
    "                    image_array = image.numpy()\n",
    "                else:\n",
    "                    image_array = np.array(image)\n",
    "                scores[idx] = np.var(image_array.flatten())\n",
    "        \n",
    "        elif self.difficulty_type == 'edge_density':\n",
    "            # Difficulty based on edge density\n",
    "            from scipy import ndimage\n",
    "            for idx in range(len(self.dataset)):\n",
    "                image, _ = self.dataset[idx]\n",
    "                if hasattr(image, 'numpy'):\n",
    "                    image_array = image.numpy()\n",
    "                else:\n",
    "                    image_array = np.array(image)\n",
    "                \n",
    "                # Apply edge detection\n",
    "                edges = ndimage.sobel(image_array.squeeze())\n",
    "                scores[idx] = np.sum(np.abs(edges)) / (28 * 28)\n",
    "        \n",
    "        # Normalize scores to [0, 1]\n",
    "        scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)\n",
    "        return scores\n",
    "    \n",
    "    def _create_difficulty_groups(self):\n",
    "        \"\"\"Group samples by difficulty\"\"\"\n",
    "        sorted_indices = np.argsort(self.difficulty_scores)\n",
    "        total_samples = len(sorted_indices)\n",
    "        \n",
    "        # Split into thirds\n",
    "        easy_cutoff = total_samples // 3\n",
    "        medium_cutoff = 2 * total_samples // 3\n",
    "        \n",
    "        self.easy_indices = sorted_indices[:easy_cutoff].tolist()\n",
    "        self.medium_indices = sorted_indices[easy_cutoff:medium_cutoff].tolist()\n",
    "        self.hard_indices = sorted_indices[medium_cutoff:].tolist()\n",
    "        \n",
    "        print(f\"Difficulty groups created:\")\n",
    "        print(f\"  Easy: {len(self.easy_indices)} samples\")\n",
    "        print(f\"  Medium: {len(self.medium_indices)} samples\")\n",
    "        print(f\"  Hard: {len(self.hard_indices)} samples\")\n",
    "    \n",
    "    def get_samples_by_difficulty(self, difficulty_level):\n",
    "        \"\"\"Get sample indices by difficulty level\"\"\"\n",
    "        if difficulty_level == 'easy':\n",
    "            return self.easy_indices\n",
    "        elif difficulty_level == 'medium':\n",
    "            return self.medium_indices\n",
    "        elif difficulty_level == 'hard':\n",
    "            return self.hard_indices\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown difficulty level: {difficulty_level}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        difficulty = self.difficulty_scores[idx]\n",
    "        return image, label, difficulty\n",
    "\n",
    "# Create difficulty-aware datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = DifficultyAwareMNIST(train=True, transform=transform, difficulty_type='manual')\n",
    "test_dataset = DifficultyAwareMNIST(train=False, transform=transform, difficulty_type='manual')\n",
    "```\n",
    "\n",
    "## 3. Curriculum Sampler\n",
    "\n",
    "```python\n",
    "class CurriculumSampler(Sampler):\n",
    "    \"\"\"Custom sampler implementing curriculum learning strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, strategy='linear', initial_ratio=0.3, final_ratio=1.0):\n",
    "        self.dataset = dataset\n",
    "        self.strategy = strategy\n",
    "        self.initial_ratio = initial_ratio\n",
    "        self.final_ratio = final_ratio\n",
    "        \n",
    "        # Current curriculum state\n",
    "        self.current_epoch = 0\n",
    "        self.total_epochs = 100  # Will be set by trainer\n",
    "        \n",
    "        # Sample pools\n",
    "        self.easy_indices = dataset.easy_indices\n",
    "        self.medium_indices = dataset.medium_indices\n",
    "        self.hard_indices = dataset.hard_indices\n",
    "        self.all_indices = list(range(len(dataset)))\n",
    "        \n",
    "    def set_epoch(self, epoch, total_epochs=None):\n",
    "        \"\"\"Update curriculum based on current epoch\"\"\"\n",
    "        self.current_epoch = epoch\n",
    "        if total_epochs:\n",
    "            self.total_epochs = total_epochs\n",
    "    \n",
    "    def _compute_curriculum_ratio(self):\n",
    "        \"\"\"Compute current curriculum ratio based on strategy\"\"\"\n",
    "        progress = min(self.current_epoch / self.total_epochs, 1.0)\n",
    "        \n",
    "        if self.strategy == 'linear':\n",
    "            ratio = self.initial_ratio + progress * (self.final_ratio - self.initial_ratio)\n",
    "        elif self.strategy == 'exponential':\n",
    "            ratio = self.initial_ratio * (self.final_ratio / self.initial_ratio) ** progress\n",
    "        elif self.strategy == 'cosine':\n",
    "            ratio = self.initial_ratio + 0.5 * (self.final_ratio - self.initial_ratio) * (1 + math.cos(math.pi * (1 - progress)))\n",
    "        elif self.strategy == 'step':\n",
    "            # Step function: easy -> medium -> hard at fixed intervals\n",
    "            if progress < 0.33:\n",
    "                ratio = 0.3\n",
    "            elif progress < 0.66:\n",
    "                ratio = 0.6\n",
    "            else:\n",
    "                ratio = 1.0\n",
    "        else:\n",
    "            ratio = self.final_ratio  # Default to all samples\n",
    "        \n",
    "        return min(ratio, self.final_ratio)\n",
    "    \n",
    "    def _select_samples(self, ratio):\n",
    "        \"\"\"Select samples based on curriculum ratio\"\"\"\n",
    "        total_samples = len(self.all_indices)\n",
    "        num_samples = int(total_samples * ratio)\n",
    "        \n",
    "        # Determine composition based on ratio\n",
    "        if ratio <= 0.33:\n",
    "            # Early stage: mostly easy samples\n",
    "            selected = random.sample(self.easy_indices, min(num_samples, len(self.easy_indices)))\n",
    "        elif ratio <= 0.66:\n",
    "            # Middle stage: easy + medium samples\n",
    "            num_easy = len(self.easy_indices)\n",
    "            num_medium = min(num_samples - num_easy, len(self.medium_indices))\n",
    "            selected = self.easy_indices + random.sample(self.medium_indices, num_medium)\n",
    "        else:\n",
    "            # Late stage: all samples with preference for harder ones\n",
    "            num_easy = len(self.easy_indices)\n",
    "            num_medium = len(self.medium_indices)\n",
    "            num_hard = min(num_samples - num_easy - num_medium, len(self.hard_indices))\n",
    "            \n",
    "            selected = (self.easy_indices + self.medium_indices + \n",
    "                       random.sample(self.hard_indices, max(0, num_hard)))\n",
    "        \n",
    "        # Ensure we have exactly num_samples\n",
    "        if len(selected) < num_samples:\n",
    "            remaining = num_samples - len(selected)\n",
    "            available = [idx for idx in self.all_indices if idx not in selected]\n",
    "            selected.extend(random.sample(available, min(remaining, len(available))))\n",
    "        \n",
    "        return selected[:num_samples]\n",
    "    \n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        # Compute current curriculum ratio\n",
    "        ratio = self._compute_curriculum_ratio()\n",
    "        \n",
    "        # Select samples based on curriculum\n",
    "        selected_indices = self._select_samples(ratio)\n",
    "        \n",
    "        # Shuffle selected samples\n",
    "        random.shuffle(selected_indices)\n",
    "        \n",
    "        return iter(selected_indices)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        ratio = self._compute_curriculum_ratio()\n",
    "        return int(len(self.all_indices) * ratio)\n",
    "\n",
    "print(\"Curriculum sampler implementation complete!\")\n",
    "```\n",
    "\n",
    "## 4. Custom Batch Loop for Curriculum Learning\n",
    "\n",
    "```python\n",
    "class CurriculumBatchLoop(Loop):\n",
    "    \"\"\"Custom batch loop with curriculum learning capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, curriculum_strategy='adaptive', difficulty_threshold=0.7):\n",
    "        super().__init__()\n",
    "        self.curriculum_strategy = curriculum_strategy\n",
    "        self.difficulty_threshold = difficulty_threshold\n",
    "        \n",
    "        # Curriculum state\n",
    "        self.current_difficulty_ratio = 0.3\n",
    "        self.batch_losses = []\n",
    "        self.epoch_difficulty_progression = []\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.recent_losses = []\n",
    "        self.loss_window_size = 100\n",
    "        \n",
    "    @property\n",
    "    def done(self) -> bool:\n",
    "        \"\"\"Check if all batches are processed\"\"\"\n",
    "        return not hasattr(self, 'dataloader_iter') or self.current_batch >= self.total_batches\n",
    "    \n",
    "    def setup(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Setup the curriculum batch loop\"\"\"\n",
    "        # Get the training dataloader\n",
    "        if hasattr(self.trainer, 'train_dataloader'):\n",
    "            self.dataloader = self.trainer.train_dataloader()\n",
    "        else:\n",
    "            raise ValueError(\"No training dataloader found\")\n",
    "        \n",
    "        self.total_batches = len(self.dataloader)\n",
    "        self.current_batch = 0\n",
    "        \n",
    "        # Update curriculum sampler if available\n",
    "        if hasattr(self.dataloader.sampler, 'set_epoch'):\n",
    "            self.dataloader.sampler.set_epoch(\n",
    "                self.trainer.current_epoch, \n",
    "                self.trainer.max_epochs\n",
    "            )\n",
    "        \n",
    "        self.dataloader_iter = iter(self.dataloader)\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset loop state\"\"\"\n",
    "        self.current_batch = 0\n",
    "        self.batch_losses = []\n",
    "        if hasattr(self, 'dataloader_iter'):\n",
    "            del self.dataloader_iter\n",
    "    \n",
    "    def advance(self) -> None:\n",
    "        \"\"\"Process one batch with curriculum considerations\"\"\"\n",
    "        try:\n",
    "            # Get next batch\n",
    "            batch = next(self.dataloader_iter)\n",
    "            \n",
    "            # Apply curriculum filtering if needed\n",
    "            if self.curriculum_strategy == 'adaptive':\n",
    "                batch = self._apply_adaptive_curriculum(batch)\n",
    "            \n",
    "            # Standard training step\n",
    "            loss = self._run_training_step(batch)\n",
    "            \n",
    "            # Update curriculum based on performance\n",
    "            self._update_curriculum_state(loss)\n",
    "            \n",
    "            self.current_batch += 1\n",
    "            \n",
    "        except StopIteration:\n",
    "            # End of epoch\n",
    "            pass\n",
    "    \n",
    "    def _apply_adaptive_curriculum(self, batch):\n",
    "        \"\"\"Apply adaptive curriculum filtering to batch\"\"\"\n",
    "        if len(batch) == 3:  # Has difficulty scores\n",
    "            images, labels, difficulties = batch\n",
    "            \n",
    "            # Filter based on current difficulty threshold\n",
    "            mask = difficulties <= self.current_difficulty_ratio\n",
    "            \n",
    "            if mask.sum() > 0:  # Ensure we have samples\n",
    "                filtered_images = images[mask]\n",
    "                filtered_labels = labels[mask]\n",
    "                return (filtered_images, filtered_labels)\n",
    "            else:\n",
    "                # If no samples pass filter, use easiest samples\n",
    "                num_samples = max(1, len(images) // 4)\n",
    "                easiest_indices = torch.topk(difficulties, num_samples, largest=False)[1]\n",
    "                return (images[easiest_indices], labels[easiest_indices])\n",
    "        \n",
    "        return batch[:2]  # Return images and labels only\n",
    "    \n",
    "    def _run_training_step(self, batch):\n",
    "        \"\"\"Execute training step and return loss\"\"\"\n",
    "        # Standard Lightning training step\n",
    "        loss = self.trainer.lightning_module.training_step(batch, self.current_batch)\n",
    "        \n",
    "        # Manual optimization if needed\n",
    "        if not self.trainer.lightning_module.automatic_optimization:\n",
    "            self.trainer.lightning_module.manual_backward(loss)\n",
    "            optimizer = self.trainer.optimizers[0]\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        self.batch_losses.append(loss.item() if hasattr(loss, 'item') else loss)\n",
    "        return loss\n",
    "    \n",
    "    def _update_curriculum_state(self, loss):\n",
    "        \"\"\"Update curriculum state based on current performance\"\"\"\n",
    "        current_loss = loss.item() if hasattr(loss, 'item') else loss\n",
    "        self.recent_losses.append(current_loss)\n",
    "        \n",
    "        # Keep only recent losses\n",
    "        if len(self.recent_losses) > self.loss_window_size:\n",
    "            self.recent_losses = self.recent_losses[-self.loss_window_size:]\n",
    "        \n",
    "        if self.curriculum_strategy == 'adaptive' and len(self.recent_losses) >= 10:\n",
    "            # Adaptive curriculum adjustment\n",
    "            recent_avg_loss = np.mean(self.recent_losses[-10:])\n",
    "            overall_avg_loss = np.mean(self.recent_losses)\n",
    "            \n",
    "            # If recent performance is good, increase difficulty\n",
    "            if recent_avg_loss < overall_avg_loss * 0.9:\n",
    "                self.current_difficulty_ratio = min(1.0, self.current_difficulty_ratio + 0.01)\n",
    "            # If performance is poor, decrease difficulty\n",
    "            elif recent_avg_loss > overall_avg_loss * 1.1:\n",
    "                self.current_difficulty_ratio = max(0.1, self.current_difficulty_ratio - 0.005)\n",
    "    \n",
    "    def on_run_end(self) -> None:\n",
    "        \"\"\"Called at the end of epoch\"\"\"\n",
    "        if self.batch_losses:\n",
    "            avg_loss = np.mean(self.batch_losses)\n",
    "            self.epoch_difficulty_progression.append({\n",
    "                'epoch': self.trainer.current_epoch,\n",
    "                'avg_loss': avg_loss,\n",
    "                'difficulty_ratio': self.current_difficulty_ratio,\n",
    "                'batches_processed': len(self.batch_losses)\n",
    "            })\n",
    "            \n",
    "            print(f\"Epoch {self.trainer.current_epoch}: \"\n",
    "                  f\"Avg Loss = {avg_loss:.4f}, \"\n",
    "                  f\"Difficulty Ratio = {self.current_difficulty_ratio:.3f}\")\n",
    "\n",
    "print(\"Custom curriculum batch loop implementation complete!\")\n",
    "```\n",
    "\n",
    "## 5. Curriculum-Aware Lightning Module\n",
    "\n",
    "```python\n",
    "class CurriculumLearningModel(pl.LightningModule):\n",
    "    \"\"\"Lightning module with curriculum learning capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, learning_rate=0.001, curriculum_strategy='linear'):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Model architecture\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='none')  # Per-sample loss\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_acc = pl.metrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = pl.metrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "        # Curriculum tracking\n",
    "        self.curriculum_strategy = curriculum_strategy\n",
    "        self.sample_difficulties = []\n",
    "        self.sample_losses = []\n",
    "        self.curriculum_progress = []\n",
    "        \n",
    "        # Custom batch loop\n",
    "        self.custom_batch_loop = CurriculumBatchLoop(curriculum_strategy=curriculum_strategy)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Handle both 2-tuple and 3-tuple batches\n",
    "        if len(batch) == 3:\n",
    "            x, y, difficulties = batch\n",
    "            self.sample_difficulties.extend(difficulties.cpu().numpy())\n",
    "        else:\n",
    "            x, y = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = self(x)\n",
    "        \n",
    "        # Compute per-sample losses\n",
    "        losses = self.criterion(logits, y)\n",
    "        \n",
    "        # Store sample losses for curriculum analysis\n",
    "        self.sample_losses.extend(losses.detach().cpu().numpy())\n",
    "        \n",
    "        # Mean loss for optimization\n",
    "        loss = losses.mean()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        self.train_acc(logits, y)\n",
    "        \n",
    "        # Logging\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Log curriculum-specific metrics\n",
    "        if len(batch) == 3:\n",
    "            avg_difficulty = difficulties.mean()\n",
    "            self.log('avg_batch_difficulty', avg_difficulty, on_step=True, on_epoch=False)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if len(batch) == 3:\n",
    "            x, y, _ = batch\n",
    "        else:\n",
    "            x, y = batch\n",
    "        \n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y).mean()\n",
    "        \n",
    "        self.val_acc(logits, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"Analyze curriculum progress at epoch end\"\"\"\n",
    "        if self.sample_difficulties and self.sample_losses:\n",
    "            # Compute correlation between difficulty and loss\n",
    "            correlation = np.corrcoef(self.sample_difficulties, self.sample_losses)[0, 1]\n",
    "            \n",
    "            # Track curriculum progress\n",
    "            progress = {\n",
    "                'epoch': self.current_epoch,\n",
    "                'avg_difficulty': np.mean(self.sample_difficulties),\n",
    "                'avg_loss': np.mean(self.sample_losses),\n",
    "                'difficulty_loss_correlation': correlation,\n",
    "                'samples_seen': len(self.sample_difficulties)\n",
    "            }\n",
    "            \n",
    "            self.curriculum_progress.append(progress)\n",
    "            self.log('difficulty_loss_correlation', correlation, on_epoch=True)\n",
    "            self.log('avg_sample_difficulty', np.mean(self.sample_difficulties), on_epoch=True)\n",
    "            \n",
    "            # Reset for next epoch\n",
    "            self.sample_difficulties = []\n",
    "            self.sample_losses = []\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_curriculum_summary(self):\n",
    "        \"\"\"Get summary of curriculum learning progress\"\"\"\n",
    "        if not self.curriculum_progress:\n",
    "            return \"No curriculum progress recorded\"\n",
    "        \n",
    "        summary = [\"Curriculum Learning Summary:\", \"=\" * 40]\n",
    "        \n",
    "        for progress in self.curriculum_progress[-5:]:  # Last 5 epochs\n",
    "            epoch = progress['epoch']\n",
    "            avg_diff = progress['avg_difficulty']\n",
    "            avg_loss = progress['avg_loss']\n",
    "            correlation = progress['difficulty_loss_correlation']\n",
    "            \n",
    "            summary.append(f\"Epoch {epoch:2d}: \"\n",
    "                          f\"Difficulty={avg_diff:.3f}, \"\n",
    "                          f\"Loss={avg_loss:.4f}, \"\n",
    "                          f\"Correlation={correlation:.3f}\")\n",
    "        \n",
    "        return \"\\n\".join(summary)\n",
    "\n",
    "# Initialize model\n",
    "model = CurriculumLearningModel(num_classes=10, learning_rate=0.001, curriculum_strategy='adaptive')\n",
    "```\n",
    "\n",
    "## 6. Curriculum Data Module\n",
    "\n",
    "```python\n",
    "class CurriculumDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Data module with curriculum learning support\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size=64, curriculum_strategy='linear', num_workers=4):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.curriculum_strategy = curriculum_strategy\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        # Transform\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        \n",
    "        # Curriculum sampler\n",
    "        self.curriculum_sampler = None\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            # Create difficulty-aware datasets\n",
    "            self.train_dataset = DifficultyAwareMNIST(\n",
    "                train=True, \n",
    "                transform=self.transform, \n",
    "                difficulty_type='manual'\n",
    "            )\n",
    "            self.val_dataset = DifficultyAwareMNIST(\n",
    "                train=False, \n",
    "                transform=self.transform, \n",
    "                difficulty_type='manual'\n",
    "            )\n",
    "            \n",
    "            # Create curriculum sampler\n",
    "            self.curriculum_sampler = CurriculumSampler(\n",
    "                self.train_dataset,\n",
    "                strategy=self.curriculum_strategy,\n",
    "                initial_ratio=0.3,\n",
    "                final_ratio=1.0\n",
    "            )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.curriculum_sampler,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def update_curriculum_epoch(self, epoch, total_epochs):\n",
    "        \"\"\"Update curriculum sampler for new epoch\"\"\"\n",
    "        if self.curriculum_sampler:\n",
    "            self.curriculum_sampler.set_epoch(epoch, total_epochs)\n",
    "\n",
    "# Initialize data module\n",
    "data_module = CurriculumDataModule(\n",
    "    batch_size=64, \n",
    "    curriculum_strategy='linear', \n",
    "    num_workers=4\n",
    ")\n",
    "```\n",
    "\n",
    "## 7. Training with Curriculum Learning\n",
    "\n",
    "```python\n",
    "# Custom trainer with curriculum support\n",
    "class CurriculumTrainer(pl.Trainer):\n",
    "    \"\"\"Trainer with integrated curriculum learning\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, model, datamodule=None, *args, **kwargs):\n",
    "        \"\"\"Override fit to update curriculum each epoch\"\"\"\n",
    "        # Store original training_epoch_end\n",
    "        original_training_epoch_end = model.training_epoch_end\n",
    "        \n",
    "        def curriculum_training_epoch_end(outputs):\n",
    "            # Update curriculum for next epoch\n",
    "            if datamodule and hasattr(datamodule, 'update_curriculum_epoch'):\n",
    "                datamodule.update_curriculum_epoch(\n",
    "                    self.current_epoch + 1, \n",
    "                    self.max_epochs\n",
    "                )\n",
    "            \n",
    "            # Call original method\n",
    "            return original_training_epoch_end(outputs) if original_training_epoch_end else None\n",
    "        \n",
    "        # Replace method\n",
    "        model.training_epoch_end = curriculum_training_epoch_end\n",
    "        \n",
    "        # Call parent fit\n",
    "        return super().fit(model, datamodule, *args, **kwargs)\n",
    "\n",
    "# Setup training\n",
    "trainer = CurriculumTrainer(\n",
    "    max_epochs=20,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    log_every_n_steps=50,\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=[\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            monitor='val_acc',\n",
    "            mode='max',\n",
    "            save_top_k=3,\n",
    "            filename='curriculum-{epoch:02d}-{val_acc:.2f}'\n",
    "        ),\n",
    "        pl.callbacks.EarlyStopping(\n",
    "            monitor='val_acc',\n",
    "            patience=10,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train with curriculum learning\n",
    "print(\"Starting curriculum learning training...\")\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Print curriculum summary\n",
    "print(model.get_curriculum_summary())\n",
    "```\n",
    "\n",
    "## 8. Curriculum Learning Analysis and Visualization\n",
    "\n",
    "```python\n",
    "class CurriculumAnalyzer:\n",
    "    \"\"\"Analyze and visualize curriculum learning progress\"\"\"\n",
    "    \n",
    "    def __init__(self, model, data_module):\n",
    "        self.model = model\n",
    "        self.data_module = data_module\n",
    "        \n",
    "    def plot_difficulty_distribution(self):\n",
    "        \"\"\"Plot distribution of sample difficulties\"\"\"\n",
    "        if not hasattr(self.data_module, 'train_dataset'):\n",
    "            print(\"Train dataset not available\")\n",
    "            return\n",
    "        \n",
    "        difficulties = self.data_module.train_dataset.difficulty_scores\n",
    "        \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Overall distribution\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.hist(difficulties, bins=50, alpha=0.7, color='blue')\n",
    "        plt.xlabel('Difficulty Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Overall Difficulty Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Distribution by class\n",
    "        plt.subplot(1, 3, 2)\n",
    "        for digit in range(10):\n",
    "            digit_indices = [i for i, (_, label, _) in enumerate(self.data_module.train_dataset) if label == digit]\n",
    "            digit_difficulties = [difficulties[i] for i in digit_indices[:100]]  # Sample for speed\n",
    "            plt.hist(digit_difficulties, bins=20, alpha=0.5, label=f'Digit {digit}')\n",
    "        \n",
    "        plt.xlabel('Difficulty Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Difficulty by Digit Class')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Cumulative distribution\n",
    "        plt.subplot(1, 3, 3)\n",
    "        sorted_difficulties = np.sort(difficulties)\n",
    "        cumulative = np.arange(1, len(sorted_difficulties) + 1) / len(sorted_difficulties)\n",
    "        plt.plot(sorted_difficulties, cumulative, 'b-', linewidth=2)\n",
    "        plt.xlabel('Difficulty Score')\n",
    "        plt.ylabel('Cumulative Probability')\n",
    "        plt.title('Cumulative Difficulty Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_curriculum_progress(self):\n",
    "        \"\"\"Plot curriculum learning progress\"\"\"\n",
    "        if not hasattr(self.model, 'curriculum_progress') or not self.model.curriculum_progress:\n",
    "            print(\"No curriculum progress data available\")\n",
    "            return\n",
    "        \n",
    "        progress = self.model.curriculum_progress\n",
    "        epochs = [p['epoch'] for p in progress]\n",
    "        difficulties = [p['avg_difficulty'] for p in progress]\n",
    "        losses = [p['avg_loss'] for p in progress]\n",
    "        correlations = [p['difficulty_loss_correlation'] for p in progress]\n",
    "        samples = [p['samples_seen'] for p in progress]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Average difficulty over time\n",
    "        axes[0, 0].plot(epochs, difficulties, 'b-', marker='o', linewidth=2)\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Average Difficulty')\n",
    "        axes[0, 0].set_title('Curriculum Progression: Difficulty')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss over time\n",
    "        axes[0, 1].plot(epochs, losses, 'r-', marker='s', linewidth=2)\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Average Loss')\n",
    "        axes[0, 1].set_title('Training Loss Progression')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Difficulty-Loss correlation\n",
    "        axes[1, 0].plot(epochs, correlations, 'g-', marker='^', linewidth=2)\n",
    "        axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Correlation')\n",
    "        axes[1, 0].set_title('Difficulty-Loss Correlation')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Samples seen\n",
    "        axes[1, 1].plot(epochs, samples, 'm-', marker='D', linewidth=2)\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Samples Seen')\n",
    "        axes[1, 1].set_title('Training Set Coverage')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def compare_strategies(self, strategies=['linear', 'exponential', 'step'], epochs=10):\n",
    "        \"\"\"Compare different curriculum strategies\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            print(f\"Training with {strategy} curriculum strategy...\")\n",
    "            \n",
    "            # Create model and data module\n",
    "            model = CurriculumLearningModel(curriculum_strategy=strategy)\n",
    "            data_module = CurriculumDataModule(curriculum_strategy=strategy, batch_size=64)\n",
    "            \n",
    "            # Train\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=epochs,\n",
    "                accelerator='auto',\n",
    "                devices=1,\n",
    "                logger=False,\n",
    "                enable_checkpointing=False,\n",
    "                enable_progress_bar=False\n",
    "            )\n",
    "            \n",
    "            trainer.fit(model, data_module)\n",
    "            \n",
    "            # Store results\n",
    "            val_results = trainer.validate(model, data_module, verbose=False)\n",
    "            results[strategy] = {\n",
    "                'val_acc': val_results[0]['val_acc'],\n",
    "                'val_loss': val_results[0]['val_loss'],\n",
    "                'curriculum_progress': model.curriculum_progress\n",
    "            }\n",
    "        \n",
    "        # Plot comparison\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Validation accuracy comparison\n",
    "        strategies_list = list(results.keys())\n",
    "        val_accs = [results[s]['val_acc'] for s in strategies_list]\n",
    "        \n",
    "        axes[0].bar(strategies_list, val_accs, alpha=0.7)\n",
    "        axes[0].set_ylabel('Validation Accuracy')\n",
    "        axes[0].set_title('Strategy Comparison: Final Accuracy')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning curves\n",
    "        for strategy in strategies_list:\n",
    "            progress = results[strategy]['curriculum_progress']\n",
    "            if progress:\n",
    "                epochs_list = [p['epoch'] for p in progress]\n",
    "                difficulties = [p['avg_difficulty'] for p in progress]\n",
    "                axes[1].plot(epochs_list, difficulties, marker='o', label=strategy, linewidth=2)\n",
    "        \n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Average Difficulty')\n",
    "        axes[1].set_title('Curriculum Progression Comparison')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Run analysis\n",
    "analyzer = CurriculumAnalyzer(model, data_module)\n",
    "analyzer.plot_difficulty_distribution()\n",
    "analyzer.plot_curriculum_progress()\n",
    "\n",
    "# Compare different strategies (commented out for speed)\n",
    "# comparison_results = analyzer.compare_strategies(strategies=['linear', 'step'], epochs=5)\n",
    "```\n",
    "\n",
    "## 9. Advanced Curriculum Techniques\n",
    "\n",
    "```python\n",
    "class AdvancedCurriculumTechniques:\n",
    "    \"\"\"Advanced curriculum learning techniques\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def self_paced_learning(model, dataloader, lambda_param=1.0):\n",
    "        \"\"\"Implement self-paced learning based on sample losses\"\"\"\n",
    "        model.eval()\n",
    "        sample_losses = []\n",
    "        sample_indices = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(dataloader):\n",
    "                if len(batch) == 3:\n",
    "                    x, y, _ = batch\n",
    "                else:\n",
    "                    x, y = batch\n",
    "                \n",
    "                logits = model(x)\n",
    "                losses = F.cross_entropy(logits, y, reduction='none')\n",
    "                \n",
    "                # Store losses and indices\n",
    "                sample_losses.extend(losses.cpu().numpy())\n",
    "                batch_size = x.size(0)\n",
    "                indices = range(batch_idx * batch_size, batch_idx * batch_size + batch_size)\n",
    "                sample_indices.extend(indices)\n",
    "        \n",
    "        # Self-paced selection based on loss threshold\n",
    "        sample_losses = np.array(sample_losses)\n",
    "        threshold = np.percentile(sample_losses, lambda_param * 100)\n",
    "        selected_indices = [idx for idx, loss in zip(sample_indices, sample_losses) if loss <= threshold]\n",
    "        \n",
    "        return selected_indices, threshold\n",
    "    \n",
    "    @staticmethod\n",
    "    def mentornet_scoring(student_losses, mentor_losses, beta=0.1):\n",
    "        \"\"\"MentorNet-style sample weighting\"\"\"\n",
    "        # Compute relative loss difference\n",
    "        loss_diff = student_losses - mentor_losses\n",
    "        \n",
    "        # Apply weighting function\n",
    "        weights = torch.sigmoid(-beta * loss_diff)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def superloss_curriculum(losses, tau=1.0):\n",
    "        \"\"\"SuperLoss: automatic curriculum via loss reweighting\"\"\"\n",
    "        # Compute confidence based on loss ranking\n",
    "        sorted_losses, sorted_indices = torch.sort(losses)\n",
    "        ranks = torch.zeros_like(sorted_losses)\n",
    "        ranks[sorted_indices] = torch.arange(len(losses), dtype=torch.float)\n",
    "        \n",
    "        # Normalize ranks to [0, 1]\n",
    "        normalized_ranks = ranks / len(losses)\n",
    "        \n",
    "        # Apply SuperLoss weighting\n",
    "        weights = torch.exp(-tau * normalized_ranks)\n",
    "        \n",
    "        return weights\n",
    "\n",
    "print(\"Advanced curriculum techniques implemented!\")\n",
    "```\n",
    "\n",
    "## 10. Curriculum Learning Evaluation\n",
    "\n",
    "```python\n",
    "def evaluate_curriculum_effectiveness(baseline_model, curriculum_model, test_dataloader):\n",
    "    \"\"\"Compare baseline vs curriculum learning performance\"\"\"\n",
    "    \n",
    "    def evaluate_model(model, dataloader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                if len(batch) == 3:\n",
    "                    x, y, _ = batch\n",
    "                else:\n",
    "                    x, y = batch\n",
    "                \n",
    "                logits = model(x)\n",
    "                loss = F.cross_entropy(logits, y)\n",
    "                losses.append(loss.item())\n",
    "                \n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        avg_loss = np.mean(losses)\n",
    "        \n",
    "        return accuracy, avg_loss\n",
    "    \n",
    "    # Evaluate both models\n",
    "    baseline_acc, baseline_loss = evaluate_model(baseline_model, test_dataloader)\n",
    "    curriculum_acc, curriculum_loss = evaluate_model(curriculum_model, test_dataloader)\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"Curriculum Learning Evaluation:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Baseline Model:\")\n",
    "    print(f\"  Accuracy: {baseline_acc:.4f}\")\n",
    "    print(f\"  Loss: {baseline_loss:.4f}\")\n",
    "    print(f\"Curriculum Model:\")\n",
    "    print(f\"  Accuracy: {curriculum_acc:.4f}\")\n",
    "    print(f\"  Loss: {curriculum_loss:.4f}\")\n",
    "    print(f\"Improvement:\")\n",
    "    print(f\"  Accuracy: +{curriculum_acc - baseline_acc:.4f}\")\n",
    "    print(f\"  Loss: {curriculum_loss - baseline_loss:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'baseline': {'accuracy': baseline_acc, 'loss': baseline_loss},\n",
    "        'curriculum': {'accuracy': curriculum_acc, 'loss': curriculum_loss},\n",
    "        'improvement': {\n",
    "            'accuracy': curriculum_acc - baseline_acc,\n",
    "            'loss': curriculum_loss - baseline_loss\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Create baseline model for comparison\n",
    "baseline_model = CurriculumLearningModel(curriculum_strategy='none')\n",
    "baseline_trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    logger=False,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False\n",
    ")\n",
    "\n",
    "# Train baseline (without curriculum)\n",
    "baseline_data = CurriculumDataModule(curriculum_strategy='none', batch_size=64)\n",
    "baseline_trainer.fit(baseline_model, baseline_data)\n",
    "\n",
    "# Compare with curriculum model\n",
    "test_dataloader = data_module.val_dataloader()\n",
    "comparison_results = evaluate_curriculum_effectiveness(baseline_model, model, test_dataloader)\n",
    "\n",
    "print(\"Curriculum learning evaluation completed!\")\n",
    "\n",
    "# Summary\n",
    "class CurriculumSummary:\n",
    "    \"\"\"Summary of curriculum learning benefits and results\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_summary(model, comparison_results):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CURRICULUM LEARNING SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(\"\\nKey Benefits Demonstrated:\")\n",
    "        print(\"- Progressive difficulty scheduling\")\n",
    "        print(\"- Adaptive sample selection\")\n",
    "        print(\"- Custom batch loop implementation\")\n",
    "        print(\"- Performance monitoring and analysis\")\n",
    "        \n",
    "        if comparison_results:\n",
    "            acc_improvement = comparison_results['improvement']['accuracy']\n",
    "            print(f\"\\nPerformance Improvement:\")\n",
    "            print(f\"- Accuracy improvement: {acc_improvement:+.4f}\")\n",
    "            print(f\"- Final curriculum accuracy: {comparison_results['curriculum']['accuracy']:.4f}\")\n",
    "        \n",
    "        if hasattr(model, 'curriculum_progress') and model.curriculum_progress:\n",
    "            final_progress = model.curriculum_progress[-1]\n",
    "            print(f\"\\nCurriculum Progress:\")\n",
    "            print(f\"- Final difficulty level: {final_progress['avg_difficulty']:.3f}\")\n",
    "            print(f\"- Difficulty-loss correlation: {final_progress['difficulty_loss_correlation']:.3f}\")\n",
    "        \n",
    "        print(\"\\nTechniques Implemented:\")\n",
    "        print(\"- Custom difficulty scoring\")\n",
    "        print(\"- Progressive sampling strategies\")\n",
    "        print(\"- Adaptive difficulty adjustment\")\n",
    "        print(\"- Performance-based curriculum control\")\n",
    "\n",
    "CurriculumSummary.print_summary(model, comparison_results)\n",
    "```\n",
    "\n",
    "# Summary\n",
    "\n",
    "This notebook demonstrated advanced curriculum learning implementation using custom batch loops in PyTorch Lightning. Key concepts and techniques covered:\n",
    "\n",
    "## Core Curriculum Learning Concepts\n",
    "- **Progressive Difficulty**: Starting with easy samples and gradually increasing complexity\n",
    "- **Adaptive Strategies**: Dynamic curriculum adjustment based on model performance\n",
    "- **Sample Selection**: Intelligent selection of training samples based on difficulty metrics\n",
    "- **Pacing Functions**: Mathematical functions controlling curriculum progression speed\n",
    "\n",
    "## Custom Implementation Components\n",
    "- **Difficulty-Aware Datasets**: Datasets with automatic difficulty scoring\n",
    "- **Curriculum Samplers**: Custom samplers implementing various progression strategies\n",
    "- **Custom Batch Loops**: Advanced loops with curriculum logic integration\n",
    "- **Adaptive Controllers**: Performance-based curriculum adjustment mechanisms\n",
    "\n",
    "## Advanced Techniques Demonstrated\n",
    "- **Self-Paced Learning**: Automatic sample selection based on model losses\n",
    "- **Multi-Strategy Comparison**: Evaluation of linear, exponential, and step curricula\n",
    "- **Performance Monitoring**: Real-time tracking of curriculum effectiveness\n",
    "- **Statistical Analysis**: Correlation analysis between difficulty and performance\n",
    "\n",
    "## Key Benefits Achieved\n",
    "- **Faster Convergence**: Models learn fundamental patterns before complex ones\n",
    "- **Better Stability**: Gradual complexity introduction prevents training collapse\n",
    "- **Improved Generalization**: Systematic learning reduces overfitting tendencies\n",
    "- **Resource Efficiency**: Smarter training requires fewer total epochs\n",
    "\n",
    "## Implementation Highlights\n",
    "- Custom loop architecture seamlessly integrated with Lightning\n",
    "- Multiple curriculum strategies with easy switching\n",
    "- Real-time adaptation based on training performance\n",
    "- Comprehensive analysis and visualization tools\n",
    "\n",
    "## Practical Applications\n",
    "- **Computer Vision**: Progressive image complexity for better feature learning\n",
    "- **Natural Language Processing**: Sentence length and complexity progression\n",
    "- **Reinforcement Learning**: Task difficulty scheduling for agent training\n",
    "- **Scientific Computing**: Multi-scale problem solving approaches\n",
    "\n",
    "## Next Steps\n",
    "- Implement curriculum learning for different domains (NLP, RL)\n",
    "- Explore meta-learning approaches for automatic curriculum design\n",
    "- Integrate with hyperparameter optimization frameworks\n",
    "- Develop curriculum learning for multi-task scenarios\n",
    "\n",
    "The curriculum learning framework provides a powerful tool for improving training efficiency and model performance across various machine learning applications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
