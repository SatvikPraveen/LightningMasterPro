{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed878bbf",
   "metadata": {},
   "source": [
    "# File Location: notebooks/06_advanced_mechanics/14_custom_loops_kfold.ipynb\n",
    "\n",
    "# Custom Loops and K-Fold Cross Validation\n",
    "\n",
    "This notebook explores advanced PyTorch Lightning loop customization through K-Fold cross validation implementation. We'll learn how to create custom training loops, implement cross-validation strategies, and wrap existing training loops for enhanced functionality.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand PyTorch Lightning's loop architecture\n",
    "- Implement custom training loops and FitLoop wrappers\n",
    "- Build a comprehensive K-Fold cross validation system\n",
    "- Handle data splitting and validation across multiple folds\n",
    "- Aggregate results and perform statistical analysis\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from pytorch_lightning.loops import FitLoop\n",
    "from pytorch_lightning.loops.base import Loop\n",
    "from pytorch_lightning.trainer.states import TrainerFn\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Lightning version: {pl.__version__}\")\n",
    "```\n",
    "\n",
    "## 1. Understanding Lightning Loop Architecture\n",
    "\n",
    "```python\n",
    "class LoopArchitectureExplainer:\n",
    "    \"\"\"\n",
    "    PyTorch Lightning Loop Architecture:\n",
    "    \n",
    "    1. Base Loop: Core loop functionality\n",
    "    2. FitLoop: Main training loop\n",
    "    3. EvaluationLoop: Validation/test loops\n",
    "    4. OptimizerLoop: Optimizer step loop\n",
    "    5. Custom Loops: User-defined loops\n",
    "    \n",
    "    Loop Hierarchy:\n",
    "    - TrainerLoop (top-level)\n",
    "      ├── FitLoop (training)\n",
    "      │   ├── TrainingEpochLoop\n",
    "      │   └── ValidationEpochLoop\n",
    "      └── EvaluationLoop (test/predict)\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def explain_loop_methods():\n",
    "        methods = {\n",
    "            \"setup()\": \"Initialize loop state and resources\",\n",
    "            \"reset()\": \"Reset loop state between runs\",\n",
    "            \"on_run_start()\": \"Called at the beginning of loop execution\",\n",
    "            \"advance()\": \"Main loop iteration logic\",\n",
    "            \"on_run_end()\": \"Called at the end of loop execution\",\n",
    "            \"run()\": \"Main entry point that orchestrates the loop\"\n",
    "        }\n",
    "        \n",
    "        print(\"Key Loop Methods:\")\n",
    "        for method, description in methods.items():\n",
    "            print(f\"  {method}: {description}\")\n",
    "\n",
    "LoopArchitectureExplainer.explain_loop_methods()\n",
    "```\n",
    "\n",
    "## 2. Custom K-Fold Cross Validation Loop\n",
    "\n",
    "```python\n",
    "class KFoldLoop(Loop):\n",
    "    \"\"\"Custom loop for K-Fold cross validation\"\"\"\n",
    "    \n",
    "    def __init__(self, num_folds: int = 5, shuffle: bool = True, stratified: bool = True):\n",
    "        super().__init__()\n",
    "        self.num_folds = num_folds\n",
    "        self.shuffle = shuffle\n",
    "        self.stratified = stratified\n",
    "        \n",
    "        # Results storage\n",
    "        self.fold_results = []\n",
    "        self.current_fold = 0\n",
    "        self.kfold_splitter = None\n",
    "        \n",
    "        # Data storage\n",
    "        self.full_dataset = None\n",
    "        self.targets = None\n",
    "        \n",
    "    @property\n",
    "    def done(self) -> bool:\n",
    "        \"\"\"Check if all folds are completed\"\"\"\n",
    "        return self.current_fold >= self.num_folds\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the loop state\"\"\"\n",
    "        self.current_fold = 0\n",
    "        self.fold_results = []\n",
    "    \n",
    "    def on_run_start(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Initialize K-Fold splitter and data\"\"\"\n",
    "        print(f\"Starting {self.num_folds}-fold cross validation\")\n",
    "        \n",
    "        # Get full dataset from trainer's datamodule\n",
    "        datamodule = self.trainer.datamodule\n",
    "        if hasattr(datamodule, 'full_dataset'):\n",
    "            self.full_dataset = datamodule.full_dataset\n",
    "            self.targets = datamodule.targets\n",
    "        else:\n",
    "            raise ValueError(\"DataModule must have 'full_dataset' and 'targets' attributes\")\n",
    "        \n",
    "        # Initialize appropriate splitter\n",
    "        if self.stratified and self.targets is not None:\n",
    "            self.kfold_splitter = StratifiedKFold(\n",
    "                n_splits=self.num_folds, \n",
    "                shuffle=self.shuffle, \n",
    "                random_state=42\n",
    "            )\n",
    "            splits = self.kfold_splitter.split(\n",
    "                range(len(self.full_dataset)), \n",
    "                self.targets\n",
    "            )\n",
    "        else:\n",
    "            self.kfold_splitter = KFold(\n",
    "                n_splits=self.num_folds, \n",
    "                shuffle=self.shuffle, \n",
    "                random_state=42\n",
    "            )\n",
    "            splits = self.kfold_splitter.split(range(len(self.full_dataset)))\n",
    "        \n",
    "        # Store all splits\n",
    "        self.splits = list(splits)\n",
    "    \n",
    "    def advance(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Execute one fold of cross validation\"\"\"\n",
    "        print(f\"\\n--- Fold {self.current_fold + 1}/{self.num_folds} ---\")\n",
    "        \n",
    "        # Get train/val indices for current fold\n",
    "        train_indices, val_indices = self.splits[self.current_fold]\n",
    "        \n",
    "        # Create datasets for current fold\n",
    "        train_dataset = Subset(self.full_dataset, train_indices)\n",
    "        val_dataset = Subset(self.full_dataset, val_indices)\n",
    "        \n",
    "        # Update datamodule with fold-specific datasets\n",
    "        self.trainer.datamodule.setup_fold(train_dataset, val_dataset)\n",
    "        \n",
    "        # Reset model for this fold (important for fair comparison)\n",
    "        self.trainer.lightning_module = self.trainer.lightning_module.__class__(\n",
    "            **self.trainer.lightning_module.hparams\n",
    "        )\n",
    "        \n",
    "        # Run training for this fold\n",
    "        self.trainer.fit_loop.reset()\n",
    "        self.trainer.fit_loop.run()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_results = self.trainer.validate(\n",
    "            model=self.trainer.lightning_module,\n",
    "            dataloaders=self.trainer.datamodule.val_dataloader(),\n",
    "            verbose=False\n",
    "        )[0]\n",
    "        \n",
    "        # Store fold results\n",
    "        fold_result = {\n",
    "            'fold': self.current_fold + 1,\n",
    "            'train_indices': train_indices.tolist(),\n",
    "            'val_indices': val_indices.tolist(),\n",
    "            'val_results': val_results,\n",
    "            'model_state': self.trainer.lightning_module.state_dict().copy()\n",
    "        }\n",
    "        \n",
    "        self.fold_results.append(fold_result)\n",
    "        \n",
    "        print(f\"Fold {self.current_fold + 1} completed:\")\n",
    "        for metric, value in val_results.items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "        self.current_fold += 1\n",
    "    \n",
    "    def on_run_end(self) -> None:\n",
    "        \"\"\"Aggregate and report final results\"\"\"\n",
    "        print(f\"\\n=== K-Fold Cross Validation Results ===\")\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        metric_aggregates = self._aggregate_metrics()\n",
    "        \n",
    "        # Print summary\n",
    "        for metric, stats in metric_aggregates.items():\n",
    "            print(f\"{metric}:\")\n",
    "            print(f\"  Mean: {stats['mean']:.4f} ± {stats['std']:.4f}\")\n",
    "            print(f\"  Min: {stats['min']:.4f}, Max: {stats['max']:.4f}\")\n",
    "        \n",
    "        # Store aggregated results\n",
    "        self.trainer.lightning_module.kfold_results = {\n",
    "            'fold_results': self.fold_results,\n",
    "            'aggregated_metrics': metric_aggregates\n",
    "        }\n",
    "    \n",
    "    def _aggregate_metrics(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Aggregate metrics across all folds\"\"\"\n",
    "        metrics = defaultdict(list)\n",
    "        \n",
    "        # Collect metrics from all folds\n",
    "        for fold_result in self.fold_results:\n",
    "            for metric, value in fold_result['val_results'].items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    metrics[metric].append(value)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        aggregated = {}\n",
    "        for metric, values in metrics.items():\n",
    "            aggregated[metric] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values),\n",
    "                'values': values\n",
    "            }\n",
    "        \n",
    "        return aggregated\n",
    "\n",
    "print(\"K-Fold Loop implementation complete!\")\n",
    "```\n",
    "\n",
    "## 3. Custom FitLoop Wrapper\n",
    "\n",
    "```python\n",
    "class KFoldFitLoop(FitLoop):\n",
    "    \"\"\"Custom FitLoop wrapper that integrates K-Fold validation\"\"\"\n",
    "    \n",
    "    def __init__(self, num_folds: int = 5, stratified: bool = True):\n",
    "        super().__init__()\n",
    "        self.kfold_loop = KFoldLoop(num_folds=num_folds, stratified=stratified)\n",
    "    \n",
    "    def run(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Override run to use K-Fold instead of standard training\"\"\"\n",
    "        # Setup the K-Fold loop\n",
    "        self.kfold_loop.trainer = self.trainer\n",
    "        \n",
    "        # Run K-Fold cross validation\n",
    "        self.kfold_loop.run()\n",
    "        \n",
    "        # The rest of the training state is handled by the K-Fold loop\n",
    "        return None\n",
    "\n",
    "print(\"Custom FitLoop wrapper created!\")\n",
    "```\n",
    "\n",
    "## 4. Lightning Module for K-Fold\n",
    "\n",
    "```python\n",
    "class KFoldClassifier(pl.LightningModule):\n",
    "    \"\"\"Lightning module optimized for K-Fold cross validation\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        num_classes: int = 10, \n",
    "        learning_rate: float = 0.001,\n",
    "        architecture: str = 'simple',\n",
    "        dropout: float = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Build model based on architecture\n",
    "        if architecture == 'simple':\n",
    "            self.model = self._build_simple_model()\n",
    "        elif architecture == 'resnet':\n",
    "            self.model = self._build_resnet_model()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown architecture: {architecture}\")\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Metrics for each split\n",
    "        self.train_acc = pl.metrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = pl.metrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "        # Storage for K-fold results\n",
    "        self.kfold_results = None\n",
    "        \n",
    "        # Track fold-specific metrics\n",
    "        self.fold_metrics = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_acc': []\n",
    "        }\n",
    "    \n",
    "    def _build_simple_model(self):\n",
    "        \"\"\"Build a simple MLP model\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.hparams.dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.hparams.dropout),\n",
    "            nn.Linear(128, self.hparams.num_classes)\n",
    "        )\n",
    "    \n",
    "    def _build_resnet_model(self):\n",
    "        \"\"\"Build a ResNet-like model for MNIST\"\"\"\n",
    "        class ResNetBlock(nn.Module):\n",
    "            def __init__(self, in_channels, out_channels, stride=1):\n",
    "                super().__init__()\n",
    "                self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
    "                self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "                self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "                self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "                \n",
    "                # Shortcut connection\n",
    "                self.shortcut = nn.Sequential()\n",
    "                if stride != 1 or in_channels != out_channels:\n",
    "                    self.shortcut = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
    "                        nn.BatchNorm2d(out_channels)\n",
    "                    )\n",
    "            \n",
    "            def forward(self, x):\n",
    "                residual = x\n",
    "                out = F.relu(self.bn1(self.conv1(x)))\n",
    "                out = self.bn2(self.conv2(out))\n",
    "                out += self.shortcut(residual)\n",
    "                out = F.relu(out)\n",
    "                return out\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            ResNetBlock(32, 32),\n",
    "            ResNetBlock(32, 64, stride=2),\n",
    "            ResNetBlock(64, 64),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, self.hparams.num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        self.train_acc(logits, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        self.val_acc(logits, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return {'val_loss': loss, 'val_acc': self.val_acc.compute()}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_fold_summary(self):\n",
    "        \"\"\"Get summary of K-fold results\"\"\"\n",
    "        if self.kfold_results is None:\n",
    "            return \"No K-fold results available\"\n",
    "        \n",
    "        summary = []\n",
    "        summary.append(\"K-Fold Cross Validation Summary:\")\n",
    "        summary.append(\"=\" * 40)\n",
    "        \n",
    "        for metric, stats in self.kfold_results['aggregated_metrics'].items():\n",
    "            summary.append(f\"{metric}: {stats['mean']:.4f} ± {stats['std']:.4f}\")\n",
    "        \n",
    "        return \"\\n\".join(summary)\n",
    "\n",
    "# Initialize model\n",
    "model = KFoldClassifier(num_classes=10, learning_rate=0.001, architecture='simple')\n",
    "```\n",
    "\n",
    "## 5. Data Module for K-Fold Cross Validation\n",
    "\n",
    "```python\n",
    "class KFoldDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Data module specifically designed for K-Fold cross validation\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size: int = 64, num_workers: int = 4):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        # Transform\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        \n",
    "        # Will be set during K-fold\n",
    "        self.fold_train_dataset = None\n",
    "        self.fold_val_dataset = None\n",
    "        \n",
    "        # Full dataset storage\n",
    "        self.full_dataset = None\n",
    "        self.targets = None\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # Download MNIST\n",
    "        torchvision.datasets.MNIST('./data', train=True, download=True)\n",
    "        torchvision.datasets.MNIST('./data', train=False, download=True)\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            # Combine train and test sets for K-fold\n",
    "            train_dataset = torchvision.datasets.MNIST('./data', train=True, transform=self.transform)\n",
    "            test_dataset = torchvision.datasets.MNIST('./data', train=False, transform=self.transform)\n",
    "            \n",
    "            # Combine datasets\n",
    "            full_data = torch.cat([train_dataset.data, test_dataset.data])\n",
    "            full_targets = torch.cat([train_dataset.targets, test_dataset.targets])\n",
    "            \n",
    "            # Create combined dataset\n",
    "            self.full_dataset = torchvision.datasets.MNIST('./data', train=True, transform=self.transform)\n",
    "            self.full_dataset.data = full_data\n",
    "            self.full_dataset.targets = full_targets\n",
    "            \n",
    "            # Store targets for stratified splitting\n",
    "            self.targets = full_targets.numpy()\n",
    "    \n",
    "    def setup_fold(self, train_dataset, val_dataset):\n",
    "        \"\"\"Setup datasets for a specific fold\"\"\"\n",
    "        self.fold_train_dataset = train_dataset\n",
    "        self.fold_val_dataset = val_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.fold_train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.fold_val_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "# Initialize data module\n",
    "data_module = KFoldDataModule(batch_size=64, num_workers=4)\n",
    "```\n",
    "\n",
    "## 6. Custom Trainer with K-Fold Integration\n",
    "\n",
    "```python\n",
    "class KFoldTrainer(pl.Trainer):\n",
    "    \"\"\"Custom trainer with integrated K-Fold cross validation\"\"\"\n",
    "    \n",
    "    def __init__(self, num_folds: int = 5, stratified: bool = True, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Replace the default fit loop with K-Fold loop\n",
    "        self.fit_loop = KFoldFitLoop(num_folds=num_folds, stratified=stratified)\n",
    "        self.fit_loop.trainer = self\n",
    "    \n",
    "    def fit(self, model, datamodule=None, *args, **kwargs):\n",
    "        \"\"\"Override fit to use K-Fold validation\"\"\"\n",
    "        # Standard setup\n",
    "        self.lightning_module = model\n",
    "        self.datamodule = datamodule\n",
    "        \n",
    "        if datamodule:\n",
    "            datamodule.setup('fit')\n",
    "        \n",
    "        # Run K-Fold cross validation\n",
    "        self.fit_loop.run()\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Alternative: Standard trainer with manual K-Fold\n",
    "def run_kfold_validation(model_class, data_module, num_folds=5, max_epochs=10):\n",
    "    \"\"\"Manual K-Fold validation function\"\"\"\n",
    "    \n",
    "    # Initialize results storage\n",
    "    fold_results = []\n",
    "    \n",
    "    # Setup full dataset\n",
    "    data_module.setup('fit')\n",
    "    full_dataset = data_module.full_dataset\n",
    "    targets = data_module.targets\n",
    "    \n",
    "    # Initialize K-Fold splitter\n",
    "    kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_indices, val_indices) in enumerate(kfold.split(range(len(full_dataset)), targets)):\n",
    "        print(f\"\\n--- Fold {fold + 1}/{num_folds} ---\")\n",
    "        \n",
    "        # Create fold datasets\n",
    "        train_dataset = Subset(full_dataset, train_indices)\n",
    "        val_dataset = Subset(full_dataset, val_indices)\n",
    "        \n",
    "        # Setup data module for this fold\n",
    "        data_module.setup_fold(train_dataset, val_dataset)\n",
    "        \n",
    "        # Initialize fresh model for this fold\n",
    "        model = model_class()\n",
    "        \n",
    "        # Create trainer for this fold\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            accelerator='auto',\n",
    "            devices=1,\n",
    "            logger=False,\n",
    "            enable_checkpointing=False,\n",
    "            enable_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        # Validate\n",
    "        val_results = trainer.validate(model, data_module, verbose=False)[0]\n",
    "        \n",
    "        # Store results\n",
    "        fold_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'val_results': val_results,\n",
    "            'model': model\n",
    "        })\n",
    "        \n",
    "        print(f\"Fold {fold + 1} - Val Acc: {val_results['val_acc']:.4f}, Val Loss: {val_results['val_loss']:.4f}\")\n",
    "    \n",
    "    return fold_results\n",
    "\n",
    "print(\"K-Fold trainer and manual validation function ready!\")\n",
    "```\n",
    "\n",
    "## 7. Running K-Fold Cross Validation\n",
    "\n",
    "```python\n",
    "# Method 1: Using Custom Trainer\n",
    "print(\"Method 1: Custom K-Fold Trainer\")\n",
    "kfold_trainer = KFoldTrainer(\n",
    "    num_folds=5,\n",
    "    stratified=True,\n",
    "    max_epochs=10,\n",
    "    accelerator='auto',\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "# Train with K-Fold\n",
    "kfold_trainer.fit(model, data_module)\n",
    "\n",
    "# Get results\n",
    "kfold_results = model.kfold_results\n",
    "print(model.get_fold_summary())\n",
    "\n",
    "# Method 2: Manual K-Fold validation\n",
    "print(\"\\nMethod 2: Manual K-Fold Validation\")\n",
    "manual_results = run_kfold_validation(\n",
    "    lambda: KFoldClassifier(num_classes=10, learning_rate=0.001),\n",
    "    data_module,\n",
    "    num_folds=5,\n",
    "    max_epochs=10\n",
    ")\n",
    "\n",
    "# Aggregate manual results\n",
    "manual_metrics = defaultdict(list)\n",
    "for result in manual_results:\n",
    "    for metric, value in result['val_results'].items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            manual_metrics[metric].append(value)\n",
    "\n",
    "print(\"\\nManual K-Fold Results:\")\n",
    "for metric, values in manual_metrics.items():\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values)\n",
    "    print(f\"{metric}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "```\n",
    "\n",
    "## 8. Advanced K-Fold Analysis\n",
    "\n",
    "```python\n",
    "class KFoldAnalyzer:\n",
    "    \"\"\"Advanced analysis tools for K-Fold results\"\"\"\n",
    "    \n",
    "    def __init__(self, fold_results):\n",
    "        self.fold_results = fold_results\n",
    "    \n",
    "    def plot_fold_performance(self):\n",
    "        \"\"\"Plot performance across folds\"\"\"\n",
    "        if isinstance(self.fold_results, dict) and 'fold_results' in self.fold_results:\n",
    "            results = self.fold_results['fold_results']\n",
    "        else:\n",
    "            results = self.fold_results\n",
    "        \n",
    "        metrics = ['val_acc', 'val_loss']\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        for idx, metric in enumerate(metrics):\n",
    "            values = []\n",
    "            fold_nums = []\n",
    "            \n",
    "            for result in results:\n",
    "                if 'val_results' in result and metric in result['val_results']:\n",
    "                    values.append(result['val_results'][metric])\n",
    "                    fold_nums.append(result.get('fold', len(fold_nums) + 1))\n",
    "                elif metric in result:\n",
    "                    values.append(result[metric])\n",
    "                    fold_nums.append(result.get('fold', len(fold_nums) + 1))\n",
    "            \n",
    "            if values:\n",
    "                axes[idx].bar(fold_nums, values, alpha=0.7)\n",
    "                axes[idx].axhline(y=np.mean(values), color='r', linestyle='--', label=f'Mean: {np.mean(values):.4f}')\n",
    "                axes[idx].set_xlabel('Fold')\n",
    "                axes[idx].set_ylabel(metric.replace('_', ' ').title())\n",
    "                axes[idx].set_title(f'{metric.replace(\"_\", \" \").title()} Across Folds')\n",
    "                axes[idx].legend()\n",
    "                axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def statistical_analysis(self):\n",
    "        \"\"\"Perform statistical analysis of fold results\"\"\"\n",
    "        if isinstance(self.fold_results, dict) and 'fold_results' in self.fold_results:\n",
    "            results = self.fold_results['fold_results']\n",
    "        else:\n",
    "            results = self.fold_results\n",
    "        \n",
    "        # Extract metrics\n",
    "        metrics_data = defaultdict(list)\n",
    "        for result in results:\n",
    "            val_results = result.get('val_results', result)\n",
    "            for metric, value in val_results.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    metrics_data[metric].append(value)\n",
    "        \n",
    "        # Statistical tests\n",
    "        print(\"Statistical Analysis:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for metric, values in metrics_data.items():\n",
    "            values = np.array(values)\n",
    "            \n",
    "            print(f\"\\n{metric.upper()}:\")\n",
    "            print(f\"  Mean: {np.mean(values):.4f}\")\n",
    "            print(f\"  Std:  {np.std(values):.4f}\")\n",
    "            print(f\"  Min:  {np.min(values):.4f}\")\n",
    "            print(f\"  Max:  {np.max(values):.4f}\")\n",
    "            print(f\"  CV:   {np.std(values)/np.mean(values)*100:.2f}%\")\n",
    "            \n",
    "            # Confidence interval (95%)\n",
    "            ci_lower = np.percentile(values, 2.5)\n",
    "            ci_upper = np.percentile(values, 97.5)\n",
    "            print(f\"  95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "    \n",
    "    def compare_architectures(self, other_results, labels=None):\n",
    "        \"\"\"Compare results from different architectures\"\"\"\n",
    "        if labels is None:\n",
    "            labels = ['Architecture 1', 'Architecture 2']\n",
    "        \n",
    "        results_list = [self.fold_results, other_results]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        metric = 'val_acc'  # Focus on accuracy\n",
    "        \n",
    "        for idx, (results, label) in enumerate(zip(results_list, labels)):\n",
    "            if isinstance(results, dict) and 'fold_results' in results:\n",
    "                fold_results = results['fold_results']\n",
    "            else:\n",
    "                fold_results = results\n",
    "            \n",
    "            values = []\n",
    "            for result in fold_results:\n",
    "                val_results = result.get('val_results', result)\n",
    "                if metric in val_results:\n",
    "                    values.append(val_results[metric])\n",
    "            \n",
    "            if values:\n",
    "                positions = np.arange(len(values)) + idx * 0.4\n",
    "                ax.bar(positions, values, width=0.35, label=label, alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('Fold')\n",
    "        ax.set_ylabel('Validation Accuracy')\n",
    "        ax.set_title('Architecture Comparison Across Folds')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Analyze results\n",
    "analyzer = KFoldAnalyzer(manual_results)\n",
    "analyzer.plot_fold_performance()\n",
    "analyzer.statistical_analysis()\n",
    "```\n",
    "\n",
    "## 9. Model Selection with K-Fold\n",
    "\n",
    "```python\n",
    "class ModelSelector:\n",
    "    \"\"\"Model selection using K-Fold cross validation\"\"\"\n",
    "    \n",
    "    def __init__(self, data_module):\n",
    "        self.data_module = data_module\n",
    "        self.results = {}\n",
    "    \n",
    "    def evaluate_architecture(self, name, model_class, model_params, num_folds=5, max_epochs=10):\n",
    "        \"\"\"Evaluate a specific architecture\"\"\"\n",
    "        print(f\"\\nEvaluating {name}...\")\n",
    "        \n",
    "        fold_results = run_kfold_validation(\n",
    "            lambda: model_class(**model_params),\n",
    "            self.data_module,\n",
    "            num_folds=num_folds,\n",
    "            max_epochs=max_epochs\n",
    "        )\n",
    "        \n",
    "        # Aggregate results\n",
    "        metrics = defaultdict(list)\n",
    "        for result in fold_results:\n",
    "            for metric, value in result['val_results'].items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    metrics[metric].append(value)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        aggregated = {}\n",
    "        for metric, values in metrics.items():\n",
    "            aggregated[metric] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'values': values\n",
    "            }\n",
    "        \n",
    "        self.results[name] = {\n",
    "            'fold_results': fold_results,\n",
    "            'aggregated': aggregated\n",
    "        }\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Compare all evaluated models\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No models evaluated yet!\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nModel Comparison:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Create comparison table\n",
    "        comparison_data = []\n",
    "        for name, results in self.results.items():\n",
    "            agg = results['aggregated']\n",
    "            comparison_data.append({\n",
    "                'Model': name,\n",
    "                'Val Acc Mean': f\"{agg['val_acc']['mean']:.4f}\",\n",
    "                'Val Acc Std': f\"{agg['val_acc']['std']:.4f}\",\n",
    "                'Val Loss Mean': f\"{agg['val_loss']['mean']:.4f}\",\n",
    "                'Val Loss Std': f\"{agg['val_loss']['std']:.4f}\"\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Find best model\n",
    "        best_acc = 0\n",
    "        best_model = None\n",
    "        for name, results in self.results.items():\n",
    "            acc = results['aggregated']['val_acc']['mean']\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_model = name\n",
    "        \n",
    "        print(f\"\\nBest Model: {best_model} (Val Acc: {best_acc:.4f})\")\n",
    "    \n",
    "    def plot_comparison(self):\n",
    "        \"\"\"Plot model comparison\"\"\"\n",
    "        if len(self.results) < 2:\n",
    "            print(\"Need at least 2 models for comparison\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        models = list(self.results.keys())\n",
    "        metrics = ['val_acc', 'val_loss']\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
    "        \n",
    "        for metric_idx, metric in enumerate(metrics):\n",
    "            for model_idx, model_name in enumerate(models):\n",
    "                values = self.results[model_name]['aggregated'][metric]['values']\n",
    "                x_pos = np.arange(len(values)) + model_idx * 0.8 / len(models)\n",
    "                \n",
    "                axes[metric_idx].bar(\n",
    "                    x_pos, values, \n",
    "                    width=0.8/len(models), \n",
    "                    label=model_name,\n",
    "                    color=colors[model_idx],\n",
    "                    alpha=0.7\n",
    "                )\n",
    "            \n",
    "            axes[metric_idx].set_xlabel('Fold')\n",
    "            axes[metric_idx].set_ylabel(metric.replace('_', ' ').title())\n",
    "            axes[metric_idx].set_title(f'{metric.replace(\"_\", \" \").title()} Comparison')\n",
    "            axes[metric_idx].legend()\n",
    "            axes[metric_idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Model selection example\n",
    "selector = ModelSelector(data_module)\n",
    "\n",
    "# Evaluate different architectures\n",
    "selector.evaluate_architecture(\n",
    "    'Simple MLP',\n",
    "    KFoldClassifier,\n",
    "    {'num_classes': 10, 'learning_rate': 0.001, 'architecture': 'simple', 'dropout': 0.3},\n",
    "    num_folds=3, max_epochs=5\n",
    ")\n",
    "\n",
    "selector.evaluate_architecture(\n",
    "    'Simple MLP (High Dropout)',\n",
    "    KFoldClassifier,\n",
    "    {'num_classes': 10, 'learning_rate': 0.001, 'architecture': 'simple', 'dropout': 0.7},\n",
    "    num_folds=3, max_epochs=5\n",
    ")\n",
    "\n",
    "selector.evaluate_architecture(\n",
    "    'ResNet',\n",
    "    KFoldClassifier,\n",
    "    {'num_classes': 10, 'learning_rate': 0.001, 'architecture': 'resnet', 'dropout': 0.5},\n",
    "    num_folds=3, max_epochs=5\n",
    ")\n",
    "\n",
    "# Compare models\n",
    "selector.compare_models()\n",
    "selector.plot_comparison()\n",
    "```\n",
    "\n",
    "## 10. Advanced Loop Customization\n",
    "\n",
    "```python\n",
    "class CustomValidationLoop(Loop):\n",
    "    \"\"\"Custom validation loop with detailed metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.outputs = []\n",
    "        self.current_batch = 0\n",
    "        \n",
    "    @property\n",
    "    def done(self) -> bool:\n",
    "        return self.current_batch >= len(self.trainer.val_dataloaders[0])\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.outputs = []\n",
    "        self.current_batch = 0\n",
    "    \n",
    "    def advance(self, *args, **kwargs) -> None:\n",
    "        # Get next batch\n",
    "        batch = next(iter(self.trainer.val_dataloaders[0]))\n",
    "        \n",
    "        # Run validation step\n",
    "        with torch.no_grad():\n",
    "            output = self.trainer.lightning_module.validation_step(batch, self.current_batch)\n",
    "            self.outputs.append(output)\n",
    "        \n",
    "        self.current_batch += 1\n",
    "    \n",
    "    def on_run_end(self) -> None:\n",
    "        # Aggregate outputs\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in self.outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_acc'] for x in self.outputs]).mean()\n",
    "        \n",
    "        # Log aggregated metrics\n",
    "        self.trainer.lightning_module.log('custom_val_loss', avg_loss)\n",
    "        self.trainer.lightning_module.log('custom_val_acc', avg_acc)\n",
    "        \n",
    "        print(f\"Custom validation completed: Loss={avg_loss:.4f}, Acc={avg_acc:.4f}\")\n",
    "\n",
    "# Usage example\n",
    "class CustomLoopModel(KFoldClassifier):\n",
    "    \"\"\"Model with custom validation loop\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.custom_val_loop = CustomValidationLoop()\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Custom validation logic\n",
    "        if hasattr(self, 'custom_val_loop') and self.trainer:\n",
    "            self.custom_val_loop.trainer = self.trainer\n",
    "            # Could run custom validation here\n",
    "            pass\n",
    "        \n",
    "        return super().validation_epoch_end(outputs)\n",
    "\n",
    "print(\"Advanced loop customization examples complete!\")\n",
    "```\n",
    "\n",
    "# Summary\n",
    "\n",
    "This notebook demonstrated advanced PyTorch Lightning loop customization through comprehensive K-Fold cross validation implementation. Key concepts covered:\n",
    "\n",
    "## Core Loop Architecture\n",
    "- **Lightning Loop Hierarchy**: Understanding base loops, fit loops, and evaluation loops\n",
    "- **Custom Loop Creation**: Building loops from scratch with proper state management\n",
    "- **FitLoop Wrapping**: Extending existing loops with additional functionality\n",
    "- **Loop Integration**: Seamlessly integrating custom loops with Lightning trainers\n",
    "\n",
    "## K-Fold Cross Validation Implementation\n",
    "- **Data Splitting**: Proper stratified and non-stratified data splitting\n",
    "- **Fold Management**: Handling multiple training runs with fresh model initialization\n",
    "- **Result Aggregation**: Statistical analysis and confidence interval calculation\n",
    "- **Model Selection**: Systematic comparison of different architectures\n",
    "\n",
    "## Advanced Features Implemented\n",
    "- **Custom Trainers**: Extended trainer classes with K-Fold integration\n",
    "- **Statistical Analysis**: Comprehensive performance analysis with visualization\n",
    "- **Architecture Comparison**: Side-by-side model performance evaluation\n",
    "- **Validation Loops**: Custom validation logic with detailed metrics\n",
    "\n",
    "## Best Practices Established\n",
    "- Proper dataset handling for cross-validation scenarios\n",
    "- Model state management across folds\n",
    "- Statistical significance testing and confidence intervals\n",
    "- Memory-efficient data loading and model initialization\n",
    "\n",
    "## Key Benefits\n",
    "- **Robust Evaluation**: More reliable model performance estimation\n",
    "- **Model Selection**: Data-driven architecture choice\n",
    "- **Variance Analysis**: Understanding model stability across data splits\n",
    "- **Research Flexibility**: Framework for custom training experiments\n",
    "\n",
    "## Next Steps\n",
    "- Implement nested cross-validation for hyperparameter optimization\n",
    "- Add support for time series cross-validation\n",
    "- Integrate with hyperparameter optimization libraries\n",
    "- Extend to multi-dataset evaluation scenarios\n",
    "\n",
    "The custom loop architecture provides the foundation for sophisticated training strategies while maintaining Lightning's ease of use and best practices."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
