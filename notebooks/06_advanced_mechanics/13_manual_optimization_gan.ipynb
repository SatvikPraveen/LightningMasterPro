{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c9b0c5",
   "metadata": {},
   "source": [
    "# File Location: notebooks/06_advanced_mechanics/13_manual_optimization_gan.ipynb\n",
    "\n",
    "# Manual Optimization with GAN Implementation\n",
    "\n",
    "This notebook demonstrates advanced manual optimization techniques in PyTorch Lightning using a Generative Adversarial Network (GAN) as the primary example. We'll explore multi-optimizer setups, manual backward passes, and custom training loops.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement manual optimization in PyTorch Lightning\n",
    "- Build and train a GAN with separate optimizers\n",
    "- Use manual_backward() for custom gradient computation\n",
    "- Handle complex training dynamics with multiple loss functions\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Lightning version: {pl.__version__}\")\n",
    "```\n",
    "\n",
    "## 1. Understanding Manual Optimization\n",
    "\n",
    "```python\n",
    "class ManualOptimizationConcepts:\n",
    "    \"\"\"\n",
    "    Manual optimization concepts in PyTorch Lightning:\n",
    "    1. automatic_optimization = False\n",
    "    2. Multiple optimizers for different model components\n",
    "    3. manual_backward() for custom gradient computation\n",
    "    4. Optimizer stepping control\n",
    "    5. Learning rate scheduling management\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def explain_benefits():\n",
    "        benefits = {\n",
    "            \"Fine-grained Control\": \"Control when and how optimizers step\",\n",
    "            \"Multiple Models\": \"Different optimization strategies per model\",\n",
    "            \"Complex Losses\": \"Custom backward passes for complex loss functions\",\n",
    "            \"Gradient Manipulation\": \"Custom gradient clipping, accumulation, etc.\",\n",
    "            \"Research Flexibility\": \"Implement novel training algorithms\"\n",
    "        }\n",
    "        \n",
    "        for benefit, explanation in benefits.items():\n",
    "            print(f\"{benefit}: {explanation}\")\n",
    "\n",
    "ManualOptimizationConcepts.explain_benefits()\n",
    "```\n",
    "\n",
    "## 2. GAN Architecture Components\n",
    "\n",
    "```python\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Simple Generator for MNIST GAN\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=100, img_shape=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.img_shape = img_shape\n",
    "        self.img_size = int(np.prod(img_shape))\n",
    "        \n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, self.img_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Simple Discriminator for MNIST GAN\"\"\"\n",
    "    \n",
    "    def __init__(self, img_shape=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.img_size = int(np.prod(img_shape))\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.img_size, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "# Test the architectures\n",
    "latent_dim = 100\n",
    "generator = Generator(latent_dim)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "print(f\"Generator parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "```\n",
    "\n",
    "## 3. Manual Optimization GAN Implementation\n",
    "\n",
    "```python\n",
    "class ManualGAN(pl.LightningModule):\n",
    "    \"\"\"GAN with manual optimization for fine-grained control\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=100, lr=0.0002, b1=0.5, b2=0.999, img_shape=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Networks\n",
    "        self.generator = Generator(latent_dim, img_shape)\n",
    "        self.discriminator = Discriminator(img_shape)\n",
    "        \n",
    "        # Loss function\n",
    "        self.adversarial_loss = nn.BCELoss()\n",
    "        \n",
    "        # Important: Disable automatic optimization\n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "        # For logging\n",
    "        self.generated_imgs = None\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, _ = batch\n",
    "        \n",
    "        # Get optimizers (manually managed)\n",
    "        opt_g, opt_d = self.optimizers()\n",
    "        \n",
    "        # Sample noise for generator\n",
    "        z = torch.randn(imgs.shape[0], self.hparams.latent_dim, device=self.device)\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        self.toggle_optimizer(opt_g)\n",
    "        \n",
    "        # Generate fake images\n",
    "        fake_imgs = self(z)\n",
    "        \n",
    "        # Ground truth result (all fake)\n",
    "        # We want discriminator to think these are real\n",
    "        valid = torch.ones(imgs.size(0), 1, device=self.device)\n",
    "        \n",
    "        # Generator loss: fool the discriminator\n",
    "        g_loss = self.adversarial_loss(self.discriminator(fake_imgs), valid)\n",
    "        \n",
    "        # Manual backward pass for generator\n",
    "        self.manual_backward(g_loss)\n",
    "        opt_g.step()\n",
    "        opt_g.zero_grad()\n",
    "        self.untoggle_optimizer(opt_g)\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        self.toggle_optimizer(opt_d)\n",
    "        \n",
    "        # Real images\n",
    "        valid = torch.ones(imgs.size(0), 1, device=self.device)\n",
    "        real_loss = self.adversarial_loss(self.discriminator(imgs), valid)\n",
    "        \n",
    "        # Fake images (detach to avoid training generator)\n",
    "        fake = torch.zeros(imgs.size(0), 1, device=self.device)\n",
    "        fake_imgs = self(z).detach()  # Detach to stop gradients to generator\n",
    "        fake_loss = self.adversarial_loss(self.discriminator(fake_imgs), fake)\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        # Manual backward pass for discriminator\n",
    "        self.manual_backward(d_loss)\n",
    "        opt_d.step()\n",
    "        opt_d.zero_grad()\n",
    "        self.untoggle_optimizer(opt_d)\n",
    "        \n",
    "        # Logging\n",
    "        self.log('g_loss', g_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('d_loss', d_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Store generated images for visualization\n",
    "        if batch_idx == 0:\n",
    "            self.generated_imgs = fake_imgs[:16]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "        \n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        \n",
    "        return [opt_g, opt_d], []\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # Generate and log images\n",
    "        if self.generated_imgs is not None:\n",
    "            grid = make_grid(self.generated_imgs, nrow=4, normalize=True)\n",
    "            self.logger.experiment.add_image('generated_images', grid, self.current_epoch)\n",
    "\n",
    "# Initialize the model\n",
    "model = ManualGAN(latent_dim=100, lr=0.0002)\n",
    "```\n",
    "\n",
    "## 4. Advanced Manual Optimization Techniques\n",
    "\n",
    "```python\n",
    "class AdvancedManualGAN(ManualGAN):\n",
    "    \"\"\"Enhanced GAN with advanced manual optimization features\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Gradient penalties and clipping\n",
    "        self.gradient_penalty_lambda = 10.0\n",
    "        self.gradient_clip_val = 1.0\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        self.lr_decay_step = 50\n",
    "        self.lr_decay_factor = 0.5\n",
    "        \n",
    "        # Training balance\n",
    "        self.d_steps_per_g_step = 1\n",
    "        self.step_counter = 0\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, _ = batch\n",
    "        opt_g, opt_d = self.optimizers()\n",
    "        sch_g, sch_d = self.lr_schedulers()\n",
    "        \n",
    "        # Sample noise\n",
    "        z = torch.randn(imgs.shape[0], self.hparams.latent_dim, device=self.device)\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator (potentially multiple times)\n",
    "        # ---------------------\n",
    "        d_loss_total = 0\n",
    "        for _ in range(self.d_steps_per_g_step):\n",
    "            self.toggle_optimizer(opt_d)\n",
    "            \n",
    "            # Real and fake losses\n",
    "            valid = torch.ones(imgs.size(0), 1, device=self.device)\n",
    "            fake = torch.zeros(imgs.size(0), 1, device=self.device)\n",
    "            \n",
    "            real_loss = self.adversarial_loss(self.discriminator(imgs), valid)\n",
    "            fake_imgs = self(z).detach()\n",
    "            fake_loss = self.adversarial_loss(self.discriminator(fake_imgs), fake)\n",
    "            \n",
    "            # Gradient penalty (for WGAN-GP style training)\n",
    "            gp = self.compute_gradient_penalty(imgs, fake_imgs)\n",
    "            \n",
    "            d_loss = (real_loss + fake_loss) / 2 + self.gradient_penalty_lambda * gp\n",
    "            d_loss_total += d_loss.item()\n",
    "            \n",
    "            # Manual backward with gradient clipping\n",
    "            self.manual_backward(d_loss)\n",
    "            \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.discriminator.parameters(), \n",
    "                self.gradient_clip_val\n",
    "            )\n",
    "            \n",
    "            opt_d.step()\n",
    "            opt_d.zero_grad()\n",
    "            self.untoggle_optimizer(opt_d)\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        self.toggle_optimizer(opt_g)\n",
    "        \n",
    "        fake_imgs = self(z)\n",
    "        valid = torch.ones(imgs.size(0), 1, device=self.device)\n",
    "        g_loss = self.adversarial_loss(self.discriminator(fake_imgs), valid)\n",
    "        \n",
    "        # Manual backward with gradient clipping\n",
    "        self.manual_backward(g_loss)\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            self.generator.parameters(), \n",
    "            self.gradient_clip_val\n",
    "        )\n",
    "        \n",
    "        opt_g.step()\n",
    "        opt_g.zero_grad()\n",
    "        self.untoggle_optimizer(opt_g)\n",
    "        \n",
    "        # Update learning rate schedulers\n",
    "        if self.step_counter % self.lr_decay_step == 0 and self.step_counter > 0:\n",
    "            sch_g.step()\n",
    "            sch_d.step()\n",
    "        \n",
    "        self.step_counter += 1\n",
    "        \n",
    "        # Logging\n",
    "        self.log('g_loss', g_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('d_loss', d_loss_total / self.d_steps_per_g_step, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('gradient_penalty', gp, on_step=True, on_epoch=True)\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            self.generated_imgs = fake_imgs[:16]\n",
    "    \n",
    "    def compute_gradient_penalty(self, real_samples, fake_samples):\n",
    "        \"\"\"Compute gradient penalty for improved training stability\"\"\"\n",
    "        alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=self.device)\n",
    "        \n",
    "        # Interpolate between real and fake samples\n",
    "        interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "        \n",
    "        # Get discriminator output for interpolated samples\n",
    "        d_interpolates = self.discriminator(interpolates)\n",
    "        \n",
    "        fake = torch.ones(d_interpolates.size(), device=self.device, requires_grad=False)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        \n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        \n",
    "        return gradient_penalty\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "        \n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        \n",
    "        # Learning rate schedulers\n",
    "        sch_g = torch.optim.lr_scheduler.StepLR(opt_g, step_size=self.lr_decay_step, gamma=self.lr_decay_factor)\n",
    "        sch_d = torch.optim.lr_scheduler.StepLR(opt_d, step_size=self.lr_decay_step, gamma=self.lr_decay_factor)\n",
    "        \n",
    "        return [opt_g, opt_d], [sch_g, sch_d]\n",
    "\n",
    "# Initialize advanced model\n",
    "advanced_model = AdvancedManualGAN(latent_dim=100, lr=0.0002)\n",
    "```\n",
    "\n",
    "## 5. Custom Training Loop with Manual Optimization\n",
    "\n",
    "```python\n",
    "class CustomTrainingGAN(pl.LightningModule):\n",
    "    \"\"\"GAN with completely custom training loop\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=100, lr=0.0002, img_shape=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.generator = Generator(latent_dim, img_shape)\n",
    "        self.discriminator = Discriminator(img_shape)\n",
    "        self.adversarial_loss = nn.BCELoss()\n",
    "        \n",
    "        # Disable automatic optimization\n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "        # Custom training state\n",
    "        self.training_state = {\n",
    "            'g_losses': [],\n",
    "            'd_losses': [],\n",
    "            'epoch_g_loss': 0.0,\n",
    "            'epoch_d_loss': 0.0,\n",
    "            'batch_count': 0\n",
    "        }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.custom_training_step(batch, batch_idx)\n",
    "    \n",
    "    def custom_training_step(self, batch, batch_idx):\n",
    "        \"\"\"Completely custom training step with full control\"\"\"\n",
    "        imgs, _ = batch\n",
    "        batch_size = imgs.size(0)\n",
    "        \n",
    "        # Get optimizers\n",
    "        opt_g, opt_d = self.optimizers()\n",
    "        \n",
    "        # ========================\n",
    "        # Custom Training Logic\n",
    "        # ========================\n",
    "        \n",
    "        # Phase 1: Warm-up discriminator (first 100 batches)\n",
    "        if self.global_step < 100:\n",
    "            d_loss = self.train_discriminator_only(imgs, opt_d)\n",
    "            self.log('d_loss', d_loss, on_step=True, prog_bar=True)\n",
    "            return\n",
    "        \n",
    "        # Phase 2: Alternating training with custom frequency\n",
    "        if batch_idx % 3 == 0:  # Train generator every 3rd batch\n",
    "            g_loss = self.train_generator_step(batch_size, opt_g)\n",
    "            self.training_state['g_losses'].append(g_loss.item())\n",
    "            self.training_state['epoch_g_loss'] += g_loss.item()\n",
    "            self.log('g_loss', g_loss, on_step=True, prog_bar=True)\n",
    "        \n",
    "        # Always train discriminator\n",
    "        d_loss = self.train_discriminator_step(imgs, batch_size, opt_d)\n",
    "        self.training_state['d_losses'].append(d_loss.item())\n",
    "        self.training_state['epoch_d_loss'] += d_loss.item()\n",
    "        self.log('d_loss', d_loss, on_step=True, prog_bar=True)\n",
    "        \n",
    "        self.training_state['batch_count'] += 1\n",
    "        \n",
    "        # Log custom metrics\n",
    "        if len(self.training_state['g_losses']) > 0:\n",
    "            avg_g_loss = np.mean(self.training_state['g_losses'][-10:])  # Last 10 batches\n",
    "            self.log('avg_g_loss_10', avg_g_loss, on_step=True)\n",
    "        \n",
    "        avg_d_loss = np.mean(self.training_state['d_losses'][-10:])\n",
    "        self.log('avg_d_loss_10', avg_d_loss, on_step=True)\n",
    "    \n",
    "    def train_discriminator_only(self, real_imgs, opt_d):\n",
    "        \"\"\"Warm-up phase: train only discriminator\"\"\"\n",
    "        self.toggle_optimizer(opt_d)\n",
    "        \n",
    "        batch_size = real_imgs.size(0)\n",
    "        z = torch.randn(batch_size, self.hparams.latent_dim, device=self.device)\n",
    "        \n",
    "        # Real loss\n",
    "        valid = torch.ones(batch_size, 1, device=self.device)\n",
    "        real_loss = self.adversarial_loss(self.discriminator(real_imgs), valid)\n",
    "        \n",
    "        # Fake loss\n",
    "        fake = torch.zeros(batch_size, 1, device=self.device)\n",
    "        fake_imgs = self.generator(z).detach()\n",
    "        fake_loss = self.adversarial_loss(self.discriminator(fake_imgs), fake)\n",
    "        \n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        self.manual_backward(d_loss)\n",
    "        opt_d.step()\n",
    "        opt_d.zero_grad()\n",
    "        self.untoggle_optimizer(opt_d)\n",
    "        \n",
    "        return d_loss\n",
    "    \n",
    "    def train_generator_step(self, batch_size, opt_g):\n",
    "        \"\"\"Custom generator training step\"\"\"\n",
    "        self.toggle_optimizer(opt_g)\n",
    "        \n",
    "        z = torch.randn(batch_size, self.hparams.latent_dim, device=self.device)\n",
    "        fake_imgs = self.generator(z)\n",
    "        valid = torch.ones(batch_size, 1, device=self.device)\n",
    "        \n",
    "        g_loss = self.adversarial_loss(self.discriminator(fake_imgs), valid)\n",
    "        \n",
    "        self.manual_backward(g_loss)\n",
    "        opt_g.step()\n",
    "        opt_g.zero_grad()\n",
    "        self.untoggle_optimizer(opt_g)\n",
    "        \n",
    "        return g_loss\n",
    "    \n",
    "    def train_discriminator_step(self, real_imgs, batch_size, opt_d):\n",
    "        \"\"\"Custom discriminator training step\"\"\"\n",
    "        self.toggle_optimizer(opt_d)\n",
    "        \n",
    "        z = torch.randn(batch_size, self.hparams.latent_dim, device=self.device)\n",
    "        \n",
    "        # Real loss\n",
    "        valid = torch.ones(batch_size, 1, device=self.device)\n",
    "        real_loss = self.adversarial_loss(self.discriminator(real_imgs), valid)\n",
    "        \n",
    "        # Fake loss\n",
    "        fake = torch.zeros(batch_size, 1, device=self.device)\n",
    "        fake_imgs = self.generator(z).detach()\n",
    "        fake_loss = self.adversarial_loss(self.discriminator(fake_imgs), fake)\n",
    "        \n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        self.manual_backward(d_loss)\n",
    "        opt_d.step()\n",
    "        opt_d.zero_grad()\n",
    "        self.untoggle_optimizer(opt_d)\n",
    "        \n",
    "        return d_loss\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Custom epoch end logic\"\"\"\n",
    "        # Calculate epoch averages\n",
    "        if self.training_state['batch_count'] > 0:\n",
    "            avg_g_loss = self.training_state['epoch_g_loss'] / max(1, len(self.training_state['g_losses']))\n",
    "            avg_d_loss = self.training_state['epoch_d_loss'] / self.training_state['batch_count']\n",
    "            \n",
    "            self.log('epoch_avg_g_loss', avg_g_loss)\n",
    "            self.log('epoch_avg_d_loss', avg_d_loss)\n",
    "            \n",
    "            # Reset epoch counters\n",
    "            self.training_state['epoch_g_loss'] = 0.0\n",
    "            self.training_state['epoch_d_loss'] = 0.0\n",
    "            self.training_state['batch_count'] = 0\n",
    "        \n",
    "        # Generate sample images\n",
    "        z = torch.randn(16, self.hparams.latent_dim, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            fake_imgs = self.generator(z)\n",
    "            grid = make_grid(fake_imgs, nrow=4, normalize=True)\n",
    "            self.logger.experiment.add_image('generated_images', grid, self.current_epoch)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "# Initialize custom training model\n",
    "custom_model = CustomTrainingGAN(latent_dim=100, lr=0.0002)\n",
    "```\n",
    "\n",
    "## 6. Data Module for GAN Training\n",
    "\n",
    "```python\n",
    "class GANDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Data module for GAN training\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size=64, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1] for tanh output\n",
    "        ])\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # Download MNIST\n",
    "        torchvision.datasets.MNIST('./data', train=True, download=True)\n",
    "        torchvision.datasets.MNIST('./data', train=False, download=True)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.mnist_train = torchvision.datasets.MNIST('./data', train=True, transform=self.transform)\n",
    "        self.mnist_test = torchvision.datasets.MNIST('./data', train=False, transform=self.transform)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "# Initialize data module\n",
    "data_module = GANDataModule(batch_size=64)\n",
    "```\n",
    "\n",
    "## 7. Training the Manual Optimization GAN\n",
    "\n",
    "```python\n",
    "# Configure trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    log_every_n_steps=50,\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=[\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            dirpath='./checkpoints/gan',\n",
    "            filename='manual-gan-{epoch:02d}',\n",
    "            save_top_k=3,\n",
    "            monitor='g_loss',\n",
    "            mode='min'\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train the model (choose one)\n",
    "print(\"Training Manual GAN with custom optimization...\")\n",
    "\n",
    "# Option 1: Basic manual optimization\n",
    "# trainer.fit(model, data_module)\n",
    "\n",
    "# Option 2: Advanced manual optimization\n",
    "# trainer.fit(advanced_model, data_module)\n",
    "\n",
    "# Option 3: Custom training loop\n",
    "trainer.fit(custom_model, data_module)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "```\n",
    "\n",
    "## 8. Evaluation and Visualization\n",
    "\n",
    "```python\n",
    "def evaluate_gan_samples(model, num_samples=64):\n",
    "    \"\"\"Generate and visualize GAN samples\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, model.hparams.latent_dim, device=model.device)\n",
    "        fake_images = model.generator(z)\n",
    "        \n",
    "        # Create grid\n",
    "        grid = make_grid(fake_images, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "        \n",
    "        # Convert to numpy for matplotlib\n",
    "        grid_np = grid.cpu().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.imshow(grid_np[:, :, 0], cmap='gray')\n",
    "        plt.title(f'Generated MNIST Samples (Epoch {model.current_epoch})')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    return fake_images\n",
    "\n",
    "# Generate samples\n",
    "if torch.cuda.is_available():\n",
    "    custom_model = custom_model.cuda()\n",
    "\n",
    "generated_samples = evaluate_gan_samples(custom_model, num_samples=64)\n",
    "```\n",
    "\n",
    "## 9. Manual Optimization Best Practices\n",
    "\n",
    "```python\n",
    "class ManualOptimizationBestPractices:\n",
    "    \"\"\"Best practices for manual optimization in PyTorch Lightning\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def demonstrate_best_practices():\n",
    "        practices = {\n",
    "            \"Optimizer Management\": [\n",
    "                \"Always use toggle_optimizer() and untoggle_optimizer()\",\n",
    "                \"Call optimizer.zero_grad() after each step\",\n",
    "                \"Use self.optimizers() to get optimizer list\"\n",
    "            ],\n",
    "            \"Gradient Management\": [\n",
    "                \"Use self.manual_backward() instead of loss.backward()\",\n",
    "                \"Implement gradient clipping for stability\",\n",
    "                \"Be careful with gradient accumulation\"\n",
    "            ],\n",
    "            \"Learning Rate Scheduling\": [\n",
    "                \"Handle scheduler stepping manually\",\n",
    "                \"Use self.lr_schedulers() to get scheduler list\",\n",
    "                \"Log learning rates for monitoring\"\n",
    "            ],\n",
    "            \"Multi-Model Training\": [\n",
    "                \"Use separate optimizers for different models\",\n",
    "                \"Balance training frequency between models\",\n",
    "                \"Monitor loss ratios for training stability\"\n",
    "            ],\n",
    "            \"Logging and Monitoring\": [\n",
    "                \"Use sync_dist=True for distributed training\",\n",
    "                \"Log all relevant metrics\",\n",
    "                \"Implement custom validation logic if needed\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for category, tips in practices.items():\n",
    "            print(f\"\\n{category}:\")\n",
    "            for i, tip in enumerate(tips, 1):\n",
    "                print(f\"  {i}. {tip}\")\n",
    "\n",
    "ManualOptimizationBestPractices.demonstrate_best_practices()\n",
    "```\n",
    "\n",
    "## 10. Common Issues and Debugging\n",
    "\n",
    "```python\n",
    "class ManualOptimizationDebugging:\n",
    "    \"\"\"Common issues and debugging techniques for manual optimization\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def common_issues():\n",
    "        issues = {\n",
    "            \"Gradient Accumulation\": \"Gradients not being cleared properly\",\n",
    "            \"Optimizer State\": \"Forgetting to toggle optimizers\",\n",
    "            \"Learning Rate\": \"Schedulers not stepping correctly\",\n",
    "            \"Loss Scaling\": \"Inconsistent loss computation across models\",\n",
    "            \"Memory Leaks\": \"Not detaching tensors when needed\"\n",
    "        }\n",
    "        \n",
    "        solutions = {\n",
    "            \"Gradient Accumulation\": \"Always call optimizer.zero_grad() after step\",\n",
    "            \"Optimizer State\": \"Use toggle_optimizer() and untoggle_optimizer()\",\n",
    "            \"Learning Rate\": \"Manually step schedulers when needed\",\n",
    "            \"Loss Scaling\": \"Normalize losses properly for fair comparison\",\n",
    "            \"Memory Leaks\": \"Use .detach() on tensors that shouldn't flow gradients\"\n",
    "        }\n",
    "        \n",
    "        print(\"Common Issues and Solutions:\")\n",
    "        for issue, description in issues.items():\n",
    "            print(f\"\\nIssue: {issue}\")\n",
    "            print(f\"  Description: {description}\")\n",
    "            print(f\"  Solution: {solutions[issue]}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def debugging_checklist():\n",
    "        checklist = [\n",
    "            \"Set automatic_optimization = False\",\n",
    "            \"Use manual_backward() instead of loss.backward()\",\n",
    "            \"Toggle optimizers before and after use\",\n",
    "            \"Clear gradients after each optimizer step\",\n",
    "            \"Handle schedulers manually\",\n",
    "            \"Log all relevant metrics\",\n",
    "            \"Use proper tensor detaching\",\n",
    "            \"Validate gradient flow\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nDebugging Checklist:\")\n",
    "        for i, item in enumerate(checklist, 1):\n",
    "            print(f\"{i}. {item}\")\n",
    "\n",
    "debugger = ManualOptimizationDebugging()\n",
    "debugger.common_issues()\n",
    "debugger.debugging_checklist()\n",
    "```\n",
    "\n",
    "# Summary\n",
    "\n",
    "This notebook demonstrated manual optimization techniques in PyTorch Lightning using GAN implementation as a comprehensive example. Key concepts covered:\n",
    "\n",
    "## Manual Optimization Fundamentals\n",
    "- **Disabled Automatic Optimization**: Setting `automatic_optimization = False`\n",
    "- **Multi-Optimizer Management**: Handling separate optimizers for generator and discriminator\n",
    "- **Manual Backward Passes**: Using `manual_backward()` for custom gradient computation\n",
    "- **Optimizer Control**: Fine-grained control over when and how optimizers step\n",
    "\n",
    "## Advanced Techniques Implemented\n",
    "- **Gradient Penalties**: Custom gradient penalty computation for training stability\n",
    "- **Gradient Clipping**: Manual gradient norm clipping\n",
    "- **Custom Training Schedules**: Alternating training frequencies between models\n",
    "- **Learning Rate Management**: Manual scheduler stepping and monitoring\n",
    "\n",
    "## GAN-Specific Optimizations\n",
    "- **Balanced Training**: Custom logic for generator vs discriminator training frequency\n",
    "- **Gradient Flow Control**: Proper use of detach() to control gradient flow\n",
    "- **Loss Monitoring**: Comprehensive logging of training dynamics\n",
    "- **Warm-up Strategies**: Initial discriminator-only training phase\n",
    "\n",
    "## Best Practices Established\n",
    "- Always use optimizer toggling mechanisms\n",
    "- Implement proper gradient management\n",
    "- Handle learning rate schedulers manually\n",
    "- Monitor training balance and stability\n",
    "- Use proper tensor detaching for memory efficiency\n",
    "\n",
    "## Next Steps\n",
    "- Implement more sophisticated GAN variants (WGAN, Progressive GAN)\n",
    "- Experiment with different manual optimization strategies\n",
    "- Apply manual optimization to other multi-model architectures\n",
    "- Explore advanced gradient manipulation techniques\n",
    "\n",
    "Manual optimization provides the flexibility needed for complex training scenarios while maintaining the benefits of PyTorch Lightning's infrastructure."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
