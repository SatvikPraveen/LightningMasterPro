{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96930baf",
   "metadata": {},
   "source": [
    "# Trainer Sanity Checks and Debugging Tools\n",
    "\n",
    "**File Location:** `notebooks/01_lightning_fundamentals/02_trainer_sanity_and_debug.ipynb`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook covers PyTorch Lightning's powerful debugging and development tools. Learn to use `fast_dev_run`, `overfit_batches`, `limit_train_batches`, and other Trainer flags that make development faster and debugging easier.\n",
    "\n",
    "## Fast Development Run\n",
    "\n",
    "### fast_dev_run - Quick Smoke Test\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Simple model for testing\n",
    "class DebugModel(pl.LightningModule):\n",
    "    def __init__(self, input_size=20, hidden_size=64, output_size=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        \n",
    "        # Add some debugging logs\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('batch_size', float(x.shape[0]))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Quick data setup\n",
    "def create_debug_data(num_samples=1000, input_size=20, num_classes=10, batch_size=32):\n",
    "    torch.manual_seed(42)\n",
    "    x = torch.randn(num_samples, input_size)\n",
    "    y = torch.randint(0, num_classes, (num_samples,))\n",
    "    dataset = TensorDataset(x, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = create_debug_data(1000, 20, 10, 32)\n",
    "val_loader = create_debug_data(200, 20, 10, 32)\n",
    "\n",
    "# fast_dev_run: Run 1 batch of train, val, test to check everything works\n",
    "print(\"=== Fast Dev Run ===\")\n",
    "model = DebugModel()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    fast_dev_run=True,  # This runs 1 batch through the entire pipeline\n",
    "    logger=False,\n",
    "    enable_checkpointing=False\n",
    ")\n",
    "\n",
    "print(\"Running fast development check (1 batch only)...\")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "print(\"âœ“ Fast dev run completed - no errors in the pipeline!\")\n",
    "\n",
    "# You can also specify number of batches\n",
    "trainer_5_batches = pl.Trainer(\n",
    "    fast_dev_run=5,  # Run 5 batches\n",
    "    logger=False,\n",
    "    enable_checkpointing=False\n",
    ")\n",
    "\n",
    "print(\"\\nRunning with 5 batches...\")\n",
    "trainer_5_batches.fit(model, train_loader, val_loader)\n",
    "print(\"âœ“ 5-batch dev run completed!\")\n",
    "```\n",
    "\n",
    "### overfit_batches - Check Model Capacity\n",
    "\n",
    "```python\n",
    "print(\"=== Overfit Batches Test ===\")\n",
    "\n",
    "# Test if model can overfit to a small subset - good for debugging model capacity\n",
    "model = DebugModel()\n",
    "\n",
    "trainer_overfit = pl.Trainer(\n",
    "    overfit_batches=2,  # Use only 2 batches for train AND val\n",
    "    max_epochs=50,      # More epochs to see overfitting\n",
    "    logger=False,\n",
    "    enable_checkpointing=False,\n",
    "    log_every_n_steps=1  # Log every step to see progress\n",
    ")\n",
    "\n",
    "print(\"Training on only 2 batches for 50 epochs...\")\n",
    "print(\"Model should overfit and reach very low loss\")\n",
    "trainer_overfit.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Check if model actually overfitted\n",
    "final_loss = trainer_overfit.logged_metrics.get('train_loss', float('inf'))\n",
    "print(f\"Final training loss: {final_loss:.6f}\")\n",
    "if final_loss < 0.1:\n",
    "    print(\"âœ“ Model successfully overfitted - has sufficient capacity\")\n",
    "else:\n",
    "    print(\"âš  Model may have insufficient capacity or learning rate issues\")\n",
    "```\n",
    "\n",
    "### Limiting Data - Control Training Size\n",
    "\n",
    "```python\n",
    "print(\"=== Limiting Training Data ===\")\n",
    "\n",
    "# limit_train_batches: Use only subset of training data\n",
    "model = DebugModel()\n",
    "\n",
    "trainer_limited = pl.Trainer(\n",
    "    limit_train_batches=0.3,  # Use only 30% of training data\n",
    "    limit_val_batches=0.5,    # Use only 50% of validation data\n",
    "    max_epochs=3,\n",
    "    logger=False,\n",
    "    enable_checkpointing=False\n",
    ")\n",
    "\n",
    "print(\"Training with 30% of train data and 50% of val data...\")\n",
    "trainer_limited.fit(model, train_loader, val_loader)\n",
    "print(\"âœ“ Limited data training completed\")\n",
    "\n",
    "# Can also specify exact number of batches\n",
    "trainer_exact = pl.Trainer(\n",
    "    limit_train_batches=10,   # Use exactly 10 training batches\n",
    "    limit_val_batches=5,      # Use exactly 5 validation batches\n",
    "    max_epochs=2,\n",
    "    logger=False,\n",
    "    enable_checkpointing=False\n",
    ")\n",
    "\n",
    "print(\"\\nTraining with exactly 10 train batches and 5 val batches...\")\n",
    "trainer_exact.fit(model, train_loader, val_loader)\n",
    "print(\"âœ“ Exact batch count training completed\")\n",
    "```\n",
    "\n",
    "## Advanced Debugging Features\n",
    "\n",
    "### Gradient and Loss Debugging\n",
    "\n",
    "```python\n",
    "print(\"=== Gradient and Loss Debugging ===\")\n",
    "\n",
    "class DebugAdvancedModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(20, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        \n",
    "        # Advanced logging for debugging\n",
    "        self.log('train_loss', loss)\n",
    "        \n",
    "        # Log gradient norms (useful for debugging)\n",
    "        if batch_idx % 10 == 0:  # Every 10 batches\n",
    "            for name, param in self.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_norm = param.grad.data.norm(2)\n",
    "                    self.log(f'grad_norm/{name}', grad_norm, on_step=True, on_epoch=False)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        \n",
    "        # Log predictions distribution for debugging\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        unique_preds = torch.unique(preds, return_counts=True)\n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        self.log('unique_predictions', float(len(unique_preds[0])))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "model = DebugAdvancedModel()\n",
    "\n",
    "# Enable gradient clipping and track gradients\n",
    "trainer_debug = pl.Trainer(\n",
    "    max_epochs=2,\n",
    "    limit_train_batches=20,\n",
    "    limit_val_batches=5,\n",
    "    gradient_clip_val=1.0,      # Clip gradients to prevent explosion\n",
    "    track_grad_norm=2,          # Track L2 norm of gradients  \n",
    "    log_every_n_steps=5,\n",
    "    logger=False,\n",
    "    enable_checkpointing=False\n",
    ")\n",
    "\n",
    "print(\"Training with gradient tracking and clipping...\")\n",
    "trainer_debug.fit(model, train_loader, val_loader)\n",
    "print(\"âœ“ Gradient debugging completed\")\n",
    "```\n",
    "\n",
    "### Detecting Anomalies\n",
    "\n",
    "```python\n",
    "print(\"=== Anomaly Detection ===\")\n",
    "\n",
    "# Enable anomaly detection to catch NaN/Inf values\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class PotentiallyBuggyModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(20, 10)\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        # Introduce potential numerical issues for demonstration\n",
    "        self.step_count += 1\n",
    "        if self.step_count == 15:  # Introduce NaN at step 15\n",
    "            print(\"âš  Introducing numerical instability for demo...\")\n",
    "            loss = torch.tensor(float('nan'))\n",
    "        else:\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "model = PotentiallyBuggyModel()\n",
    "\n",
    "trainer_anomaly = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    limit_train_batches=20,\n",
    "    detect_anomaly=True,       # This will catch anomalies\n",
    "    logger=False,\n",
    "    enable_checkpointing=False\n",
    ")\n",
    "\n",
    "print(\"Testing anomaly detection (will catch NaN)...\")\n",
    "try:\n",
    "    trainer_anomaly.fit(model, train_loader)\n",
    "except Exception as e:\n",
    "    print(f\"âœ“ Anomaly detected: {type(e).__name__}\")\n",
    "    print(\"This is expected - anomaly detection caught the NaN!\")\n",
    "\n",
    "# Reset anomaly detection\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "```\n",
    "\n",
    "### Profiling Performance\n",
    "\n",
    "```python\n",
    "print(\"=== Performance Profiling ===\")\n",
    "\n",
    "model = DebugModel()\n",
    "\n",
    "# Enable profiler to identify bottlenecks\n",
    "from pytorch_lightning.profilers import SimpleProfiler, PyTorchProfiler\n",
    "\n",
    "# Simple profiler - basic timing\n",
    "simple_profiler = SimpleProfiler(dirpath=\".\", filename=\"simple_profile\")\n",
    "\n",
    "trainer_profile = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    limit_train_batches=10,\n",
    "    limit_val_batches=3,\n",
    "    profiler=simple_profiler,\n",
    "    logger=False,\n",
    "    enable_checkpointing=False\n",
    ")\n",
    "\n",
    "print(\"Running with simple profiler...\")\n",
    "trainer_profile.fit(model, train_loader, val_loader)\n",
    "print(\"âœ“ Profiling completed - check simple_profile.txt for timing info\")\n",
    "\n",
    "# Advanced PyTorch profiler (more detailed)\n",
    "pytorch_profiler = PyTorchProfiler(\n",
    "    dirpath=\".\",\n",
    "    filename=\"pytorch_profile\",\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ] if torch.cuda.is_available() else [torch.profiler.ProfilerActivity.CPU]\n",
    ")\n",
    "\n",
    "trainer_advanced_profile = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    limit_train_batches=5,\n",
    "    profiler=pytorch_profiler,\n",
    "    logger=False,\n",
    "    enable_checkpointing=False\n",
    ")\n",
    "\n",
    "print(\"Running with PyTorch profiler...\")\n",
    "trainer_advanced_profile.fit(model, train_loader, val_loader)\n",
    "print(\"âœ“ Advanced profiling completed\")\n",
    "```\n",
    "\n",
    "## Development Workflow Best Practices\n",
    "\n",
    "### Complete Debug Workflow\n",
    "\n",
    "```python\n",
    "print(\"=== Complete Development Workflow ===\")\n",
    "\n",
    "def debug_model_pipeline(model, train_loader, val_loader):\n",
    "    \"\"\"Complete debugging pipeline for new models\"\"\"\n",
    "    \n",
    "    print(\"Step 1: Fast dev run - Check basic functionality\")\n",
    "    trainer = pl.Trainer(fast_dev_run=True, logger=False, enable_checkpointing=False)\n",
    "    try:\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        print(\"âœ“ Pipeline working\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Pipeline error: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"\\nStep 2: Overfit test - Check model capacity\")\n",
    "    trainer = pl.Trainer(\n",
    "        overfit_batches=1, \n",
    "        max_epochs=20, \n",
    "        logger=False, \n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    final_loss = trainer.logged_metrics.get('train_loss', float('inf'))\n",
    "    \n",
    "    if final_loss < 0.1:\n",
    "        print(f\"âœ“ Model can overfit (final loss: {final_loss:.4f})\")\n",
    "    else:\n",
    "        print(f\"âš  Model may have issues (final loss: {final_loss:.4f})\")\n",
    "    \n",
    "    print(\"\\nStep 3: Limited data run - Check training stability\")\n",
    "    trainer = pl.Trainer(\n",
    "        limit_train_batches=10,\n",
    "        limit_val_batches=3,\n",
    "        max_epochs=3,\n",
    "        logger=False,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    print(\"âœ“ Limited training completed\")\n",
    "    \n",
    "    print(\"\\nStep 4: Full training ready!\")\n",
    "    return True\n",
    "\n",
    "# Test the workflow\n",
    "model = DebugModel()\n",
    "success = debug_model_pipeline(model, train_loader, val_loader)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nðŸŽ‰ Model passed all debugging checks - ready for full training!\")\n",
    "else:\n",
    "    print(\"\\nâŒ Model needs fixes before full training\")\n",
    "```\n",
    "\n",
    "### Debugging Checklist\n",
    "\n",
    "```python\n",
    "print(\"=== Debugging Checklist ===\")\n",
    "\n",
    "checklist = {\n",
    "    \"fast_dev_run\": \"Quick smoke test - does the pipeline work?\",\n",
    "    \"overfit_batches\": \"Can the model learn? (overfit small data)\",\n",
    "    \"limit_train_batches\": \"Does training work with limited data?\",\n",
    "    \"gradient_clip_val\": \"Are gradients exploding?\",\n",
    "    \"track_grad_norm\": \"Monitor gradient health\",\n",
    "    \"detect_anomaly\": \"Catch NaN/Inf values early\", \n",
    "    \"profiler\": \"Identify performance bottlenecks\",\n",
    "    \"log_every_n_steps\": \"Monitor training frequently during debug\"\n",
    "}\n",
    "\n",
    "print(\"Development debugging workflow:\")\n",
    "for i, (flag, description) in enumerate(checklist.items(), 1):\n",
    "    print(f\"{i}. {flag}: {description}\")\n",
    "\n",
    "print(\"\\nCommon debugging trainer configurations:\")\n",
    "print(\"\"\"\n",
    "# Quick development check:\n",
    "trainer = pl.Trainer(fast_dev_run=True)\n",
    "\n",
    "# Overfit test:\n",
    "trainer = pl.Trainer(overfit_batches=5, max_epochs=50)\n",
    "\n",
    "# Limited data debugging:\n",
    "trainer = pl.Trainer(limit_train_batches=0.1, limit_val_batches=0.2, max_epochs=3)\n",
    "\n",
    "# Full debugging mode:\n",
    "trainer = pl.Trainer(\n",
    "    limit_train_batches=50,\n",
    "    gradient_clip_val=1.0,\n",
    "    track_grad_norm=2,\n",
    "    detect_anomaly=True,\n",
    "    profiler=\"simple\"\n",
    ")\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered essential debugging and development tools in PyTorch Lightning:\n",
    "\n",
    "1. **fast_dev_run**: Quick smoke test with minimal batches to verify pipeline integrity\n",
    "2. **overfit_batches**: Test model capacity by overfitting to small data subset\n",
    "3. **limit_*_batches**: Control training data size for faster iteration during development\n",
    "4. **Gradient debugging**: Track gradient norms and clip values to prevent instability\n",
    "5. **Anomaly detection**: Catch NaN/Inf values early in development\n",
    "6. **Profiling**: Identify performance bottlenecks in your training pipeline\n",
    "\n",
    "Key development workflow:\n",
    "1. Start with `fast_dev_run=True` to catch basic errors\n",
    "2. Use `overfit_batches` to verify model can learn\n",
    "3. Test with `limit_train_batches` for quick iterations\n",
    "4. Add gradient monitoring for numerical stability\n",
    "5. Profile performance before scaling up\n",
    "\n",
    "These tools dramatically speed up development by catching issues early and enabling rapid iteration on model architectures and hyperparameters.\n",
    "\n",
    "Next notebook: We'll explore LightningCLI for config-driven experiments."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
