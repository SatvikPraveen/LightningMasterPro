{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55425951",
   "metadata": {},
   "source": [
    "# LightningCLI and Config-First Experiments\n",
    "\n",
    "**File Location:** `notebooks/01_lightning_fundamentals/03_lightningcli_config_runs.ipynb`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook introduces LightningCLI for config-driven experiments. Learn how to structure experiments using YAML configurations, enabling reproducible research and easy hyperparameter management without changing code.\n",
    "\n",
    "## Basic LightningCLI Setup\n",
    "\n",
    "### Creating CLI-Compatible Components\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_lightning.cli import LightningCLI\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "class ConfigurableModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int = 784,\n",
    "        hidden_size: int = 128, \n",
    "        num_classes: int = 10,\n",
    "        learning_rate: float = 1e-3,\n",
    "        optimizer: str = \"adam\",\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams.optimizer == \"adam\":\n",
    "            return Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        elif self.hparams.optimizer == \"sgd\":\n",
    "            return SGD(self.parameters(), lr=self.hparams.learning_rate, momentum=0.9)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {self.hparams.optimizer}\")\n",
    "\n",
    "class ConfigurableDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_samples: int = 1000,\n",
    "        input_size: int = 784,\n",
    "        num_classes: int = 10,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        torch.manual_seed(42)  # For reproducibility\n",
    "        \n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # Training data\n",
    "            self.train_x = torch.randn(self.hparams.num_samples, self.hparams.input_size)\n",
    "            self.train_y = torch.randint(0, self.hparams.num_classes, (self.hparams.num_samples,))\n",
    "            \n",
    "            # Validation data\n",
    "            val_size = self.hparams.num_samples // 5\n",
    "            self.val_x = torch.randn(val_size, self.hparams.input_size)\n",
    "            self.val_y = torch.randint(0, self.hparams.num_classes, (val_size,))\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = TensorDataset(self.train_x, self.train_y)\n",
    "        return DataLoader(\n",
    "            dataset, \n",
    "            batch_size=self.hparams.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=self.hparams.num_workers\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = TensorDataset(self.val_x, self.val_y)\n",
    "        return DataLoader(\n",
    "            dataset, \n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers\n",
    "        )\n",
    "\n",
    "print(\"âœ“ CLI-compatible model and datamodule created\")\n",
    "```\n",
    "\n",
    "### Creating Configuration Files\n",
    "\n",
    "```python\n",
    "# Create temporary directory for configs\n",
    "import os\n",
    "config_dir = Path(\"temp_configs\")\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Basic configuration\n",
    "basic_config = {\n",
    "    \"model\": {\n",
    "        \"class_path\": \"__main__.ConfigurableModel\",\n",
    "        \"init_args\": {\n",
    "            \"input_size\": 784,\n",
    "            \"hidden_size\": 128,\n",
    "            \"num_classes\": 10,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"dropout\": 0.2\n",
    "        }\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"class_path\": \"__main__.ConfigurableDataModule\", \n",
    "        \"init_args\": {\n",
    "            \"num_samples\": 1000,\n",
    "            \"input_size\": 784,\n",
    "            \"num_classes\": 10,\n",
    "            \"batch_size\": 32,\n",
    "            \"num_workers\": 0\n",
    "        }\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"max_epochs\": 3,\n",
    "        \"enable_checkpointing\": False,\n",
    "        \"logger\": False,\n",
    "        \"enable_progress_bar\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save basic config\n",
    "with open(config_dir / \"basic_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(basic_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"âœ“ Basic configuration created\")\n",
    "print(f\"Config saved to: {config_dir / 'basic_config.yaml'}\")\n",
    "\n",
    "# Display the config\n",
    "with open(config_dir / \"basic_config.yaml\", \"r\") as f:\n",
    "    print(\"\\n--- Basic Config Content ---\")\n",
    "    print(f.read())\n",
    "```\n",
    "\n",
    "### High Performance Configuration\n",
    "\n",
    "```python\n",
    "# High performance configuration\n",
    "high_perf_config = {\n",
    "    \"model\": {\n",
    "        \"class_path\": \"__main__.ConfigurableModel\",\n",
    "        \"init_args\": {\n",
    "            \"input_size\": 784,\n",
    "            \"hidden_size\": 256,  # Larger model\n",
    "            \"num_classes\": 10,\n",
    "            \"learning_rate\": 0.003,  # Higher learning rate\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"dropout\": 0.1  # Less dropout\n",
    "        }\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"class_path\": \"__main__.ConfigurableDataModule\",\n",
    "        \"init_args\": {\n",
    "            \"num_samples\": 2000,  # More data\n",
    "            \"input_size\": 784,\n",
    "            \"num_classes\": 10,\n",
    "            \"batch_size\": 64,  # Larger batches\n",
    "            \"num_workers\": 2\n",
    "        }\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"max_epochs\": 5,\n",
    "        \"enable_checkpointing\": False,\n",
    "        \"logger\": False,\n",
    "        \"gradient_clip_val\": 1.0,  # Gradient clipping\n",
    "        \"enable_progress_bar\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save high performance config\n",
    "with open(config_dir / \"high_perf_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(high_perf_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"âœ“ High performance configuration created\")\n",
    "```\n",
    "\n",
    "### Debugging Configuration\n",
    "\n",
    "```python\n",
    "# Debugging configuration\n",
    "debug_config = {\n",
    "    \"model\": {\n",
    "        \"class_path\": \"__main__.ConfigurableModel\",\n",
    "        \"init_args\": {\n",
    "            \"input_size\": 784,\n",
    "            \"hidden_size\": 64,  # Smaller for faster debugging\n",
    "            \"num_classes\": 10,\n",
    "            \"learning_rate\": 0.01,  # Higher LR for faster convergence\n",
    "            \"optimizer\": \"sgd\",\n",
    "            \"dropout\": 0.0  # No dropout for debugging\n",
    "        }\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"class_path\": \"__main__.ConfigurableDataModule\",\n",
    "        \"init_args\": {\n",
    "            \"num_samples\": 500,  # Less data\n",
    "            \"input_size\": 784,\n",
    "            \"num_classes\": 10,\n",
    "            \"batch_size\": 16,  # Smaller batches\n",
    "            \"num_workers\": 0\n",
    "        }\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"fast_dev_run\": True,  # Debug mode\n",
    "        \"enable_checkpointing\": False,\n",
    "        \"logger\": False,\n",
    "        \"detect_anomaly\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save debug config\n",
    "with open(config_dir / \"debug_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(debug_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"âœ“ Debug configuration created\")\n",
    "```\n",
    "\n",
    "## Running with Configurations\n",
    "\n",
    "### Manual CLI Usage (Programmatic)\n",
    "\n",
    "```python\n",
    "# Function to run training with config\n",
    "def run_with_config(config_path):\n",
    "    \"\"\"Run training using a configuration file\"\"\"\n",
    "    \n",
    "    # Custom CLI class for notebook usage\n",
    "    class NotebookCLI(LightningCLI):\n",
    "        def __init__(self, config_path):\n",
    "            # Override sys.argv for notebook\n",
    "            import sys\n",
    "            original_argv = sys.argv.copy()\n",
    "            sys.argv = [\"notebook\", \"--config\", str(config_path)]\n",
    "            \n",
    "            try:\n",
    "                super().__init__(\n",
    "                    model_class=ConfigurableModel,\n",
    "                    datamodule_class=ConfigurableDataModule,\n",
    "                    run=True,  # Run training immediately\n",
    "                    save_config_callback=None,  # Don't save config\n",
    "                )\n",
    "            finally:\n",
    "                sys.argv = original_argv  # Restore original argv\n",
    "    \n",
    "    print(f\"Running with config: {config_path}\")\n",
    "    try:\n",
    "        cli = NotebookCLI(config_path)\n",
    "        print(\"âœ“ Training completed successfully\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Training failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run with different configurations\n",
    "print(\"=== Running Debug Configuration ===\")\n",
    "success = run_with_config(config_dir / \"debug_config.yaml\")\n",
    "\n",
    "if success:\n",
    "    print(\"\\n=== Running Basic Configuration ===\")\n",
    "    run_with_config(config_dir / \"basic_config.yaml\")\n",
    "```\n",
    "\n",
    "### Configuration Inheritance and Overrides\n",
    "\n",
    "```python\n",
    "# Create a base configuration\n",
    "base_config = {\n",
    "    \"model\": {\n",
    "        \"class_path\": \"__main__.ConfigurableModel\",\n",
    "        \"init_args\": {\n",
    "            \"input_size\": 784,\n",
    "            \"hidden_size\": 128,\n",
    "            \"num_classes\": 10,\n",
    "            \"dropout\": 0.2\n",
    "        }\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"class_path\": \"__main__.ConfigurableDataModule\",\n",
    "        \"init_args\": {\n",
    "            \"num_samples\": 1000,\n",
    "            \"input_size\": 784,\n",
    "            \"num_classes\": 10,\n",
    "            \"batch_size\": 32\n",
    "        }\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"max_epochs\": 3,\n",
    "        \"enable_checkpointing\": False,\n",
    "        \"logger\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save base config\n",
    "with open(config_dir / \"base_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(base_config, f, default_flow_style=False)\n",
    "\n",
    "# Create experiment configs that override base settings\n",
    "experiments = {\n",
    "    \"adam_experiment\": {\n",
    "        \"model\": {\"init_args\": {\"learning_rate\": 0.001, \"optimizer\": \"adam\"}},\n",
    "        \"trainer\": {\"max_epochs\": 5}\n",
    "    },\n",
    "    \"sgd_experiment\": {\n",
    "        \"model\": {\"init_args\": {\"learning_rate\": 0.01, \"optimizer\": \"sgd\"}},\n",
    "        \"trainer\": {\"max_epochs\": 5}\n",
    "    },\n",
    "    \"large_model\": {\n",
    "        \"model\": {\"init_args\": {\"hidden_size\": 256, \"learning_rate\": 0.001}},\n",
    "        \"data\": {\"init_args\": {\"batch_size\": 64}},\n",
    "        \"trainer\": {\"max_epochs\": 4}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save experiment configs\n",
    "for exp_name, exp_config in experiments.items():\n",
    "    # Deep merge with base config\n",
    "    import copy\n",
    "    full_config = copy.deepcopy(base_config)\n",
    "    \n",
    "    # Simple merge (for demonstration)\n",
    "    for section, params in exp_config.items():\n",
    "        if section in full_config:\n",
    "            if \"init_args\" in params:\n",
    "                full_config[section][\"init_args\"].update(params[\"init_args\"])\n",
    "            else:\n",
    "                full_config[section].update(params)\n",
    "    \n",
    "    with open(config_dir / f\"{exp_name}.yaml\", \"w\") as f:\n",
    "        yaml.dump(full_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"âœ“ Experiment configurations created:\")\n",
    "for exp_name in experiments.keys():\n",
    "    print(f\"  - {exp_name}.yaml\")\n",
    "```\n",
    "\n",
    "### Hyperparameter Sweeps via Config\n",
    "\n",
    "```python\n",
    "# Create multiple configs for hyperparameter sweep\n",
    "sweep_configs = []\n",
    "\n",
    "learning_rates = [0.001, 0.003, 0.01]\n",
    "hidden_sizes = [64, 128, 256]\n",
    "optimizers = [\"adam\", \"sgd\"]\n",
    "\n",
    "experiment_id = 0\n",
    "for lr in learning_rates:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for optimizer in optimizers:\n",
    "            experiment_id += 1\n",
    "            \n",
    "            config = {\n",
    "                \"model\": {\n",
    "                    \"class_path\": \"__main__.ConfigurableModel\",\n",
    "                    \"init_args\": {\n",
    "                        \"input_size\": 784,\n",
    "                        \"hidden_size\": hidden_size,\n",
    "                        \"num_classes\": 10,\n",
    "                        \"learning_rate\": lr,\n",
    "                        \"optimizer\": optimizer,\n",
    "                        \"dropout\": 0.2\n",
    "                    }\n",
    "                },\n",
    "                \"data\": {\n",
    "                    \"class_path\": \"__main__.ConfigurableDataModule\",\n",
    "                    \"init_args\": {\n",
    "                        \"num_samples\": 1000,\n",
    "                        \"input_size\": 784,\n",
    "                        \"num_classes\": 10,\n",
    "                        \"batch_size\": 32\n",
    "                    }\n",
    "                },\n",
    "                \"trainer\": {\n",
    "                    \"max_epochs\": 3,\n",
    "                    \"enable_checkpointing\": False,\n",
    "                    \"logger\": False,\n",
    "                    \"enable_progress_bar\": False  # Reduce output for sweep\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            config_name = f\"sweep_{experiment_id:02d}_lr{lr}_h{hidden_size}_{optimizer}.yaml\"\n",
    "            with open(config_dir / config_name, \"w\") as f:\n",
    "                yaml.dump(config, f, default_flow_style=False)\n",
    "            \n",
    "            sweep_configs.append((config_name, lr, hidden_size, optimizer))\n",
    "\n",
    "print(f\"âœ“ Created {len(sweep_configs)} sweep configurations\")\n",
    "print(\"First few experiments:\")\n",
    "for i, (name, lr, hs, opt) in enumerate(sweep_configs[:3]):\n",
    "    print(f\"  {name}: lr={lr}, hidden={hs}, opt={opt}\")\n",
    "```\n",
    "\n",
    "### Configuration Analysis and Comparison\n",
    "\n",
    "```python\n",
    "# Function to analyze and compare configs\n",
    "def analyze_config(config_path):\n",
    "    \"\"\"Analyze a configuration file\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    analysis = {\n",
    "        'model_params': config['model']['init_args'],\n",
    "        'data_params': config['data']['init_args'],\n",
    "        'trainer_params': config['trainer']\n",
    "    }\n",
    "    return analysis\n",
    "\n",
    "def compare_configs(config_paths):\n",
    "    \"\"\"Compare multiple configurations\"\"\"\n",
    "    analyses = {}\n",
    "    for path in config_paths:\n",
    "        name = path.stem\n",
    "        analyses[name] = analyze_config(path)\n",
    "    \n",
    "    return analyses\n",
    "\n",
    "# Compare some configurations\n",
    "configs_to_compare = [\n",
    "    config_dir / \"basic_config.yaml\",\n",
    "    config_dir / \"high_perf_config.yaml\", \n",
    "    config_dir / \"debug_config.yaml\"\n",
    "]\n",
    "\n",
    "comparisons = compare_configs(configs_to_compare)\n",
    "\n",
    "print(\"=== Configuration Comparison ===\")\n",
    "for config_name, analysis in comparisons.items():\n",
    "    print(f\"\\n{config_name.upper()}:\")\n",
    "    print(f\"  Model: hidden_size={analysis['model_params']['hidden_size']}, \"\n",
    "          f\"lr={analysis['model_params']['learning_rate']}, \"\n",
    "          f\"optimizer={analysis['model_params']['optimizer']}\")\n",
    "    print(f\"  Data: samples={analysis['data_params']['num_samples']}, \"\n",
    "          f\"batch_size={analysis['data_params']['batch_size']}\")\n",
    "    \n",
    "    trainer_info = []\n",
    "    if 'fast_dev_run' in analysis['trainer_params']:\n",
    "        trainer_info.append(\"fast_dev_run=True\")\n",
    "    else:\n",
    "        trainer_info.append(f\"max_epochs={analysis['trainer_params']['max_epochs']}\")\n",
    "    \n",
    "    if 'gradient_clip_val' in analysis['trainer_params']:\n",
    "        trainer_info.append(f\"grad_clip={analysis['trainer_params']['gradient_clip_val']}\")\n",
    "    \n",
    "    print(f\"  Trainer: {', '.join(trainer_info)}\")\n",
    "```\n",
    "\n",
    "## Advanced CLI Features\n",
    "\n",
    "### Config Validation and Schema\n",
    "\n",
    "```python\n",
    "# Create a configuration with validation\n",
    "def validate_config(config_path):\n",
    "    \"\"\"Basic config validation\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    # Check required sections\n",
    "    required_sections = ['model', 'data', 'trainer']\n",
    "    for section in required_sections:\n",
    "        if section not in config:\n",
    "            errors.append(f\"Missing required section: {section}\")\n",
    "    \n",
    "    # Validate model params\n",
    "    if 'model' in config:\n",
    "        model_args = config['model'].get('init_args', {})\n",
    "        if model_args.get('learning_rate', 0) <= 0:\n",
    "            errors.append(\"learning_rate must be positive\")\n",
    "        if model_args.get('hidden_size', 0) <= 0:\n",
    "            errors.append(\"hidden_size must be positive\")\n",
    "    \n",
    "    # Validate data params  \n",
    "    if 'data' in config:\n",
    "        data_args = config['data'].get('init_args', {})\n",
    "        if data_args.get('batch_size', 0) <= 0:\n",
    "            errors.append(\"batch_size must be positive\")\n",
    "        if data_args.get('num_samples', 0) <= 0:\n",
    "            errors.append(\"num_samples must be positive\")\n",
    "    \n",
    "    return errors\n",
    "\n",
    "# Test validation\n",
    "print(\"=== Configuration Validation ===\")\n",
    "for config_file in [\n",
    "    \"basic_config.yaml\",\n",
    "    \"high_perf_config.yaml\", \n",
    "    \"debug_config.yaml\"\n",
    "]:\n",
    "    config_path = config_dir / config_file\n",
    "    errors = validate_config(config_path)\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"âŒ {config_file}: {len(errors)} errors\")\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "    else:\n",
    "        print(f\"âœ… {config_file}: Valid\")\n",
    "```\n",
    "\n",
    "### Config-Based Experiment Tracking\n",
    "\n",
    "```python\n",
    "# Create experiment tracking system\n",
    "class ExperimentTracker:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    def run_experiment(self, config_path, experiment_name):\n",
    "        \"\"\"Run experiment and track results\"\"\"\n",
    "        print(f\"Running experiment: {experiment_name}\")\n",
    "        \n",
    "        # Load config for reference\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        \n",
    "        # Extract key parameters\n",
    "        model_params = config['model']['init_args']\n",
    "        data_params = config['data']['init_args']\n",
    "        \n",
    "        # For demonstration, simulate results\n",
    "        import random\n",
    "        random.seed(hash(experiment_name) % 1000)\n",
    "        \n",
    "        # Simulate training results based on config\n",
    "        base_acc = 0.7\n",
    "        lr_factor = min(model_params['learning_rate'] * 100, 1.0)\n",
    "        size_factor = min(model_params['hidden_size'] / 256, 1.0)\n",
    "        \n",
    "        final_acc = base_acc + lr_factor * 0.1 + size_factor * 0.15 + random.uniform(-0.05, 0.05)\n",
    "        final_loss = max(0.1, 1.0 - final_acc + random.uniform(-0.1, 0.1))\n",
    "        \n",
    "        # Store results\n",
    "        self.results[experiment_name] = {\n",
    "            'config': config_path.stem,\n",
    "            'final_accuracy': round(final_acc, 4),\n",
    "            'final_loss': round(final_loss, 4),\n",
    "            'learning_rate': model_params['learning_rate'],\n",
    "            'hidden_size': model_params['hidden_size'],\n",
    "            'optimizer': model_params['optimizer'],\n",
    "            'batch_size': data_params['batch_size']\n",
    "        }\n",
    "        \n",
    "        print(f\"  Results: acc={final_acc:.4f}, loss={final_loss:.4f}\")\n",
    "        return self.results[experiment_name]\n",
    "    \n",
    "    def summarize_results(self):\n",
    "        \"\"\"Summarize all experiment results\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No experiments run yet\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n=== Experiment Results Summary ===\")\n",
    "        print(f\"{'Experiment':<20} {'Accuracy':<10} {'Loss':<8} {'LR':<8} {'Hidden':<8} {'Optimizer':<10}\")\n",
    "        print(\"-\" * 75)\n",
    "        \n",
    "        sorted_results = sorted(self.results.items(), \n",
    "                              key=lambda x: x[1]['final_accuracy'], reverse=True)\n",
    "        \n",
    "        for name, results in sorted_results:\n",
    "            print(f\"{name:<20} {results['final_accuracy']:<10} \"\n",
    "                  f\"{results['final_loss']:<8} {results['learning_rate']:<8} \"\n",
    "                  f\"{results['hidden_size']:<8} {results['optimizer']:<10}\")\n",
    "        \n",
    "        # Best config analysis\n",
    "        best_exp = sorted_results[0]\n",
    "        print(f\"\\nðŸ† Best experiment: {best_exp[0]}\")\n",
    "        print(f\"   Accuracy: {best_exp[1]['final_accuracy']}\")\n",
    "        print(f\"   Config: {best_exp[1]['config']}.yaml\")\n",
    "\n",
    "# Run experiment tracking\n",
    "tracker = ExperimentTracker()\n",
    "\n",
    "# Run a few experiments\n",
    "experiments_to_run = [\n",
    "    (\"basic_config.yaml\", \"baseline\"),\n",
    "    (\"high_perf_config.yaml\", \"high_perf\"),  \n",
    "    (\"adam_experiment.yaml\", \"adam_opt\"),\n",
    "    (\"sgd_experiment.yaml\", \"sgd_opt\"),\n",
    "]\n",
    "\n",
    "for config_file, exp_name in experiments_to_run:\n",
    "    config_path = config_dir / config_file\n",
    "    if config_path.exists():\n",
    "        tracker.run_experiment(config_path, exp_name)\n",
    "\n",
    "# Summarize results\n",
    "tracker.summarize_results()\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered LightningCLI and config-driven experiments:\n",
    "\n",
    "1. **CLI-Compatible Components**: Models and DataModules with proper type hints and `save_hyperparameters()`\n",
    "2. **YAML Configurations**: Structured configs for model, data, and trainer parameters\n",
    "3. **Configuration Management**: Base configs, inheritance, and experiment variants\n",
    "4. **Hyperparameter Sweeps**: Systematic exploration via multiple config files\n",
    "5. **Config Validation**: Ensuring configurations are valid before training\n",
    "6. **Experiment Tracking**: Managing and comparing multiple config-based experiments\n",
    "\n",
    "Key benefits of config-driven approach:\n",
    "- **Reproducibility**: Every experiment is fully specified in a config file\n",
    "- **Version Control**: Configs can be tracked in git alongside code\n",
    "- **Easy Comparison**: Side-by-side comparison of different approaches\n",
    "- **No Code Changes**: Experiment with different settings without modifying source code\n",
    "- **Systematic Exploration**: Organized hyperparameter sweeps and ablation studies\n",
    "\n",
    "Best practices:\n",
    "- Use descriptive config names that indicate the experiment\n",
    "- Validate configs before long training runs\n",
    "- Keep base configs for common settings\n",
    "- Track experiment results alongside configurations\n",
    "- Use version control for both code and configs\n",
    "\n",
    "Next notebook: We'll dive into building custom DataModules for different data types."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
