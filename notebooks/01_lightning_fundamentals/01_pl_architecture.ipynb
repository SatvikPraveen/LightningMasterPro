{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb939347",
   "metadata": {},
   "source": [
    "# PyTorch Lightning Architecture Fundamentals\n",
    "\n",
    "**File Location:** `notebooks/01_lightning_fundamentals/01_pl_architecture.ipynb`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook introduces the core components of PyTorch Lightning: LightningModule, LightningDataModule, and Trainer. You'll learn how these components work together and understand the basic logging mechanism with `self.log()`.\n",
    "\n",
    "## Core Lightning Components\n",
    "\n",
    "### LightningModule - The Model Wrapper\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import Adam\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class SimpleMLP(pl.LightningModule):\n",
    "    def __init__(self, input_size=784, hidden_size=128, num_classes=10, lr=1e-3):\n",
    "        super().__init__()\n",
    "        # Save hyperparameters automatically\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Define the network\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Define metrics\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        \n",
    "        # Log metrics - this is the key Lightning feature!\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', self.train_acc(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        \n",
    "        # Log validation metrics\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc(y_hat, y), on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "# Test the model creation\n",
    "model = SimpleMLP()\n",
    "print(f\"Model created with hparams: {model.hparams}\")\n",
    "print(f\"Model architecture:\\n{model}\")\n",
    "```\n",
    "\n",
    "### LightningDataModule - Data Pipeline Management\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class SyntheticDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, num_samples=1000, input_size=784, num_classes=10, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        # Create synthetic data\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # Training data\n",
    "            self.train_x = torch.randn(self.hparams.num_samples, self.hparams.input_size)\n",
    "            self.train_y = torch.randint(0, self.hparams.num_classes, (self.hparams.num_samples,))\n",
    "            \n",
    "            # Validation data (20% of training size)\n",
    "            val_size = self.hparams.num_samples // 5\n",
    "            self.val_x = torch.randn(val_size, self.hparams.input_size)\n",
    "            self.val_y = torch.randint(0, self.hparams.num_classes, (val_size,))\n",
    "        \n",
    "        if stage == \"test\" or stage is None:\n",
    "            # Test data\n",
    "            test_size = self.hparams.num_samples // 10\n",
    "            self.test_x = torch.randn(test_size, self.hparams.input_size)\n",
    "            self.test_y = torch.randint(0, self.hparams.num_classes, (test_size,))\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = TensorDataset(self.train_x, self.train_y)\n",
    "        return DataLoader(dataset, batch_size=self.hparams.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = TensorDataset(self.val_x, self.val_y)\n",
    "        return DataLoader(dataset, batch_size=self.hparams.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataset = TensorDataset(self.test_x, self.test_y)\n",
    "        return DataLoader(dataset, batch_size=self.hparams.batch_size)\n",
    "\n",
    "# Test the data module\n",
    "dm = SyntheticDataModule(num_samples=1000, batch_size=64)\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "print(f\"Training set size: {len(dm.train_x)}\")\n",
    "print(f\"Validation set size: {len(dm.val_x)}\")\n",
    "print(f\"Batch size: {dm.hparams.batch_size}\")\n",
    "\n",
    "# Test a batch\n",
    "train_loader = dm.train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Batch shape: {batch[0].shape}, Labels shape: {batch[1].shape}\")\n",
    "```\n",
    "\n",
    "### Trainer - The Training Engine\n",
    "\n",
    "```python\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Create logger\n",
    "logger = TensorBoardLogger(\"logs\", name=\"pl_architecture_demo\")\n",
    "\n",
    "# Create trainer with basic configuration\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    logger=logger,\n",
    "    enable_checkpointing=True,\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True\n",
    ")\n",
    "\n",
    "# Initialize model and data\n",
    "model = SimpleMLP(lr=1e-3)\n",
    "dm = SyntheticDataModule(num_samples=1000, batch_size=64)\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "print(f\"Training completed! Logs saved to: {logger.log_dir}\")\n",
    "```\n",
    "\n",
    "### Understanding self.log() Parameters\n",
    "\n",
    "```python\n",
    "# Demonstration of different logging options\n",
    "class LoggingDemo(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(10, 1)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.mse_loss(y_hat, y.view(-1, 1))\n",
    "        \n",
    "        # Different logging configurations\n",
    "        self.log('loss_step_only', loss, on_step=True, on_epoch=False)  # Only log per step\n",
    "        self.log('loss_epoch_only', loss, on_step=False, on_epoch=True)  # Only log per epoch\n",
    "        self.log('loss_both', loss, on_step=True, on_epoch=True)  # Log both\n",
    "        self.log('loss_progbar', loss, prog_bar=True)  # Show in progress bar\n",
    "        self.log('loss_logger_only', loss, logger=True, prog_bar=False)  # Only to logger\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Create demo data for regression\n",
    "demo_dm = SyntheticDataModule(num_samples=500, input_size=10, num_classes=1, batch_size=32)\n",
    "\n",
    "# Quick demo (just 1 epoch)\n",
    "demo_trainer = pl.Trainer(max_epochs=1, logger=False, enable_checkpointing=False)\n",
    "demo_model = LoggingDemo()\n",
    "\n",
    "print(\"Demonstrating different logging options:\")\n",
    "print(\"- loss_step_only: logged every step\")\n",
    "print(\"- loss_epoch_only: logged at end of epoch\") \n",
    "print(\"- loss_both: logged both step and epoch\")\n",
    "print(\"- loss_progbar: shown in progress bar\")\n",
    "print(\"- loss_logger_only: only sent to logger (not progress bar)\")\n",
    "```\n",
    "\n",
    "### Model Inspection and Hyperparameters\n",
    "\n",
    "```python\n",
    "# Inspect the trained model\n",
    "print(\"=== Model Inspection ===\")\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Hyperparameters: {model.hparams}\")\n",
    "print(f\"Current epoch: {model.current_epoch}\")\n",
    "print(f\"Global step: {model.global_step}\")\n",
    "\n",
    "# Access logged metrics\n",
    "print(\"\\n=== Training Metrics ===\")\n",
    "if hasattr(trainer, 'logged_metrics'):\n",
    "    for key, value in trainer.logged_metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\n=== Model Summary ===\")\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "summary = ModelSummary(model, max_depth=2)\n",
    "print(summary)\n",
    "```\n",
    "\n",
    "### Architecture Benefits Demo\n",
    "\n",
    "```python\n",
    "# Compare Lightning vs Pure PyTorch approach\n",
    "print(\"=== Lightning vs Pure PyTorch ===\")\n",
    "\n",
    "# Lightning approach (what we just did)\n",
    "print(\"Lightning approach:\")\n",
    "print(\"✓ Automatic GPU/CPU handling\")\n",
    "print(\"✓ Automatic logging and metrics\")\n",
    "print(\"✓ Built-in progress bars\")\n",
    "print(\"✓ Automatic optimization steps\")\n",
    "print(\"✓ Easy experiment tracking\")\n",
    "print(\"✓ Configurable training loops\")\n",
    "\n",
    "# Pure PyTorch equivalent would require:\n",
    "print(\"\\nPure PyTorch equivalent would need:\")\n",
    "print(\"- Manual device handling (cuda/cpu)\")\n",
    "print(\"- Manual loss tracking and averaging\")\n",
    "print(\"- Manual progress bar implementation\") \n",
    "print(\"- Manual optimization loop\")\n",
    "print(\"- Manual validation loop\")\n",
    "print(\"- Manual metric computation\")\n",
    "print(\"- Manual logging setup\")\n",
    "\n",
    "# Show the simplicity\n",
    "print(f\"\\nLightning training call: trainer.fit(model, datamodule)\")\n",
    "print(f\"Lines of Lightning code for training: ~50 lines\")\n",
    "print(f\"Equivalent PyTorch code: ~200+ lines\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **LightningModule**: The core model wrapper that handles training/validation steps and optimizer configuration\n",
    "2. **LightningDataModule**: Organized data pipeline management with setup() and dataloader methods\n",
    "3. **Trainer**: The main training engine that orchestrates the entire training process\n",
    "4. **self.log()**: Flexible logging system with options for step/epoch logging and progress bar display\n",
    "5. **Architecture Benefits**: How Lightning reduces boilerplate code while maintaining flexibility\n",
    "\n",
    "Key takeaways:\n",
    "- Lightning separates concerns: model logic, data handling, and training configuration\n",
    "- `self.log()` automatically handles metric averaging and logging\n",
    "- The Trainer handles all the training loop complexity\n",
    "- Hyperparameters are automatically saved and accessible via `self.hparams`\n",
    "- This architecture makes experiments reproducible and code more maintainable\n",
    "\n",
    "Next notebook: We'll explore Trainer's debugging and development features."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
