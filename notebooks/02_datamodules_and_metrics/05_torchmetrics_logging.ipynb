{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cea730c",
   "metadata": {},
   "source": [
    "# TorchMetrics and Advanced Logging\n",
    "\n",
    "**File Location:** `notebooks/02_datamodules_and_metrics/05_torchmetrics_logging.ipynb`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook covers TorchMetrics integration with PyTorch Lightning for comprehensive model evaluation. Learn to use built-in metrics, create custom metrics, and implement advanced logging strategies for different ML tasks.\n",
    "\n",
    "## TorchMetrics Fundamentals\n",
    "\n",
    "### Basic Metric Usage\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score, AUROC, MeanSquaredError\n",
    "from torchmetrics import MetricCollection, ConfusionMatrix\n",
    "from torchmetrics.functional import accuracy, precision, recall\n",
    "import numpy as np\n",
    "\n",
    "# Simple model for demonstration\n",
    "class MetricsDemo(pl.LightningModule):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Simple model\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(20, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Individual metrics\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "        # Metrics with different averaging\n",
    "        self.val_acc_macro = Accuracy(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "        self.val_acc_weighted = Accuracy(task=\"multiclass\", num_classes=num_classes, average=\"weighted\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Update and log metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Update metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        self.val_acc_macro(preds, y) \n",
    "        self.val_acc_weighted(preds, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc_macro', self.val_acc_macro, on_epoch=True)\n",
    "        self.log('val_acc_weighted', self.val_acc_weighted, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Create synthetic data\n",
    "def create_multiclass_data(num_samples=1000, num_features=20, num_classes=5):\n",
    "    torch.manual_seed(42)\n",
    "    X = torch.randn(num_samples, num_features)\n",
    "    # Create targets with some relationship to features\n",
    "    weights = torch.randn(num_features)\n",
    "    logits = X @ weights\n",
    "    y = torch.div(logits - logits.min(), (logits.max() - logits.min()) / (num_classes - 1), rounding_mode='floor').long()\n",
    "    y = torch.clamp(y, 0, num_classes - 1)\n",
    "    return X, y\n",
    "\n",
    "X, y = create_multiclass_data()\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"✓ Basic metrics demo setup completed\")\n",
    "```\n",
    "\n",
    "### Classification Metrics Suite\n",
    "\n",
    "```python\n",
    "class ClassificationMetrics(pl.LightningModule):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(20, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Comprehensive classification metrics\n",
    "        metrics = MetricCollection({\n",
    "            'accuracy': Accuracy(task=\"multiclass\", num_classes=num_classes),\n",
    "            'precision': Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\"),\n",
    "            'recall': Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\"),\n",
    "            'f1': F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\"),\n",
    "            'auroc': AUROC(task=\"multiclass\", num_classes=num_classes)\n",
    "        })\n",
    "        \n",
    "        # Create separate collections for train/val/test\n",
    "        self.train_metrics = metrics.clone(prefix='train_')\n",
    "        self.val_metrics = metrics.clone(prefix='val_')\n",
    "        self.test_metrics = metrics.clone(prefix='test_')\n",
    "        \n",
    "        # Per-class metrics\n",
    "        self.val_precision_per_class = Precision(\n",
    "            task=\"multiclass\", num_classes=num_classes, average=None\n",
    "        )\n",
    "        self.val_recall_per_class = Recall(\n",
    "            task=\"multiclass\", num_classes=num_classes, average=None\n",
    "        )\n",
    "        \n",
    "        # Confusion matrix\n",
    "        self.val_confmat = ConfusionMatrix(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Get probabilities and predictions\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Update metrics (metrics handle probabilities automatically)\n",
    "        self.train_metrics(preds, y)\n",
    "        \n",
    "        # Log loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        \n",
    "        # Log metrics (will be computed at end of epoch)\n",
    "        self.log_dict(self.train_metrics, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=1) \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Update all metrics\n",
    "        self.val_metrics(preds, y)\n",
    "        self.val_precision_per_class(preds, y)\n",
    "        self.val_recall_per_class(preds, y)\n",
    "        self.val_confmat(preds, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log_dict(self.val_metrics, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_metrics(preds, y)\n",
    "        \n",
    "        self.log('test_loss', loss, on_epoch=True)\n",
    "        self.log_dict(self.test_metrics, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # Log per-class metrics\n",
    "        precision_per_class = self.val_precision_per_class.compute()\n",
    "        recall_per_class = self.val_recall_per_class.compute()\n",
    "        \n",
    "        for i in range(self.hparams.num_classes):\n",
    "            self.log(f'val_precision_class_{i}', precision_per_class[i])\n",
    "            self.log(f'val_recall_class_{i}', recall_per_class[i])\n",
    "        \n",
    "        # Log confusion matrix as a figure (if you have matplotlib)\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import seaborn as sns\n",
    "            \n",
    "            confmat = self.val_confmat.compute()\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            sns.heatmap(confmat.cpu().numpy(), annot=True, fmt='d', ax=ax)\n",
    "            ax.set_xlabel('Predicted')\n",
    "            ax.set_ylabel('True')\n",
    "            ax.set_title('Confusion Matrix')\n",
    "            \n",
    "            # Log figure to tensorboard\n",
    "            if self.logger:\n",
    "                self.logger.experiment.add_figure('val_confusion_matrix', fig, self.current_epoch)\n",
    "            \n",
    "            plt.close(fig)\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"Matplotlib not available for confusion matrix plot\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Test comprehensive classification metrics\n",
    "model = ClassificationMetrics(num_classes=5)\n",
    "\n",
    "# Quick training to see metrics in action\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    limit_train_batches=20,\n",
    "    limit_val_batches=10\n",
    ")\n",
    "\n",
    "print(\"Training with comprehensive classification metrics...\")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "print(\"✓ Classification metrics demo completed\")\n",
    "```\n",
    "\n",
    "### Regression Metrics\n",
    "\n",
    "```python\n",
    "class RegressionMetrics(pl.LightningModule):\n",
    "    def __init__(self, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(20, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "        # Regression metrics\n",
    "        from torchmetrics import MeanSquaredError, MeanAbsoluteError, R2Score\n",
    "        from torchmetrics import MeanAbsolutePercentageError, ExplainedVariance\n",
    "        \n",
    "        regression_metrics = MetricCollection({\n",
    "            'mse': MeanSquaredError(),\n",
    "            'rmse': MeanSquaredError(squared=False),  # RMSE\n",
    "            'mae': MeanAbsoluteError(),\n",
    "            'r2': R2Score(),\n",
    "            'explained_var': ExplainedVariance()\n",
    "        })\n",
    "        \n",
    "        self.train_metrics = regression_metrics.clone(prefix='train_')\n",
    "        self.val_metrics = regression_metrics.clone(prefix='val_')\n",
    "        self.test_metrics = regression_metrics.clone(prefix='test_')\n",
    "        \n",
    "        # Additional custom metrics\n",
    "        self.val_mape = MeanAbsolutePercentageError()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        # Ensure shapes match\n",
    "        if y.dim() == 1:\n",
    "            y = y.unsqueeze(1)\n",
    "        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.train_metrics(y_hat, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log_dict(self.train_metrics, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        if y.dim() == 1:\n",
    "            y = y.unsqueeze(1)\n",
    "        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.val_metrics(y_hat, y)\n",
    "        \n",
    "        # MAPE requires positive targets, so we add offset\n",
    "        y_positive = torch.abs(y) + 1e-6\n",
    "        y_hat_positive = torch.abs(y_hat) + 1e-6\n",
    "        self.val_mape(y_hat_positive, y_positive)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log_dict(self.val_metrics, on_step=False, on_epoch=True)\n",
    "        self.log('val_mape', self.val_mape, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Create regression data\n",
    "def create_regression_data(num_samples=1000, num_features=20, noise=0.1):\n",
    "    torch.manual_seed(42)\n",
    "    X = torch.randn(num_samples, num_features)\n",
    "    # Create targets with linear relationship + noise\n",
    "    weights = torch.randn(num_features) * 0.5\n",
    "    y = X @ weights + torch.randn(num_samples) * noise\n",
    "    return X, y\n",
    "\n",
    "X_reg, y_reg = create_regression_data()\n",
    "reg_dataset = TensorDataset(X_reg, y_reg)\n",
    "reg_train_loader = DataLoader(reg_dataset, batch_size=32, shuffle=True)\n",
    "reg_val_loader = DataLoader(reg_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Test regression metrics\n",
    "reg_model = RegressionMetrics()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    limit_train_batches=20,\n",
    "    limit_val_batches=10\n",
    ")\n",
    "\n",
    "print(\"Training with regression metrics...\")\n",
    "trainer.fit(reg_model, reg_train_loader, reg_val_loader)\n",
    "print(\"✓ Regression metrics demo completed\")\n",
    "```\n",
    "\n",
    "## Custom Metrics\n",
    "\n",
    "### Building Custom Metrics\n",
    "\n",
    "```python\n",
    "from torchmetrics import Metric\n",
    "from typing import Any\n",
    "\n",
    "class TopKCategoricalAccuracy(Metric):\n",
    "    \"\"\"Custom metric: Top-K categorical accuracy\"\"\"\n",
    "    \n",
    "    def __init__(self, k: int = 3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.k = k\n",
    "        \n",
    "        # Define metric state\n",
    "        self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "    \n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        \"\"\"Update metric state\"\"\"\n",
    "        # preds should be logits [batch_size, num_classes]\n",
    "        # target should be class indices [batch_size]\n",
    "        \n",
    "        # Get top-k predictions\n",
    "        _, top_k_preds = torch.topk(preds, self.k, dim=1)\n",
    "        \n",
    "        # Check if true class is in top-k\n",
    "        target_expanded = target.unsqueeze(1).expand_as(top_k_preds)\n",
    "        correct = torch.any(top_k_preds == target_expanded, dim=1)\n",
    "        \n",
    "        self.correct += correct.sum()\n",
    "        self.total += target.size(0)\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"Compute final metric value\"\"\"\n",
    "        return self.correct.float() / self.total\n",
    "\n",
    "class MeanPredictionError(Metric):\n",
    "    \"\"\"Custom metric: Mean prediction error (signed)\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.add_state(\"sum_error\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "    \n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        \"\"\"Update metric state\"\"\"\n",
    "        error = preds - target\n",
    "        self.sum_error += error.sum()\n",
    "        self.total += target.numel()\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"Compute final metric value\"\"\"  \n",
    "        return self.sum_error / self.total\n",
    "\n",
    "class PercentileError(Metric):\n",
    "    \"\"\"Custom metric: Error at specific percentiles\"\"\"\n",
    "    \n",
    "    def __init__(self, percentiles=[50, 90, 95], **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.percentiles = percentiles\n",
    "        \n",
    "        self.add_state(\"errors\", default=[], dist_reduce_fx=\"cat\")\n",
    "    \n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        \"\"\"Update metric state\"\"\"\n",
    "        errors = torch.abs(preds - target).flatten()\n",
    "        self.errors.append(errors)\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"Compute percentile errors\"\"\"\n",
    "        if len(self.errors) == 0:\n",
    "            return {f\"error_p{p}\": torch.tensor(0.0) for p in self.percentiles}\n",
    "        \n",
    "        all_errors = torch.cat(self.errors)\n",
    "        results = {}\n",
    "        \n",
    "        for p in self.percentiles:\n",
    "            percentile_val = torch.quantile(all_errors, p / 100.0)\n",
    "            results[f\"error_p{p}\"] = percentile_val\n",
    "            \n",
    "        return results\n",
    "\n",
    "# Model using custom metrics\n",
    "class CustomMetricsModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(20, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Standard metrics\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "        # Custom metrics\n",
    "        self.val_top3_acc = TopKCategoricalAccuracy(k=3)\n",
    "        self.val_top5_acc = TopKCategoricalAccuracy(k=5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Update standard metrics\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        # Update custom metrics with logits (not predictions)\n",
    "        self.val_top3_acc(logits, y)\n",
    "        self.val_top5_acc(logits, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_top3_acc', self.val_top3_acc, on_epoch=True)\n",
    "        self.log('val_top5_acc', self.val_top5_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Create data with more classes to test top-k accuracy\n",
    "X_multi, y_multi = create_multiclass_data(num_classes=10)\n",
    "multi_dataset = TensorDataset(X_multi, y_multi)\n",
    "multi_train_loader = DataLoader(multi_dataset, batch_size=32, shuffle=True)\n",
    "multi_val_loader = DataLoader(multi_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Test custom metrics\n",
    "custom_model = CustomMetricsModel(num_classes=10)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    limit_train_batches=20,\n",
    "    limit_val_batches=10\n",
    ")\n",
    "\n",
    "print(\"Training with custom metrics...\")\n",
    "trainer.fit(custom_model, multi_train_loader, multi_val_loader)\n",
    "print(\"✓ Custom metrics demo completed\")\n",
    "```\n",
    "\n",
    "## Advanced Logging Strategies\n",
    "\n",
    "### Structured Logging with Metric Collections\n",
    "\n",
    "```python\n",
    "class AdvancedLoggingModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(20, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Organize metrics by category\n",
    "        self.loss_metrics = MetricCollection({\n",
    "            'ce_loss': torch.nn.CrossEntropyLoss(),\n",
    "        })\n",
    "        \n",
    "        self.performance_metrics = MetricCollection({\n",
    "            'accuracy': Accuracy(task=\"multiclass\", num_classes=num_classes),\n",
    "            'f1_macro': F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\"),\n",
    "            'f1_micro': F1Score(task=\"multiclass\", num_classes=num_classes, average=\"micro\"),\n",
    "        })\n",
    "        \n",
    "        self.robustness_metrics = MetricCollection({\n",
    "            'auroc': AUROC(task=\"multiclass\", num_classes=num_classes),\n",
    "            'precision': Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\"),\n",
    "            'recall': Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\"),\n",
    "        })\n",
    "        \n",
    "        # Create train/val versions\n",
    "        self.train_performance = self.performance_metrics.clone(prefix='train_')\n",
    "        self.val_performance = self.performance_metrics.clone(prefix='val_')\n",
    "        self.val_robustness = self.robustness_metrics.clone(prefix='val_')\n",
    "        \n",
    "        # Track additional statistics\n",
    "        self.prediction_confidence_sum = 0.0\n",
    "        self.prediction_entropy_sum = 0.0\n",
    "        self.num_predictions = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.train_performance(preds, y)\n",
    "        \n",
    "        # Calculate additional statistics\n",
    "        max_probs, _ = torch.max(probs, dim=1)\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
    "        \n",
    "        # Log various aspects\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_confidence', max_probs.mean(), on_step=True, on_epoch=True)\n",
    "        self.log('train_entropy', entropy.mean(), on_step=True, on_epoch=True)\n",
    "        \n",
    "        # Log metrics at epoch end\n",
    "        self.log_dict(self.train_performance, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.val_performance(preds, y)\n",
    "        self.val_robustness(preds, y)\n",
    "        \n",
    "        # Track prediction statistics\n",
    "        max_probs, _ = torch.max(probs, dim=1)\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
    "        \n",
    "        self.prediction_confidence_sum += max_probs.sum().item()\n",
    "        self.prediction_entropy_sum += entropy.sum().item()\n",
    "        self.num_predictions += x.size(0)\n",
    "        \n",
    "        # Log basic metrics\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log_dict(self.val_performance, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log_dict(self.val_robustness, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # Log aggregated prediction statistics\n",
    "        if self.num_predictions > 0:\n",
    "            avg_confidence = self.prediction_confidence_sum / self.num_predictions\n",
    "            avg_entropy = self.prediction_entropy_sum / self.num_predictions\n",
    "            \n",
    "            self.log('val_avg_confidence', avg_confidence)\n",
    "            self.log('val_avg_entropy', avg_entropy)\n",
    "            \n",
    "            # Reset for next epoch\n",
    "            self.prediction_confidence_sum = 0.0\n",
    "            self.prediction_entropy_sum = 0.0\n",
    "            self.num_predictions = 0\n",
    "        \n",
    "        # Custom epoch-level computations\n",
    "        current_f1 = self.val_performance['val_f1_macro'].compute()\n",
    "        current_acc = self.val_performance['val_accuracy'].compute()\n",
    "        \n",
    "        # Log derived metrics\n",
    "        self.log('val_f1_acc_ratio', current_f1 / (current_acc + 1e-8))\n",
    "        \n",
    "        # Log to multiple loggers with different formats\n",
    "        if self.logger:\n",
    "            # Log scalar metrics\n",
    "            metrics_dict = {\n",
    "                'epoch': self.current_epoch,\n",
    "                'val_f1_macro': current_f1.item(),\n",
    "                'val_accuracy': current_acc.item(),\n",
    "            }\n",
    "            \n",
    "            # If using TensorBoard logger\n",
    "            if hasattr(self.logger, 'experiment'):\n",
    "                for key, value in metrics_dict.items():\n",
    "                    if key != 'epoch':\n",
    "                        self.logger.experiment.add_scalar(f'custom/{key}', value, self.current_epoch)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_accuracy',  # Metric to monitor\n",
    "                'frequency': 1,\n",
    "                'interval': 'epoch'\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Test advanced logging\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"logs\", name=\"advanced_logging\")\n",
    "\n",
    "advanced_model = AdvancedLoggingModel(num_classes=5)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    logger=logger,\n",
    "    enable_checkpointing=False,\n",
    "    limit_train_batches=20,\n",
    "    limit_val_batches=10\n",
    ")\n",
    "\n",
    "print(\"Training with advanced logging...\")\n",
    "trainer.fit(advanced_model, train_loader, val_loader)\n",
    "print(\"✓ Advanced logging demo completed\")\n",
    "print(f\"Logs saved to: {logger.log_dir}\")\n",
    "```\n",
    "\n",
    "### Multi-Modal and Hierarchical Metrics\n",
    "\n",
    "```python\n",
    "class HierarchicalMetrics(pl.LightningModule):\n",
    "    \"\"\"Model with hierarchical/grouped metric organization\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(20, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Organize metrics hierarchically\n",
    "        self.metrics = nn.ModuleDict({\n",
    "            'accuracy': nn.ModuleDict({\n",
    "                'train': Accuracy(task=\"multiclass\", num_classes=num_classes),\n",
    "                'val': Accuracy(task=\"multiclass\", num_classes=num_classes),\n",
    "                'test': Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "            }),\n",
    "            'f1': nn.ModuleDict({\n",
    "                'train_macro': F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\"),\n",
    "                'train_micro': F1Score(task=\"multiclass\", num_classes=num_classes, average=\"micro\"),\n",
    "                'val_macro': F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\"),\n",
    "                'val_micro': F1Score(task=\"multiclass\", num_classes=num_classes, average=\"micro\"),\n",
    "            })\n",
    "        })\n",
    "        \n",
    "        # Per-class metrics for detailed analysis\n",
    "        self.per_class_metrics = nn.ModuleDict({\n",
    "            f'class_{i}': nn.ModuleDict({\n",
    "                'precision': Precision(task=\"multiclass\", num_classes=num_classes, average=None),\n",
    "                'recall': Recall(task=\"multiclass\", num_classes=num_classes, average=None),\n",
    "            }) for i in range(num_classes)\n",
    "        })\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Update hierarchical metrics\n",
    "        self.metrics['accuracy']['train'](preds, y)\n",
    "        self.metrics['f1']['train_macro'](preds, y)\n",
    "        self.metrics['f1']['train_micro'](preds, y)\n",
    "        \n",
    "        # Log with hierarchical names\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('metrics/accuracy/train', self.metrics['accuracy']['train'], on_epoch=True)\n",
    "        self.log('metrics/f1/train_macro', self.metrics['f1']['train_macro'], on_epoch=True)\n",
    "        self.log('metrics/f1/train_micro', self.metrics['f1']['train_micro'], on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch  \n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Update all validation metrics\n",
    "        self.metrics['accuracy']['val'](preds, y)\n",
    "        self.metrics['f1']['val_macro'](preds, y)\n",
    "        self.metrics['f1']['val_micro'](preds, y)\n",
    "        \n",
    "        # Update per-class metrics\n",
    "        for i in range(self.hparams.num_classes):\n",
    "            self.per_class_metrics[f'class_{i}']['precision'](preds, y)\n",
    "            self.per_class_metrics[f'class_{i}']['recall'](preds, y)\n",
    "        \n",
    "        # Log hierarchical metrics\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('metrics/accuracy/val', self.metrics['accuracy']['val'], on_epoch=True, prog_bar=True)\n",
    "        self.log('metrics/f1/val_macro', self.metrics['f1']['val_macro'], on_epoch=True)\n",
    "        self.log('metrics/f1/val_micro', self.metrics['f1']['val_micro'], on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # Log per-class metrics\n",
    "        for i in range(self.hparams.num_classes):\n",
    "            precision_all = self.per_class_metrics[f'class_{i}']['precision'].compute()\n",
    "            recall_all = self.per_class_metrics[f'class_{i}']['recall'].compute()\n",
    "            \n",
    "            # Extract value for class i\n",
    "            if precision_all.numel() > i:\n",
    "                precision_i = precision_all[i]\n",
    "                recall_i = recall_all[i]\n",
    "                \n",
    "                self.log(f'class_metrics/precision/class_{i}', precision_i)\n",
    "                self.log(f'class_metrics/recall/class_{i}', recall_i)\n",
    "                \n",
    "                # F1 for this class\n",
    "                f1_i = 2 * (precision_i * recall_i) / (precision_i + recall_i + 1e-8)\n",
    "                self.log(f'class_metrics/f1/class_{i}', f1_i)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Test hierarchical metrics\n",
    "hierarchical_model = HierarchicalMetrics(num_classes=5)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    enable_checkpointing=False,\n",
    "    logger=TensorBoardLogger(\"logs\", name=\"hierarchical_metrics\"),\n",
    "    limit_train_batches=20,\n",
    "    limit_val_batches=10\n",
    ")\n",
    "\n",
    "print(\"Training with hierarchical metrics...\")\n",
    "trainer.fit(hierarchical_model, train_loader, val_loader)\n",
    "print(\"✓ Hierarchical metrics demo completed\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered comprehensive metric tracking and logging in PyTorch Lightning:\n",
    "\n",
    "1. **TorchMetrics Basics**: Using built-in metrics with proper state management and averaging\n",
    "2. **Classification Metrics**: Accuracy, precision, recall, F1, AUROC, confusion matrix\n",
    "3. **Regression Metrics**: MSE, RMSE, MAE, R², explained variance, MAPE\n",
    "4. **Custom Metrics**: Building domain-specific metrics by extending the Metric class\n",
    "5. **Advanced Logging**: Structured, hierarchical metric organization and multi-logger support\n",
    "6. **Metric Collections**: Grouping related metrics for cleaner code and logging\n",
    "\n",
    "Key best practices:\n",
    "- Use MetricCollection to group related metrics\n",
    "- Implement proper metric state management with `add_state()`\n",
    "- Log metrics at appropriate intervals (step vs epoch)\n",
    "- Organize metrics hierarchically for better visualization\n",
    "- Use custom metrics for domain-specific evaluation\n",
    "- Leverage multiple loggers for different visualization needs\n",
    "- Track prediction confidence and uncertainty alongside accuracy\n",
    "\n",
    "Advanced features covered:\n",
    "- Per-class metric tracking for imbalanced datasets\n",
    "- Prediction confidence and entropy monitoring\n",
    "- Learning rate scheduling based on metric values\n",
    "- Custom epoch-end computations and derived metrics\n",
    "- Integration with TensorBoard for rich visualizations\n",
    "\n",
    "Next notebook: We'll explore checkpointing and early stopping strategies."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
