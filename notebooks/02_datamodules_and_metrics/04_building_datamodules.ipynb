{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15f7a03",
   "metadata": {},
   "source": [
    "# Building Custom DataModules\n",
    "\n",
    "**File Location:** `notebooks/02_datamodules_and_metrics/04_building_datamodules.ipynb`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook covers building robust LightningDataModules for different data types. Learn to handle vision, NLP, tabular, and time-series data with proper data loading, preprocessing, and validation patterns.\n",
    "\n",
    "## DataModule Architecture Fundamentals\n",
    "\n",
    "### Core DataModule Structure\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class BaseDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Template for all DataModules\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = True,\n",
    "        persistent_workers: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Download data, tokenize, etc. Only called on 1 GPU/TPU in distributed\"\"\"\n",
    "        # This is where you download datasets, do one-time preprocessing\n",
    "        pass\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"Set up datasets. Called on every GPU in distributed training\"\"\"\n",
    "        # This is where you create train/val/test splits\n",
    "        # stage can be 'fit', 'validate', 'test', or 'predict'\n",
    "        pass\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Return training DataLoader\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Return validation DataLoader\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        \"\"\"Return test DataLoader\"\"\"  \n",
    "        pass\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        \"\"\"Return prediction DataLoader\"\"\"\n",
    "        pass\n",
    "\n",
    "print(\"✓ Base DataModule template defined\")\n",
    "```\n",
    "\n",
    "## Vision DataModule\n",
    "\n",
    "### Custom Vision Dataset and DataModule\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SyntheticVisionDataset(Dataset):\n",
    "    \"\"\"Synthetic image dataset for demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples: int, image_size: Tuple[int, int] = (32, 32), \n",
    "                 num_classes: int = 10, transform=None):\n",
    "        self.num_samples = num_samples\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Generate synthetic data once\n",
    "        torch.manual_seed(42)\n",
    "        self.data = torch.rand(num_samples, 3, *image_size) * 255\n",
    "        self.targets = torch.randint(0, num_classes, (num_samples,))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        # Convert to PIL for transforms\n",
    "        image = transforms.ToPILImage()(image.byte())\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "class VisionDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_train_samples: int = 5000,\n",
    "        num_val_samples: int = 1000,\n",
    "        num_test_samples: int = 1000,\n",
    "        image_size: Tuple[int, int] = (32, 32),\n",
    "        num_classes: int = 10,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 2,\n",
    "        pin_memory: bool = True,\n",
    "        augment: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Define transforms\n",
    "        self.train_transforms = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.RandomHorizontalFlip(p=0.5) if augment else transforms.Lambda(lambda x: x),\n",
    "            transforms.RandomRotation(10) if augment else transforms.Lambda(lambda x: x),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2) if augment else transforms.Lambda(lambda x: x),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.val_transforms = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = SyntheticVisionDataset(\n",
    "                num_samples=self.hparams.num_train_samples,\n",
    "                image_size=self.hparams.image_size,\n",
    "                num_classes=self.hparams.num_classes,\n",
    "                transform=self.train_transforms\n",
    "            )\n",
    "            self.val_dataset = SyntheticVisionDataset(\n",
    "                num_samples=self.hparams.num_val_samples,\n",
    "                image_size=self.hparams.image_size,\n",
    "                num_classes=self.hparams.num_classes,\n",
    "                transform=self.val_transforms\n",
    "            )\n",
    "        \n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = SyntheticVisionDataset(\n",
    "                num_samples=self.hparams.num_test_samples,\n",
    "                image_size=self.hparams.image_size,\n",
    "                num_classes=self.hparams.num_classes,\n",
    "                transform=self.val_transforms\n",
    "            )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            drop_last=True  # For batch norm stability\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory\n",
    "        )\n",
    "\n",
    "# Test the vision datamodule\n",
    "vision_dm = VisionDataModule(\n",
    "    num_train_samples=1000,\n",
    "    num_val_samples=200,\n",
    "    batch_size=16,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "vision_dm.setup(\"fit\")\n",
    "\n",
    "# Test a batch\n",
    "train_loader = vision_dm.train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch\n",
    "\n",
    "print(f\"✓ Vision DataModule created\")\n",
    "print(f\"Image batch shape: {images.shape}\")\n",
    "print(f\"Labels batch shape: {labels.shape}\")\n",
    "print(f\"Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"Unique labels in batch: {torch.unique(labels).tolist()}\")\n",
    "```\n",
    "\n",
    "## NLP DataModule\n",
    "\n",
    "### Text Processing and NLP DataModule\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "import string\n",
    "import random\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Simple text classification dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, vocab, max_length=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize and convert to indices\n",
    "        tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(indices) > self.max_length:\n",
    "            indices = indices[:self.max_length]\n",
    "        else:\n",
    "            indices.extend([self.vocab['<PAD>']] * (self.max_length - len(indices)))\n",
    "        \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class NLPDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_samples: int = 5000,\n",
    "        vocab_size: int = 1000,\n",
    "        max_length: int = 100,\n",
    "        min_freq: int = 2,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.vocab = None\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Generate synthetic text data and build vocabulary\"\"\"\n",
    "        # Generate synthetic text data\n",
    "        random.seed(42)\n",
    "        \n",
    "        # Simple vocabulary\n",
    "        base_words = [\n",
    "            'the', 'and', 'is', 'in', 'to', 'of', 'a', 'that', 'it', 'with',\n",
    "            'for', 'as', 'was', 'on', 'are', 'you', 'all', 'not', 'can', 'had',\n",
    "            'good', 'bad', 'great', 'terrible', 'amazing', 'awful', 'wonderful', 'horrible',\n",
    "            'love', 'hate', 'like', 'dislike', 'enjoy', 'despise', 'adore', 'loathe',\n",
    "            'movie', 'film', 'book', 'story', 'show', 'series', 'episode', 'scene'\n",
    "        ]\n",
    "        \n",
    "        # Generate synthetic reviews\n",
    "        positive_templates = [\n",
    "            \"this {} is really good and amazing\",\n",
    "            \"I love this {} it is wonderful\",\n",
    "            \"great {} with excellent quality\",\n",
    "            \"amazing {} highly recommend\"\n",
    "        ]\n",
    "        \n",
    "        negative_templates = [\n",
    "            \"this {} is terrible and awful\",\n",
    "            \"I hate this {} it is horrible\",\n",
    "            \"bad {} with poor quality\", \n",
    "            \"awful {} do not recommend\"\n",
    "        ]\n",
    "        \n",
    "        # Generate texts and labels\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for _ in range(self.hparams.num_samples):\n",
    "            if random.random() > 0.5:  # Positive\n",
    "                template = random.choice(positive_templates)\n",
    "                item = random.choice(['movie', 'book', 'show', 'film'])\n",
    "                text = template.format(item)\n",
    "                # Add some random words\n",
    "                extra_words = random.choices(base_words, k=random.randint(3, 8))\n",
    "                text += \" \" + \" \".join(extra_words)\n",
    "                self.texts.append(text)\n",
    "                self.labels.append(1)\n",
    "            else:  # Negative\n",
    "                template = random.choice(negative_templates)\n",
    "                item = random.choice(['movie', 'book', 'show', 'film'])\n",
    "                text = template.format(item)\n",
    "                # Add some random words\n",
    "                extra_words = random.choices(base_words, k=random.randint(3, 8))\n",
    "                text += \" \" + \" \".join(extra_words)\n",
    "                self.texts.append(text)\n",
    "                self.labels.append(0)\n",
    "        \n",
    "        # Build vocabulary\n",
    "        all_tokens = []\n",
    "        for text in self.texts:\n",
    "            tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "            all_tokens.extend(tokens)\n",
    "        \n",
    "        token_counts = Counter(all_tokens)\n",
    "        \n",
    "        # Create vocab with special tokens\n",
    "        self.vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        idx = 2\n",
    "        for token, count in token_counts.most_common():\n",
    "            if count >= self.hparams.min_freq and idx < self.hparams.vocab_size:\n",
    "                self.vocab[token] = idx\n",
    "                idx += 1\n",
    "        \n",
    "        print(f\"✓ Generated {len(self.texts)} text samples\")\n",
    "        print(f\"✓ Built vocabulary with {len(self.vocab)} tokens\")\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if self.vocab is None:\n",
    "            self.prepare_data()\n",
    "            \n",
    "        # Split data\n",
    "        train_size = int(0.8 * len(self.texts))\n",
    "        val_size = int(0.1 * len(self.texts))\n",
    "        test_size = len(self.texts) - train_size - val_size\n",
    "        \n",
    "        indices = list(range(len(self.texts)))\n",
    "        random.seed(42)\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:train_size + val_size]\n",
    "        test_indices = indices[train_size + val_size:]\n",
    "        \n",
    "        if stage == \"fit\" or stage is None:\n",
    "            train_texts = [self.texts[i] for i in train_indices]\n",
    "            train_labels = [self.labels[i] for i in train_indices]\n",
    "            self.train_dataset = TextDataset(train_texts, train_labels, self.vocab, self.hparams.max_length)\n",
    "            \n",
    "            val_texts = [self.texts[i] for i in val_indices]\n",
    "            val_labels = [self.labels[i] for i in val_indices]\n",
    "            self.val_dataset = TextDataset(val_texts, val_labels, self.vocab, self.hparams.max_length)\n",
    "        \n",
    "        if stage == \"test\" or stage is None:\n",
    "            test_texts = [self.texts[i] for i in test_indices]\n",
    "            test_labels = [self.labels[i] for i in test_indices]\n",
    "            self.test_dataset = TextDataset(test_texts, test_labels, self.vocab, self.hparams.max_length)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.hparams.num_workers\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers\n",
    "        )\n",
    "\n",
    "# Test NLP DataModule\n",
    "nlp_dm = NLPDataModule(num_samples=1000, vocab_size=500, max_length=50, batch_size=16)\n",
    "nlp_dm.setup(\"fit\")\n",
    "\n",
    "# Test a batch\n",
    "train_loader = nlp_dm.train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "texts, labels = batch\n",
    "\n",
    "print(f\"✓ NLP DataModule created\")\n",
    "print(f\"Text batch shape: {texts.shape}\")\n",
    "print(f\"Labels batch shape: {labels.shape}\")\n",
    "print(f\"Vocabulary size: {len(nlp_dm.vocab)}\")\n",
    "print(f\"Sample text indices: {texts[0][:10].tolist()}\")\n",
    "print(f\"Sample label: {labels[0].item()}\")\n",
    "```\n",
    "\n",
    "## Tabular DataModule\n",
    "\n",
    "### Structured Data Processing\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    \"\"\"Dataset for tabular data\"\"\"\n",
    "    \n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.LongTensor(targets) if targets.dtype == 'int64' else torch.FloatTensor(targets)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "class TabularDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_samples: int = 10000,\n",
    "        num_features: int = 20,\n",
    "        num_classes: int = 3,\n",
    "        task_type: str = \"classification\",  # \"classification\" or \"regression\"\n",
    "        batch_size: int = 64,\n",
    "        num_workers: int = 0,\n",
    "        normalize: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.scaler = StandardScaler() if normalize else None\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Generate synthetic tabular data\"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Generate features with different distributions\n",
    "        features = []\n",
    "        \n",
    "        # Normal features\n",
    "        normal_features = np.random.normal(0, 1, (self.hparams.num_samples, self.hparams.num_features // 2))\n",
    "        features.append(normal_features)\n",
    "        \n",
    "        # Uniform features\n",
    "        uniform_features = np.random.uniform(-2, 2, (self.hparams.num_samples, self.hparams.num_features // 4))\n",
    "        features.append(uniform_features)\n",
    "        \n",
    "        # Exponential features\n",
    "        exp_features = np.random.exponential(1, (self.hparams.num_samples, self.hparams.num_features // 4))\n",
    "        features.append(exp_features)\n",
    "        \n",
    "        self.features = np.concatenate(features, axis=1)\n",
    "        \n",
    "        if self.hparams.task_type == \"classification\":\n",
    "            # Create targets with some relationship to features\n",
    "            feature_sum = np.sum(self.features[:, :5], axis=1)\n",
    "            self.targets = np.digitize(feature_sum, \n",
    "                                     bins=np.percentile(feature_sum, \n",
    "                                                      [100/self.hparams.num_classes * i \n",
    "                                                       for i in range(1, self.hparams.num_classes)]))\n",
    "            self.targets = np.clip(self.targets, 0, self.hparams.num_classes - 1)\n",
    "        else:  # regression\n",
    "            # Create continuous target\n",
    "            weights = np.random.normal(0, 0.5, self.hparams.num_features)\n",
    "            self.targets = np.dot(self.features, weights) + np.random.normal(0, 0.1, self.hparams.num_samples)\n",
    "        \n",
    "        print(f\"✓ Generated tabular data: {self.features.shape}\")\n",
    "        print(f\"✓ Task type: {self.hparams.task_type}\")\n",
    "        if self.hparams.task_type == \"classification\":\n",
    "            print(f\"✓ Class distribution: {np.bincount(self.targets)}\")\n",
    "        else:\n",
    "            print(f\"✓ Target range: [{self.targets.min():.3f}, {self.targets.max():.3f}]\")\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if not hasattr(self, 'features'):\n",
    "            self.prepare_data()\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "            self.features, self.targets, test_size=0.3, random_state=42, \n",
    "            stratify=self.targets if self.hparams.task_type == \"classification\" else None\n",
    "        )\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_temp, y_temp, test_size=0.5, random_state=42,\n",
    "            stratify=y_temp if self.hparams.task_type == \"classification\" else None\n",
    "        )\n",
    "        \n",
    "        # Normalize features\n",
    "        if self.hparams.normalize:\n",
    "            if stage == \"fit\" or stage is None:\n",
    "                X_train = self.scaler.fit_transform(X_train)\n",
    "                X_val = self.scaler.transform(X_val)\n",
    "            if stage == \"test\" or stage is None:\n",
    "                if hasattr(self.scaler, 'mean_'):  # Already fitted\n",
    "                    X_test = self.scaler.transform(X_test)\n",
    "                else:  # Need to fit first\n",
    "                    self.scaler.fit(X_train)\n",
    "                    X_test = self.scaler.transform(X_test)\n",
    "        \n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = TabularDataset(X_train, y_train)\n",
    "            self.val_dataset = TabularDataset(X_val, y_val)\n",
    "        \n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = TabularDataset(X_test, y_test)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.hparams.num_workers\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers\n",
    "        )\n",
    "\n",
    "# Test Tabular DataModule\n",
    "tabular_dm = TabularDataModule(\n",
    "    num_samples=5000,\n",
    "    num_features=15,\n",
    "    num_classes=4,\n",
    "    task_type=\"classification\",\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "tabular_dm.setup(\"fit\")\n",
    "\n",
    "# Test a batch\n",
    "train_loader = tabular_dm.train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "features, targets = batch\n",
    "\n",
    "print(f\"✓ Tabular DataModule created\")\n",
    "print(f\"Features batch shape: {features.shape}\")\n",
    "print(f\"Targets batch shape: {targets.shape}\")\n",
    "print(f\"Feature range: [{features.min():.3f}, {features.max():.3f}]\")\n",
    "```\n",
    "\n",
    "## Time Series DataModule\n",
    "\n",
    "### Sequential Data Processing\n",
    "\n",
    "```python\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Dataset for time series data\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, targets, sequence_length):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx:idx + self.sequence_length]\n",
    "        target = self.targets[idx + self.sequence_length]\n",
    "        return torch.FloatTensor(sequence), torch.FloatTensor([target])\n",
    "\n",
    "class TimeSeriesDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_samples: int = 10000,\n",
    "        num_features: int = 5,\n",
    "        sequence_length: int = 50,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 0,\n",
    "        normalize: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.scaler = StandardScaler() if normalize else None\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Generate synthetic time series data\"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Generate time series with trend, seasonality, and noise\n",
    "        time_steps = np.arange(self.hparams.num_samples)\n",
    "        \n",
    "        # Multiple features with different patterns\n",
    "        series_data = []\n",
    "        \n",
    "        for feature_idx in range(self.hparams.num_features):\n",
    "            # Trend component\n",
    "            trend = 0.001 * time_steps + np.random.normal(0, 0.1)\n",
    "            \n",
    "            # Seasonal component\n",
    "            seasonal = np.sin(2 * np.pi * time_steps / 365) * (0.5 + feature_idx * 0.2)\n",
    "            \n",
    "            # Weekly pattern\n",
    "            weekly = np.sin(2 * np.pi * time_steps / 7) * 0.3\n",
    "            \n",
    "            # Noise\n",
    "            noise = np.random.normal(0, 0.2, self.hparams.num_samples)\n",
    "            \n",
    "            # Combine components\n",
    "            feature_series = trend + seasonal + weekly + noise\n",
    "            series_data.append(feature_series)\n",
    "        \n",
    "        self.data = np.column_stack(series_data)\n",
    "        \n",
    "        # Create targets (next value prediction)\n",
    "        self.targets = self.data[1:, 0]  # Predict first feature\n",
    "        self.data = self.data[:-1]  # Remove last sample\n",
    "        \n",
    "        print(f\"✓ Generated time series data: {self.data.shape}\")\n",
    "        print(f\"✓ Sequence length: {self.hparams.sequence_length}\")\n",
    "        print(f\"✓ Target range: [{self.targets.min():.3f}, {self.targets.max():.3f}]\")\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if not hasattr(self, 'data'):\n",
    "            self.prepare_data()\n",
    "        \n",
    "        # Time series split (no shuffling to preserve temporal order)\n",
    "        train_size = int(0.7 * len(self.data))\n",
    "        val_size = int(0.15 * len(self.data))\n",
    "        \n",
    "        train_data = self.data[:train_size]\n",
    "        train_targets = self.targets[:train_size]\n",
    "        \n",
    "        val_data = self.data[train_size:train_size + val_size]\n",
    "        val_targets = self.targets[train_size:train_size + val_size]\n",
    "        \n",
    "        test_data = self.data[train_size + val_size:]\n",
    "        test_targets = self.targets[train_size + val_size:]\n",
    "        \n",
    "        # Normalize data\n",
    "        if self.hparams.normalize:\n",
    "            if stage == \"fit\" or stage is None:\n",
    "                train_data = self.scaler.fit_transform(train_data)\n",
    "                val_data = self.scaler.transform(val_data)\n",
    "            if stage == \"test\" or stage is None:\n",
    "                if hasattr(self.scaler, 'mean_'):\n",
    "                    test_data = self.scaler.transform(test_data)\n",
    "                else:\n",
    "                    self.scaler.fit(train_data)\n",
    "                    test_data = self.scaler.transform(test_data)\n",
    "        \n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = TimeSeriesDataset(\n",
    "                train_data, train_targets, self.hparams.sequence_length\n",
    "            )\n",
    "            self.val_dataset = TimeSeriesDataset(\n",
    "                val_data, val_targets, self.hparams.sequence_length\n",
    "            )\n",
    "        \n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = TimeSeriesDataset(\n",
    "                test_data, test_targets, self.hparams.sequence_length\n",
    "            )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,  # Can shuffle for time series within sequences\n",
    "            num_workers=self.hparams.num_workers\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers\n",
    "        )\n",
    "\n",
    "# Test Time Series DataModule\n",
    "ts_dm = TimeSeriesDataModule(\n",
    "    num_samples=5000,\n",
    "    num_features=3,\n",
    "    sequence_length=30,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "ts_dm.setup(\"fit\")\n",
    "\n",
    "# Test a batch\n",
    "train_loader = ts_dm.train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "sequences, targets = batch\n",
    "\n",
    "print(f\"✓ Time Series DataModule created\")\n",
    "print(f\"Sequence batch shape: {sequences.shape}\")\n",
    "print(f\"Targets batch shape: {targets.shape}\")\n",
    "print(f\"Sequence range: [{sequences.min():.3f}, {sequences.max():.3f}]\")\n",
    "```\n",
    "\n",
    "## DataModule Best Practices\n",
    "\n",
    "### Data Validation and Health Checks\n",
    "\n",
    "```python\n",
    "def validate_datamodule(dm: pl.LightningDataModule, stage: str = \"fit\"):\n",
    "    \"\"\"Comprehensive DataModule validation\"\"\"\n",
    "    print(f\"=== Validating DataModule: {type(dm).__name__} ===\")\n",
    "    \n",
    "    # Setup the datamodule\n",
    "    dm.setup(stage)\n",
    "    \n",
    "    # Check dataloaders exist\n",
    "    loaders = []\n",
    "    if hasattr(dm, 'train_dataloader') and stage in [\"fit\", None]:\n",
    "        train_loader = dm.train_dataloader()\n",
    "        loaders.append((\"train\", train_loader))\n",
    "        \n",
    "    if hasattr(dm, 'val_dataloader') and stage in [\"fit\", \"validate\", None]:\n",
    "        val_loader = dm.val_dataloader()\n",
    "        loaders.append((\"val\", val_loader))\n",
    "        \n",
    "    if hasattr(dm, 'test_dataloader') and stage in [\"test\", None]:\n",
    "        test_loader = dm.test_dataloader()\n",
    "        loaders.append((\"test\", test_loader))\n",
    "    \n",
    "    print(f\"✓ Found {len(loaders)} dataloaders\")\n",
    "    \n",
    "    # Validate each dataloader\n",
    "    for loader_name, loader in loaders:\n",
    "        print(f\"\\n--- {loader_name.upper()} Loader ---\")\n",
    "        \n",
    "        # Basic properties\n",
    "        print(f\"Dataset size: {len(loader.dataset)}\")\n",
    "        print(f\"Batch size: {loader.batch_size}\")\n",
    "        print(f\"Number of batches: {len(loader)}\")\n",
    "        \n",
    "        # Test first batch\n",
    "        try:\n",
    "            batch = next(iter(loader))\n",
    "            if isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
    "                inputs, targets = batch\n",
    "                print(f\"Input shape: {inputs.shape}\")\n",
    "                print(f\"Target shape: {targets.shape}\")\n",
    "                print(f\"Input dtype: {inputs.dtype}\")\n",
    "                print(f\"Target dtype: {targets.dtype}\")\n",
    "                \n",
    "                # Check for NaN/Inf\n",
    "                if torch.isnan(inputs).any():\n",
    "                    print(\"⚠ WARNING: Found NaN in inputs\")\n",
    "                if torch.isinf(inputs).any():\n",
    "                    print(\"⚠ WARNING: Found Inf in inputs\")\n",
    "                    \n",
    "                # Check data range\n",
    "                print(f\"Input range: [{inputs.min():.3f}, {inputs.max():.3f}]\")\n",
    "                \n",
    "                if targets.dtype in [torch.long, torch.int]:\n",
    "                    print(f\"Target classes: {torch.unique(targets).tolist()}\")\n",
    "                else:\n",
    "                    print(f\"Target range: [{targets.min():.3f}, {targets.max():.3f}]\")\n",
    "                    \n",
    "            else:\n",
    "                print(f\"Batch structure: {type(batch)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading batch: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n✓ DataModule validation completed\")\n",
    "\n",
    "# Validate all our datamodules\n",
    "print(\"=== DataModule Validation ===\")\n",
    "datamodules = [\n",
    "    (\"Vision\", vision_dm),\n",
    "    (\"NLP\", nlp_dm), \n",
    "    (\"Tabular\", tabular_dm),\n",
    "    (\"TimeSeries\", ts_dm)\n",
    "]\n",
    "\n",
    "for name, dm in datamodules:\n",
    "    print(f\"\\n{name} DataModule:\")\n",
    "    try:\n",
    "        validate_datamodule(dm)\n",
    "        print(\"✅ PASSED\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ FAILED: {e}\")\n",
    "```\n",
    "\n",
    "### Memory and Performance Optimization\n",
    "\n",
    "```python\n",
    "def benchmark_datamodule(dm: pl.LightningDataModule, num_batches: int = 10):\n",
    "    \"\"\"Benchmark DataModule loading performance\"\"\"\n",
    "    import time\n",
    "    \n",
    "    print(f\"=== Benchmarking {type(dm).__name__} ===\")\n",
    "    \n",
    "    dm.setup(\"fit\")\n",
    "    train_loader = dm.train_dataloader()\n",
    "    \n",
    "    # Warmup\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        if i >= 2:  # Just warmup\n",
    "            break\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    batch_times = []\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        # Simulate some processing\n",
    "        if isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
    "            inputs, targets = batch\n",
    "            _ = inputs.mean()  # Simple operation\n",
    "        \n",
    "        batch_end = time.time()\n",
    "        batch_times.append(batch_end - batch_start)\n",
    "        \n",
    "        if i >= num_batches - 1:\n",
    "            break\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    avg_batch_time = np.mean(batch_times)\n",
    "    \n",
    "    print(f\"Total time: {total_time:.3f}s\")\n",
    "    print(f\"Average batch time: {avg_batch_time*1000:.1f}ms\")\n",
    "    print(f\"Batches per second: {1/avg_batch_time:.1f}\")\n",
    "    print(f\"Samples per second: {len(batch[0])/avg_batch_time:.0f}\")\n",
    "    \n",
    "    return {\n",
    "        'total_time': total_time,\n",
    "        'avg_batch_time': avg_batch_time,\n",
    "        'batches_per_sec': 1/avg_batch_time\n",
    "    }\n",
    "\n",
    "# Benchmark our datamodules\n",
    "print(\"=== Performance Benchmarking ===\")\n",
    "for name, dm in datamodules:\n",
    "    print(f\"\\n{name} DataModule:\")\n",
    "    try:\n",
    "        results = benchmark_datamodule(dm, num_batches=5)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Benchmark failed: {e}\")\n",
    "```\n",
    "\n",
    "# Advanced DataModule Patterns\n",
    "\n",
    "## Multi-Modal DataModule\n",
    "\n",
    "```python\n",
    "class MultiModalDataset(Dataset):\n",
    "    \"\"\"Dataset that combines vision and text data\"\"\"\n",
    "    \n",
    "    def __init__(self, images, texts, labels, image_transform=None, text_vocab=None, max_text_length=50):\n",
    "        self.images = images\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.image_transform = image_transform\n",
    "        self.text_vocab = text_vocab or {}\n",
    "        self.max_text_length = max_text_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Process image\n",
    "        image = self.images[idx]\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        \n",
    "        # Process text\n",
    "        text = self.texts[idx]\n",
    "        tokens = text.lower().split()\n",
    "        text_indices = [self.text_vocab.get(token, self.text_vocab.get('<UNK>', 0)) for token in tokens]\n",
    "        \n",
    "        # Pad or truncate text\n",
    "        if len(text_indices) > self.max_text_length:\n",
    "            text_indices = text_indices[:self.max_text_length]\n",
    "        else:\n",
    "            text_indices.extend([self.text_vocab.get('<PAD>', 0)] * (self.max_text_length - len(text_indices)))\n",
    "        \n",
    "        text_tensor = torch.tensor(text_indices, dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text_tensor,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "class MultiModalDataModule(pl.LightningDataModule):\n",
    "    \"\"\"DataModule for multi-modal vision + text data\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_samples: int = 1000,\n",
    "        image_size: Tuple[int, int] = (64, 64),\n",
    "        vocab_size: int = 500,\n",
    "        max_text_length: int = 50,\n",
    "        num_classes: int = 5,\n",
    "        batch_size: int = 16,\n",
    "        num_workers: int = 0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Image transforms\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Generate synthetic multi-modal data\"\"\"\n",
    "        torch.manual_seed(42)\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Generate synthetic images\n",
    "        self.images = torch.rand(self.hparams.num_samples, 3, *self.hparams.image_size) * 255\n",
    "        \n",
    "        # Generate synthetic texts\n",
    "        base_words = ['cat', 'dog', 'bird', 'fish', 'car', 'bike', 'tree', 'house', 'red', 'blue', 'big', 'small']\n",
    "        self.texts = []\n",
    "        for _ in range(self.hparams.num_samples):\n",
    "            num_words = np.random.randint(5, 15)\n",
    "            text = ' '.join(np.random.choice(base_words, num_words))\n",
    "            self.texts.append(text)\n",
    "        \n",
    "        # Generate labels\n",
    "        self.labels = np.random.randint(0, self.hparams.num_classes, self.hparams.num_samples)\n",
    "        \n",
    "        # Build text vocabulary\n",
    "        all_words = set()\n",
    "        for text in self.texts:\n",
    "            all_words.update(text.split())\n",
    "        \n",
    "        self.vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        for i, word in enumerate(sorted(all_words)[:self.hparams.vocab_size-2]):\n",
    "            self.vocab[word] = i + 2\n",
    "        \n",
    "        print(f\"✓ Generated {len(self.images)} multi-modal samples\")\n",
    "        print(f\"✓ Vocabulary size: {len(self.vocab)}\")\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if not hasattr(self, 'images'):\n",
    "            self.prepare_data()\n",
    "        \n",
    "        # Split data\n",
    "        train_size = int(0.8 * self.hparams.num_samples)\n",
    "        val_size = int(0.1 * self.hparams.num_samples)\n",
    "        \n",
    "        indices = list(range(self.hparams.num_samples))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:train_size + val_size]\n",
    "        test_indices = indices[train_size + val_size:]\n",
    "        \n",
    "        if stage == \"fit\" or stage is None:\n",
    "            train_images = [self.images[i] for i in train_indices]\n",
    "            train_texts = [self.texts[i] for i in train_indices]\n",
    "            train_labels = [self.labels[i] for i in train_indices]\n",
    "            \n",
    "            self.train_dataset = MultiModalDataset(\n",
    "                train_images, train_texts, train_labels, \n",
    "                self.image_transform, self.vocab, self.hparams.max_text_length\n",
    "            )\n",
    "            \n",
    "            val_images = [self.images[i] for i in val_indices]\n",
    "            val_texts = [self.texts[i] for i in val_indices]\n",
    "            val_labels = [self.labels[i] for i in val_indices]\n",
    "            \n",
    "            self.val_dataset = MultiModalDataset(\n",
    "                val_images, val_texts, val_labels,\n",
    "                self.image_transform, self.vocab, self.hparams.max_text_length\n",
    "            )\n",
    "        \n",
    "        if stage == \"test\" or stage is None:\n",
    "            test_images = [self.images[i] for i in test_indices]\n",
    "            test_texts = [self.texts[i] for i in test_indices]\n",
    "            test_labels = [self.labels[i] for i in test_indices]\n",
    "            \n",
    "            self.test_dataset = MultiModalDataset(\n",
    "                test_images, test_texts, test_labels,\n",
    "                self.image_transform, self.vocab, self.hparams.max_text_length\n",
    "            )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            collate_fn=self._collate_fn\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            collate_fn=self._collate_fn\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            collate_fn=self._collate_fn\n",
    "        )\n",
    "    \n",
    "    def _collate_fn(self, batch):\n",
    "        \"\"\"Custom collate function for multi-modal data\"\"\"\n",
    "        images = torch.stack([item['image'] for item in batch])\n",
    "        texts = torch.stack([item['text'] for item in batch])\n",
    "        labels = torch.stack([item['label'] for item in batch])\n",
    "        \n",
    "        return {\n",
    "            'image': images,\n",
    "            'text': texts,\n",
    "            'label': labels\n",
    "        }\n",
    "\n",
    "# Test Multi-Modal DataModule\n",
    "multimodal_dm = MultiModalDataModule(\n",
    "    num_samples=500,\n",
    "    image_size=(32, 32),\n",
    "    vocab_size=100,\n",
    "    max_text_length=20,\n",
    "    num_classes=3,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "multimodal_dm.setup(\"fit\")\n",
    "\n",
    "# Test a batch\n",
    "train_loader = multimodal_dm.train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"✓ Multi-Modal DataModule created\")\n",
    "print(f\"Image batch shape: {batch['image'].shape}\")\n",
    "print(f\"Text batch shape: {batch['text'].shape}\")\n",
    "print(f\"Labels batch shape: {batch['label'].shape}\")\n",
    "print(f\"Sample text: {batch['text'][0][:10].tolist()}\")\n",
    "```\n",
    "\n",
    "## Data Augmentation and Transforms\n",
    "\n",
    "```python\n",
    "class AugmentationDataModule(pl.LightningDataModule):\n",
    "    \"\"\"DataModule with advanced augmentation strategies\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_datamodule: pl.LightningDataModule,\n",
    "        augment_prob: float = 0.8,\n",
    "        mixup_alpha: float = 0.2,\n",
    "        cutmix_prob: float = 0.5,\n",
    "        use_mixup: bool = False,\n",
    "        use_cutmix: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.base_dm = base_datamodule\n",
    "        self.save_hyperparameters(ignore=['base_datamodule'])\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.base_dm.setup(stage)\n",
    "        \n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # Wrap datasets with augmentation\n",
    "            self.train_dataset = AugmentedDataset(\n",
    "                self.base_dm.train_dataset,\n",
    "                use_mixup=self.hparams.use_mixup,\n",
    "                use_cutmix=self.hparams.use_cutmix,\n",
    "                mixup_alpha=self.hparams.mixup_alpha,\n",
    "                cutmix_prob=self.hparams.cutmix_prob\n",
    "            )\n",
    "            self.val_dataset = self.base_dm.val_dataset\n",
    "        \n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = self.base_dm.test_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.base_dm.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.base_dm.hparams.num_workers,\n",
    "            collate_fn=self._augmented_collate_fn\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.base_dm.val_dataloader()\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.base_dm.test_dataloader()\n",
    "    \n",
    "    def _augmented_collate_fn(self, batch):\n",
    "        \"\"\"Collate function with batch-level augmentations\"\"\"\n",
    "        # Standard collation\n",
    "        if isinstance(batch[0], (list, tuple)):\n",
    "            inputs = torch.stack([item[0] for item in batch])\n",
    "            targets = torch.stack([item[1] for item in batch])\n",
    "        else:\n",
    "            return torch.utils.data.dataloader.default_collate(batch)\n",
    "        \n",
    "        # Apply batch augmentations\n",
    "        if self.training and self.hparams.use_mixup and np.random.rand() < 0.5:\n",
    "            inputs, targets = self._mixup(inputs, targets)\n",
    "        elif self.training and self.hparams.use_cutmix and np.random.rand() < self.hparams.cutmix_prob:\n",
    "            inputs, targets = self._cutmix(inputs, targets)\n",
    "            \n",
    "        return inputs, targets\n",
    "    \n",
    "    def _mixup(self, x, y):\n",
    "        \"\"\"Apply MixUp augmentation\"\"\"\n",
    "        lam = np.random.beta(self.hparams.mixup_alpha, self.hparams.mixup_alpha)\n",
    "        batch_size = x.size(0)\n",
    "        index = torch.randperm(batch_size)\n",
    "        \n",
    "        mixed_x = lam * x + (1 - lam) * x[index]\n",
    "        y_a, y_b = y, y[index]\n",
    "        \n",
    "        return mixed_x, (y_a, y_b, lam)\n",
    "    \n",
    "    def _cutmix(self, x, y):\n",
    "        \"\"\"Apply CutMix augmentation\"\"\"\n",
    "        lam = np.random.beta(1.0, 1.0)\n",
    "        rand_index = torch.randperm(x.size(0))\n",
    "        \n",
    "        # Get random box\n",
    "        W, H = x.size(2), x.size(3)\n",
    "        cut_rat = np.sqrt(1. - lam)\n",
    "        cut_w = int(W * cut_rat)\n",
    "        cut_h = int(H * cut_rat)\n",
    "        \n",
    "        cx = np.random.randint(W)\n",
    "        cy = np.random.randint(H)\n",
    "        \n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "        \n",
    "        x[:, :, bbx1:bbx2, bby1:bby2] = x[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "        \n",
    "        return x, (y, y[rand_index], lam)\n",
    "\n",
    "class AugmentedDataset(Dataset):\n",
    "    \"\"\"Wrapper dataset for applying augmentations\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dataset, use_mixup=False, use_cutmix=False, \n",
    "                 mixup_alpha=0.2, cutmix_prob=0.5):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.use_mixup = use_mixup\n",
    "        self.use_cutmix = use_cutmix\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.cutmix_prob = cutmix_prob\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.base_dataset[idx]\n",
    "\n",
    "print(\"✓ Advanced augmentation DataModule patterns defined\")\n",
    "```\n",
    "\n",
    "## Error Handling and Recovery\n",
    "\n",
    "```python\n",
    "class RobustDataModule(pl.LightningDataModule):\n",
    "    \"\"\"DataModule with comprehensive error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, base_datamodule: pl.LightningDataModule, max_retries: int = 3):\n",
    "        super().__init__()\n",
    "        self.base_dm = base_datamodule\n",
    "        self.max_retries = max_retries\n",
    "        self.failed_samples = set()\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        try:\n",
    "            self.base_dm.setup(stage)\n",
    "            \n",
    "            # Wrap datasets with error handling\n",
    "            if hasattr(self.base_dm, 'train_dataset'):\n",
    "                self.train_dataset = RobustDataset(self.base_dm.train_dataset, self.max_retries)\n",
    "            if hasattr(self.base_dm, 'val_dataset'):\n",
    "                self.val_dataset = RobustDataset(self.base_dm.val_dataset, self.max_retries)\n",
    "            if hasattr(self.base_dm, 'test_dataset'):\n",
    "                self.test_dataset = RobustDataset(self.base_dm.test_dataset, self.max_retries)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Setup failed: {e}\")\n",
    "            # Implement fallback strategy\n",
    "            self._create_fallback_datasets()\n",
    "    \n",
    "    def _create_fallback_datasets(self):\n",
    "        \"\"\"Create minimal fallback datasets when main setup fails\"\"\"\n",
    "        print(\"⚠ Creating fallback datasets...\")\n",
    "        fallback_data = torch.randn(100, 3, 32, 32)\n",
    "        fallback_targets = torch.randint(0, 10, (100,))\n",
    "        \n",
    "        fallback_dataset = torch.utils.data.TensorDataset(fallback_data, fallback_targets)\n",
    "        \n",
    "        self.train_dataset = fallback_dataset\n",
    "        self.val_dataset = fallback_dataset\n",
    "        self.test_dataset = fallback_dataset\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self._create_robust_dataloader(self.train_dataset, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self._create_robust_dataloader(self.val_dataset, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self._create_robust_dataloader(self.test_dataset, shuffle=False)\n",
    "    \n",
    "    def _create_robust_dataloader(self, dataset, shuffle=False):\n",
    "        \"\"\"Create DataLoader with error handling\"\"\"\n",
    "        try:\n",
    "            return DataLoader(\n",
    "                dataset,\n",
    "                batch_size=getattr(self.base_dm.hparams, 'batch_size', 32),\n",
    "                shuffle=shuffle,\n",
    "                num_workers=0,  # Safer for debugging\n",
    "                collate_fn=self._robust_collate_fn\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"❌ DataLoader creation failed: {e}\")\n",
    "            # Return minimal dataloader\n",
    "            return DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    def _robust_collate_fn(self, batch):\n",
    "        \"\"\"Collate function with error recovery\"\"\"\n",
    "        try:\n",
    "            return torch.utils.data.dataloader.default_collate(batch)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Collate error: {e}, using fallback\")\n",
    "            # Filter out problematic samples\n",
    "            valid_batch = []\n",
    "            for item in batch:\n",
    "                try:\n",
    "                    # Test if item can be processed\n",
    "                    _ = torch.utils.data.dataloader.default_collate([item])\n",
    "                    valid_batch.append(item)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if valid_batch:\n",
    "                return torch.utils.data.dataloader.default_collate(valid_batch)\n",
    "            else:\n",
    "                # Return dummy batch\n",
    "                return torch.randn(1, 3, 32, 32), torch.tensor([0])\n",
    "\n",
    "class RobustDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper with error handling and recovery\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dataset, max_retries=3):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.max_retries = max_retries\n",
    "        self.failed_indices = set()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = idx\n",
    "        retries = 0\n",
    "        \n",
    "        while retries < self.max_retries:\n",
    "            try:\n",
    "                # Skip known failed samples\n",
    "                while idx in self.failed_indices and retries < len(self.base_dataset):\n",
    "                    idx = (idx + 1) % len(self.base_dataset)\n",
    "                \n",
    "                return self.base_dataset[idx]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Sample {idx} failed (attempt {retries + 1}): {e}\")\n",
    "                self.failed_indices.add(idx)\n",
    "                retries += 1\n",
    "                idx = (idx + 1) % len(self.base_dataset)\n",
    "        \n",
    "        # Return fallback sample\n",
    "        print(f\"❌ All retries failed for sample {original_idx}, returning fallback\")\n",
    "        return self._get_fallback_sample()\n",
    "    \n",
    "    def _get_fallback_sample(self):\n",
    "        \"\"\"Generate a fallback sample when all else fails\"\"\"\n",
    "        # Create a simple fallback based on expected data structure\n",
    "        try:\n",
    "            sample = self.base_dataset[0]\n",
    "            if isinstance(sample, (list, tuple)) and len(sample) == 2:\n",
    "                data, target = sample\n",
    "                fallback_data = torch.zeros_like(data)\n",
    "                return fallback_data, target\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Ultimate fallback\n",
    "        return torch.zeros(3, 32, 32), torch.tensor(0)\n",
    "\n",
    "print(\"✓ Robust error handling patterns defined\")\n",
    "```\n",
    "\n",
    "## DataModule Testing Framework\n",
    "\n",
    "```python\n",
    "class DataModuleTestSuite:\n",
    "    \"\"\"Comprehensive testing suite for DataModules\"\"\"\n",
    "    \n",
    "    def __init__(self, datamodule: pl.LightningDataModule):\n",
    "        self.dm = datamodule\n",
    "        self.test_results = {}\n",
    "    \n",
    "    def run_all_tests(self):\n",
    "        \"\"\"Run comprehensive test suite\"\"\"\n",
    "        print(f\"=== Testing {type(self.dm).__name__} ===\")\n",
    "        \n",
    "        tests = [\n",
    "            (\"Setup\", self._test_setup),\n",
    "            (\"DataLoader Creation\", self._test_dataloader_creation),\n",
    "            (\"Batch Loading\", self._test_batch_loading),\n",
    "            (\"Data Consistency\", self._test_data_consistency),\n",
    "            (\"Memory Usage\", self._test_memory_usage),\n",
    "            (\"Performance\", self._test_performance),\n",
    "            (\"Error Handling\", self._test_error_handling)\n",
    "        ]\n",
    "        \n",
    "        for test_name, test_func in tests:\n",
    "            print(f\"\\n--- {test_name} ---\")\n",
    "            try:\n",
    "                result = test_func()\n",
    "                self.test_results[test_name] = {\"status\": \"PASSED\", \"result\": result}\n",
    "                print(f\"✅ {test_name}: PASSED\")\n",
    "            except Exception as e:\n",
    "                self.test_results[test_name] = {\"status\": \"FAILED\", \"error\": str(e)}\n",
    "                print(f\"❌ {test_name}: FAILED - {e}\")\n",
    "        \n",
    "        self._print_summary()\n",
    "        return self.test_results\n",
    "    \n",
    "    def _test_setup(self):\n",
    "        \"\"\"Test DataModule setup\"\"\"\n",
    "        stages = [\"fit\", \"validate\", \"test\"]\n",
    "        results = {}\n",
    "        \n",
    "        for stage in stages:\n",
    "            try:\n",
    "                self.dm.setup(stage)\n",
    "                results[stage] = \"OK\"\n",
    "            except Exception as e:\n",
    "                results[stage] = f\"Error: {e}\"\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _test_dataloader_creation(self):\n",
    "        \"\"\"Test DataLoader creation\"\"\"\n",
    "        self.dm.setup(\"fit\")\n",
    "        \n",
    "        loaders = {}\n",
    "        if hasattr(self.dm, 'train_dataloader'):\n",
    "            loaders['train'] = len(self.dm.train_dataloader())\n",
    "        if hasattr(self.dm, 'val_dataloader'):\n",
    "            loaders['val'] = len(self.dm.val_dataloader())\n",
    "        if hasattr(self.dm, 'test_dataloader'):\n",
    "            loaders['test'] = len(self.dm.test_dataloader())\n",
    "            \n",
    "        return loaders\n",
    "    \n",
    "    def _test_batch_loading(self):\n",
    "        \"\"\"Test batch loading from all dataloaders\"\"\"\n",
    "        self.dm.setup(\"fit\")\n",
    "        results = {}\n",
    "        \n",
    "        loaders = []\n",
    "        if hasattr(self.dm, 'train_dataloader'):\n",
    "            loaders.append(('train', self.dm.train_dataloader()))\n",
    "        if hasattr(self.dm, 'val_dataloader'):\n",
    "            loaders.append(('val', self.dm.val_dataloader()))\n",
    "        \n",
    "        for name, loader in loaders:\n",
    "            batch = next(iter(loader))\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                results[name] = {\n",
    "                    'batch_size': len(batch[0]),\n",
    "                    'input_shape': batch[0].shape if hasattr(batch[0], 'shape') else str(type(batch[0])),\n",
    "                    'target_shape': batch[1].shape if hasattr(batch[1], 'shape') else str(type(batch[1]))\n",
    "                }\n",
    "            else:\n",
    "                results[name] = {'type': str(type(batch))}\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _test_data_consistency(self):\n",
    "        \"\"\"Test data consistency across epochs\"\"\"\n",
    "        self.dm.setup(\"fit\")\n",
    "        train_loader = self.dm.train_dataloader()\n",
    "        \n",
    "        # Get first batch from two different iterations\n",
    "        batch1 = next(iter(train_loader))\n",
    "        batch2 = next(iter(train_loader))\n",
    "        \n",
    "        if isinstance(batch1, (list, tuple)) and isinstance(batch2, (list, tuple)):\n",
    "            return {\n",
    "                'shapes_consistent': batch1[0].shape == batch2[0].shape,\n",
    "                'dtypes_consistent': batch1[0].dtype == batch2[0].dtype,\n",
    "                'batch_sizes_equal': len(batch1[0]) == len(batch2[0])\n",
    "            }\n",
    "        return {'status': 'Cannot test - unexpected batch format'}\n",
    "    \n",
    "    def _test_memory_usage(self):\n",
    "        \"\"\"Test memory usage during data loading\"\"\"\n",
    "        import psutil\n",
    "        import os\n",
    "        \n",
    "        process = psutil.Process(os.getpid())\n",
    "        initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        self.dm.setup(\"fit\")\n",
    "        train_loader = self.dm.train_dataloader()\n",
    "        \n",
    "        # Load several batches\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            if i >= 5:  # Load 5 batches\n",
    "                break\n",
    "        \n",
    "        final_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        memory_increase = final_memory - initial_memory\n",
    "        \n",
    "        return {\n",
    "            'initial_memory_mb': round(initial_memory, 2),\n",
    "            'final_memory_mb': round(final_memory, 2),\n",
    "            'memory_increase_mb': round(memory_increase, 2)\n",
    "        }\n",
    "    \n",
    "    def _test_performance(self):\n",
    "        \"\"\"Test loading performance\"\"\"\n",
    "        import time\n",
    "        \n",
    "        self.dm.setup(\"fit\")\n",
    "        train_loader = self.dm.train_dataloader()\n",
    "        \n",
    "        # Warmup\n",
    "        next(iter(train_loader))\n",
    "        \n",
    "        # Time batch loading\n",
    "        start_time = time.time()\n",
    "        batch_count = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch_count += 1\n",
    "            if batch_count >= 10:\n",
    "                break\n",
    "        \n",
    "        end_time = time.time()\n",
    "        avg_batch_time = (end_time - start_time) / batch_count\n",
    "        \n",
    "        return {\n",
    "            'batches_tested': batch_count,\n",
    "            'avg_batch_time_ms': round(avg_batch_time * 1000, 2),\n",
    "            'estimated_batches_per_sec': round(1 / avg_batch_time, 2)\n",
    "        }\n",
    "    \n",
    "    def _test_error_handling(self):\n",
    "        \"\"\"Test error handling capabilities\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Test with invalid stage\n",
    "        try:\n",
    "            self.dm.setup(\"invalid_stage\")\n",
    "            results['invalid_stage'] = \"No error raised\"\n",
    "        except:\n",
    "            results['invalid_stage'] = \"Error properly handled\"\n",
    "        \n",
    "        # Test multiple setups\n",
    "        try:\n",
    "            self.dm.setup(\"fit\")\n",
    "            self.dm.setup(\"fit\")  # Second setup\n",
    "            results['multiple_setup'] = \"OK\"\n",
    "        except Exception as e:\n",
    "            results['multiple_setup'] = f\"Error: {e}\"\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _print_summary(self):\n",
    "        \"\"\"Print test summary\"\"\"\n",
    "        print(f\"\\n=== Test Summary ===\")\n",
    "        passed = sum(1 for result in self.test_results.values() if result['status'] == 'PASSED')\n",
    "        total = len(self.test_results)\n",
    "        \n",
    "        print(f\"Tests Passed: {passed}/{total}\")\n",
    "        \n",
    "        if passed == total:\n",
    "            print(\"🎉 All tests passed!\")\n",
    "        else:\n",
    "            print(\"⚠ Some tests failed. Check results above.\")\n",
    "\n",
    "# Test all our DataModules\n",
    "datamodules_to_test = [\n",
    "    vision_dm,\n",
    "    nlp_dm,\n",
    "    tabular_dm,\n",
    "    ts_dm,\n",
    "    multimodal_dm\n",
    "]\n",
    "\n",
    "print(\"=== DataModule Testing Framework ===\")\n",
    "for dm in datamodules_to_test:\n",
    "    test_suite = DataModuleTestSuite(dm)\n",
    "    results = test_suite.run_all_tests()\n",
    "    print(f\"\\n{'='*50}\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "This comprehensive notebook covered building robust LightningDataModules for various data types and scenarios:\n",
    "\n",
    "**Core DataModule Concepts:**\n",
    "- **Structure**: `prepare_data()` for one-time setup, `setup()` for dataset creation, and individual dataloader methods\n",
    "- **Best Practices**: Proper data splits, normalization, and hyperparameter saving\n",
    "- **Flexibility**: Support for different stages (fit, validate, test, predict)\n",
    "\n",
    "**Data Type Implementations:**\n",
    "- **Vision DataModules**: Image preprocessing, augmentation, and batch handling\n",
    "- **NLP DataModules**: Text tokenization, vocabulary building, and sequence padding\n",
    "- **Tabular DataModules**: Feature scaling, categorical encoding, and structured data handling\n",
    "- **Time Series DataModules**: Sequential data processing and temporal splitting\n",
    "- **Multi-Modal DataModules**: Combined vision and text data processing\n",
    "\n",
    "**Advanced Patterns:**\n",
    "- **Augmentation Integration**: MixUp, CutMix, and batch-level augmentations\n",
    "- **Error Handling**: Robust data loading with fallback mechanisms\n",
    "- **Performance Optimization**: Memory management and loading speed optimization\n",
    "- **Testing Framework**: Comprehensive validation of DataModule functionality\n",
    "\n",
    "**Production Considerations:**\n",
    "- Memory usage monitoring and optimization\n",
    "- Error recovery and graceful degradation\n",
    "- Performance benchmarking and bottleneck identification\n",
    "- Comprehensive testing and validation\n",
    "\n",
    "**DataModule Benefits:**\n",
    "- **Reproducibility**: Consistent data processing across experiments\n",
    "- **Modularity**: Reusable data loading logic independent of model code\n",
    "- **Distributed Training**: Automatic handling of multi-GPU data loading\n",
    "- **Experimentation**: Easy swapping of different data processing strategies\n",
    "\n",
    "The DataModule pattern is essential for building maintainable, scalable ML pipelines in PyTorch Lightning, providing a clean separation between data processing and model training logic."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
