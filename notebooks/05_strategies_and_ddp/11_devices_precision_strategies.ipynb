{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ac39e2",
   "metadata": {},
   "source": [
    "# Devices, Precision, and Training Strategies\n",
    "\n",
    "**File Location:** `notebooks/05_strategies_and_ddp/11_devices_precision_strategies.ipynb`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook covers Lightning's device management, precision strategies, and training approaches. Learn to efficiently use CPUs, GPUs, and handle different precision modes for optimal training performance.\n",
    "\n",
    "## Device Management\n",
    "\n",
    "### Device Detection and Configuration\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class DeviceAwareModel(pl.LightningModule):\n",
    "    \"\"\"Model that demonstrates device awareness\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        # Log device information occasionally\n",
    "        if batch_idx == 0:\n",
    "            self.log('device_type', float(x.device.type == 'cuda'))\n",
    "            self.log('device_index', float(x.device.index) if x.device.index is not None else -1)\n",
    "        \n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Create dataset\n",
    "def create_device_dataset(num_samples=2000, input_size=128, num_classes=10):\n",
    "    torch.manual_seed(42)\n",
    "    X = torch.randn(num_samples, input_size)\n",
    "    weights = torch.randn(input_size)\n",
    "    logits = X @ weights\n",
    "    y = torch.div(logits - logits.min(), (logits.max() - logits.min()) / (num_classes - 1), rounding_mode='floor').long()\n",
    "    y = torch.clamp(y, 0, num_classes - 1)\n",
    "    return X, y\n",
    "\n",
    "X, y = create_device_dataset()\n",
    "dataset = TensorDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Check available devices\n",
    "def check_available_devices():\n",
    "    \"\"\"Check and report available computing devices\"\"\"\n",
    "    \n",
    "    print(\"=== Device Availability ===\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA devices: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"  GPU {i}: {props.name}\")\n",
    "            print(f\"    Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
    "            print(f\"    Compute Capability: {props.major}.{props.minor}\")\n",
    "    \n",
    "    # CPU info\n",
    "    print(f\"CPU cores: {torch.get_num_threads()}\")\n",
    "    \n",
    "    # MPS (Apple Silicon) support\n",
    "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        print(\"Apple MPS available: Yes\")\n",
    "    else:\n",
    "        print(\"Apple MPS available: No\")\n",
    "\n",
    "check_available_devices()\n",
    "print(\"✓ Device setup completed\")\n",
    "```\n",
    "\n",
    "### Training on Different Devices\n",
    "\n",
    "```python\n",
    "def train_on_different_devices():\n",
    "    \"\"\"Compare training on different available devices\"\"\"\n",
    "    \n",
    "    # Define device configurations to test\n",
    "    device_configs = []\n",
    "    \n",
    "    # CPU training\n",
    "    device_configs.append({\n",
    "        'name': 'CPU',\n",
    "        'accelerator': 'cpu',\n",
    "        'devices': 1,\n",
    "        'precision': '32-true'\n",
    "    })\n",
    "    \n",
    "    # GPU training if available\n",
    "    if torch.cuda.is_available():\n",
    "        device_configs.append({\n",
    "            'name': 'Single GPU',\n",
    "            'accelerator': 'gpu',\n",
    "            'devices': 1,\n",
    "            'precision': '16-mixed'\n",
    "        })\n",
    "        \n",
    "        if torch.cuda.device_count() > 1:\n",
    "            device_configs.append({\n",
    "                'name': 'Multi GPU',\n",
    "                'accelerator': 'gpu', \n",
    "                'devices': 2,\n",
    "                'precision': '16-mixed'\n",
    "            })\n",
    "    \n",
    "    # MPS (Apple Silicon) if available\n",
    "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device_configs.append({\n",
    "            'name': 'Apple MPS',\n",
    "            'accelerator': 'mps',\n",
    "            'devices': 1,\n",
    "            'precision': '32-true'\n",
    "        })\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in device_configs:\n",
    "        print(f\"\\n=== Training on {config['name']} ===\")\n",
    "        \n",
    "        model = DeviceAwareModel()\n",
    "        \n",
    "        try:\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=2,\n",
    "                accelerator=config['accelerator'],\n",
    "                devices=config['devices'],\n",
    "                precision=config['precision'],\n",
    "                enable_checkpointing=False,\n",
    "                logger=False,\n",
    "                enable_progress_bar=False,\n",
    "                limit_train_batches=30,\n",
    "                limit_val_batches=10\n",
    "            )\n",
    "            \n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            trainer.fit(model, train_loader, val_loader)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            final_acc = trainer.callback_metrics.get('val_acc', 0)\n",
    "            \n",
    "            results[config['name']] = {\n",
    "                'time': training_time,\n",
    "                'accuracy': final_acc,\n",
    "                'config': config\n",
    "            }\n",
    "            \n",
    "            print(f\"Training time: {training_time:.2f}s\")\n",
    "            print(f\"Final accuracy: {final_acc:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to train on {config['name']}: {e}\")\n",
    "            results[config['name']] = {'error': str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "device_results = train_on_different_devices()\n",
    "\n",
    "# Display comparison\n",
    "print(f\"\\n📊 Device Performance Comparison:\")\n",
    "successful_runs = {k: v for k, v in device_results.items() if 'error' not in v}\n",
    "\n",
    "if successful_runs:\n",
    "    fastest_time = min(result['time'] for result in successful_runs.values())\n",
    "    \n",
    "    for device, result in successful_runs.items():\n",
    "        speedup = fastest_time / result['time']\n",
    "        print(f\"{device:12} | Time: {result['time']:5.2f}s | Speedup: {speedup:.2f}x | Acc: {result['accuracy']:.4f}\")\n",
    "```\n",
    "\n",
    "## Precision Strategies\n",
    "\n",
    "### Comparing Different Precision Modes\n",
    "\n",
    "```python\n",
    "class PrecisionTestModel(pl.LightningModule):\n",
    "    \"\"\"Model to test different precision strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Model with operations sensitive to precision\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        \n",
    "        # Track precision-related metrics\n",
    "        self.gradient_scales = []\n",
    "        self.loss_values = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Track loss values for precision analysis\n",
    "        self.loss_values.append(loss.item())\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        \n",
    "        # Log precision-related info\n",
    "        if batch_idx % 25 == 0:\n",
    "            self.log('loss_magnitude', torch.log10(loss.abs() + 1e-8))\n",
    "            \n",
    "            # Log gradient scale if using AMP\n",
    "            if hasattr(self.trainer, 'precision_plugin') and hasattr(self.trainer.precision_plugin, 'scaler'):\n",
    "                scaler = self.trainer.precision_plugin.scaler\n",
    "                if scaler is not None:\n",
    "                    scale = scaler.get_scale()\n",
    "                    self.gradient_scales.append(scale)\n",
    "                    self.log('gradient_scale', scale)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Use optimizer settings that work well with different precisions\n",
    "        return torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=1e-3,\n",
    "            weight_decay=1e-4,\n",
    "            eps=1e-4  # Larger eps for numerical stability\n",
    "        )\n",
    "\n",
    "def compare_precision_strategies():\n",
    "    \"\"\"Compare different precision strategies\"\"\"\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"⚠️ CUDA not available, skipping precision comparison\")\n",
    "        return {}\n",
    "    \n",
    "    precision_configs = [\n",
    "        {'name': 'FP32', 'precision': '32-true'},\n",
    "        {'name': 'Mixed Precision', 'precision': '16-mixed'},\n",
    "        {'name': 'BFloat16', 'precision': 'bf16-mixed'} if torch.cuda.is_bf16_supported() else None\n",
    "    ]\n",
    "    \n",
    "    # Remove None entries\n",
    "    precision_configs = [config for config in precision_configs if config is not None]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in precision_configs:\n",
    "        print(f\"\\n=== Testing {config['name']} Precision ===\")\n",
    "        \n",
    "        model = PrecisionTestModel()\n",
    "        \n",
    "        try:\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=3,\n",
    "                accelerator='gpu',\n",
    "                devices=1,\n",
    "                precision=config['precision'],\n",
    "                enable_checkpointing=False,\n",
    "                logger=False,\n",
    "                enable_progress_bar=False,\n",
    "                limit_train_batches=50,\n",
    "                limit_val_batches=15\n",
    "            )\n",
    "            \n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            trainer.fit(model, train_loader, val_loader)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            final_acc = trainer.callback_metrics.get('val_acc', 0)\n",
    "            final_loss = trainer.callback_metrics.get('val_loss', float('inf'))\n",
    "            \n",
    "            # Get precision-specific metrics\n",
    "            avg_loss = np.mean(model.loss_values) if model.loss_values else 0\n",
    "            loss_std = np.std(model.loss_values) if model.loss_values else 0\n",
    "            \n",
    "            results[config['name']] = {\n",
    "                'time': training_time,\n",
    "                'accuracy': final_acc,\n",
    "                'final_loss': final_loss,\n",
    "                'avg_loss': avg_loss,\n",
    "                'loss_std': loss_std,\n",
    "                'gradient_scales': len(model.gradient_scales)\n",
    "            }\n",
    "            \n",
    "            print(f\"Training time: {training_time:.2f}s\")\n",
    "            print(f\"Final accuracy: {final_acc:.4f}\")\n",
    "            print(f\"Loss stability (std): {loss_std:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed with {config['name']}: {e}\")\n",
    "            results[config['name']] = {'error': str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "precision_results = compare_precision_strategies()\n",
    "\n",
    "# Display precision comparison\n",
    "print(f\"\\n📊 Precision Strategy Comparison:\")\n",
    "successful_runs = {k: v for k, v in precision_results.items() if 'error' not in v}\n",
    "\n",
    "if successful_runs:\n",
    "    for precision, result in successful_runs.items():\n",
    "        print(f\"{precision:15} | Time: {result['time']:5.2f}s | Acc: {result['accuracy']:.4f} | Loss Std: {result['loss_std']:.6f}\")\n",
    "```\n",
    "\n",
    "## Training Strategies\n",
    "\n",
    "### Strategy Selection and Configuration\n",
    "\n",
    "```python\n",
    "class StrategyTestModel(pl.LightningModule):\n",
    "    \"\"\"Model to test different training strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Slightly larger model to benefit from parallelization\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        \n",
    "        # Strategy-specific tracking\n",
    "        self.strategy_info = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        \n",
    "        # Log strategy-specific information\n",
    "        if batch_idx == 0:\n",
    "            strategy = str(type(self.trainer.strategy).__name__)\n",
    "            self.strategy_info['strategy_name'] = strategy\n",
    "            \n",
    "            # Log device information\n",
    "            device_count = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "            self.log('device_count', float(device_count))\n",
    "            self.log('strategy_hash', float(hash(strategy) % 1000))  # Simple identifier\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "def test_training_strategies():\n",
    "    \"\"\"Test different training strategies\"\"\"\n",
    "    \n",
    "    strategy_configs = [\n",
    "        {'name': 'Single Device', 'strategy': 'auto', 'devices': 1},\n",
    "    ]\n",
    "    \n",
    "    # Add multi-GPU strategies if available\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        strategy_configs.extend([\n",
    "            {'name': 'Data Parallel', 'strategy': 'dp', 'devices': 2},\n",
    "            {'name': 'DDP', 'strategy': 'ddp', 'devices': 2},\n",
    "        ])\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in strategy_configs:\n",
    "        print(f\"\\n=== Testing {config['name']} Strategy ===\")\n",
    "        \n",
    "        model = StrategyTestModel()\n",
    "        \n",
    "        try:\n",
    "            # Handle different device configurations\n",
    "            if torch.cuda.is_available():\n",
    "                accelerator = 'gpu'\n",
    "                devices = min(config['devices'], torch.cuda.device_count())\n",
    "            else:\n",
    "                accelerator = 'cpu'\n",
    "                devices = 1\n",
    "                config['strategy'] = 'auto'  # Only auto strategy works with CPU\n",
    "            \n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=2,\n",
    "                accelerator=accelerator,\n",
    "                devices=devices,\n",
    "                strategy=config['strategy'],\n",
    "                enable_checkpointing=False,\n",
    "                logger=False,\n",
    "                enable_progress_bar=False,\n",
    "                limit_train_batches=25,\n",
    "                limit_val_batches=10\n",
    "            )\n",
    "            \n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            trainer.fit(model, train_loader, val_loader)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            final_acc = trainer.callback_metrics.get('val_acc', 0)\n",
    "            \n",
    "            results[config['name']] = {\n",
    "                'time': training_time,\n",
    "                'accuracy': final_acc,\n",
    "                'strategy': config['strategy'],\n",
    "                'devices_used': devices\n",
    "            }\n",
    "            \n",
    "            print(f\"Strategy: {config['strategy']}\")\n",
    "            print(f\"Devices used: {devices}\")\n",
    "            print(f\"Training time: {training_time:.2f}s\")\n",
    "            print(f\"Final accuracy: {final_acc:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed with {config['name']} strategy: {e}\")\n",
    "            results[config['name']] = {'error': str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "strategy_results = test_training_strategies()\n",
    "\n",
    "# Display strategy comparison\n",
    "print(f\"\\n📊 Training Strategy Comparison:\")\n",
    "for strategy, result in strategy_results.items():\n",
    "    if 'error' not in result:\n",
    "        efficiency = result['accuracy'] / result['time'] if result['time'] > 0 else 0\n",
    "        print(f\"{strategy:15} | Time: {result['time']:5.2f}s | Acc: {result['accuracy']:.4f} | Efficiency: {efficiency:.3f}\")\n",
    "    else:\n",
    "        print(f\"{strategy:15} | ERROR: {result['error']}\")\n",
    "```\n",
    "\n",
    "## Advanced Device and Strategy Configuration\n",
    "\n",
    "### Custom Strategy Implementation\n",
    "\n",
    "```python\n",
    "class CustomTrainingStrategy:\n",
    "    \"\"\"Custom training strategy configuration\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_optimal_config(task_type='classification', model_size='medium'):\n",
    "        \"\"\"Get optimal configuration based on task and model size\"\"\"\n",
    "        \n",
    "        config = {\n",
    "            'accelerator': 'auto',\n",
    "            'devices': 'auto',\n",
    "            'precision': '32-true',\n",
    "            'strategy': 'auto'\n",
    "        }\n",
    "        \n",
    "        # Device selection\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_count = torch.cuda.device_count()\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            \n",
    "            config['accelerator'] = 'gpu'\n",
    "            \n",
    "            # Device count based on model size and available GPUs\n",
    "            if model_size == 'small':\n",
    "                config['devices'] = 1\n",
    "            elif model_size == 'medium' and gpu_count >= 2:\n",
    "                config['devices'] = min(2, gpu_count)\n",
    "            elif model_size == 'large' and gpu_count >= 4:\n",
    "                config['devices'] = min(4, gpu_count)\n",
    "            else:\n",
    "                config['devices'] = 1\n",
    "            \n",
    "            # Precision based on GPU capability\n",
    "            if gpu_memory > 8:  # High-memory GPU\n",
    "                config['precision'] = '16-mixed'\n",
    "            else:  # Lower-memory GPU\n",
    "                config['precision'] = '16-mixed'\n",
    "            \n",
    "            # Strategy selection\n",
    "            if config['devices'] > 1:\n",
    "                config['strategy'] = 'ddp'  # DDP for multi-GPU\n",
    "            else:\n",
    "                config['strategy'] = 'auto'\n",
    "                \n",
    "        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            config['accelerator'] = 'mps'\n",
    "            config['devices'] = 1\n",
    "            config['precision'] = '32-true'  # MPS doesn't support mixed precision yet\n",
    "            \n",
    "        else:  # CPU fallback\n",
    "            config['accelerator'] = 'cpu'\n",
    "            config['devices'] = 1\n",
    "            config['precision'] = '32-true'\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_trainer(config, **trainer_kwargs):\n",
    "        \"\"\"Create trainer with optimal configuration\"\"\"\n",
    "        \n",
    "        default_kwargs = {\n",
    "            'max_epochs': 10,\n",
    "            'enable_checkpointing': True,\n",
    "            'log_every_n_steps': 50,\n",
    "            'check_val_every_n_epoch': 1\n",
    "        }\n",
    "        \n",
    "        # Merge configurations\n",
    "        final_kwargs = {**default_kwargs, **trainer_kwargs}\n",
    "        final_kwargs.update(config)\n",
    "        \n",
    "        return pl.Trainer(**final_kwargs)\n",
    "\n",
    "# Test custom strategy configuration\n",
    "def test_custom_strategy():\n",
    "    \"\"\"Test custom strategy configuration\"\"\"\n",
    "    \n",
    "    print(\"=== Custom Strategy Configuration ===\")\n",
    "    \n",
    "    # Test different model sizes\n",
    "    model_sizes = ['small', 'medium', 'large']\n",
    "    \n",
    "    for size in model_sizes:\n",
    "        config = CustomTrainingStrategy.get_optimal_config(model_size=size)\n",
    "        print(f\"\\n{size.upper()} Model Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Test if configuration is valid\n",
    "        try:\n",
    "            model = DeviceAwareModel()\n",
    "            trainer = CustomTrainingStrategy.create_trainer(\n",
    "                config,\n",
    "                max_epochs=1,\n",
    "                limit_train_batches=5,\n",
    "                limit_val_batches=2,\n",
    "                enable_checkpointing=False,\n",
    "                logger=False,\n",
    "                enable_progress_bar=False\n",
    "            )\n",
    "            \n",
    "            trainer.fit(model, train_loader, val_loader)\n",
    "            print(f\"  ✓ Configuration valid and tested\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Configuration failed: {e}\")\n",
    "\n",
    "test_custom_strategy()\n",
    "```\n",
    "\n",
    "## Production Configuration Guide\n",
    "\n",
    "### Complete Configuration Templates\n",
    "\n",
    "```python\n",
    "def get_production_configs():\n",
    "    \"\"\"Get production-ready configurations for different scenarios\"\"\"\n",
    "    \n",
    "    configs = {\n",
    "        'development': {\n",
    "            'description': 'Fast iteration for development',\n",
    "            'config': {\n",
    "                'accelerator': 'auto',\n",
    "                'devices': 1,\n",
    "                'precision': '32-true',\n",
    "                'strategy': 'auto',\n",
    "                'max_epochs': 5,\n",
    "                'limit_train_batches': 100,\n",
    "                'limit_val_batches': 50,\n",
    "                'fast_dev_run': False,\n",
    "                'enable_progress_bar': True,\n",
    "                'log_every_n_steps': 10\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        'single_gpu_production': {\n",
    "            'description': 'Single GPU production training',\n",
    "            'config': {\n",
    "                'accelerator': 'gpu',\n",
    "                'devices': 1,\n",
    "                'precision': '16-mixed',\n",
    "                'strategy': 'auto',\n",
    "                'max_epochs': 100,\n",
    "                'gradient_clip_val': 1.0,\n",
    "                'accumulate_grad_batches': 1,\n",
    "                'enable_progress_bar': True,\n",
    "                'log_every_n_steps': 100,\n",
    "                'check_val_every_n_epoch': 5\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        'multi_gpu_production': {\n",
    "            'description': 'Multi-GPU production training',\n",
    "            'config': {\n",
    "                'accelerator': 'gpu',\n",
    "                'devices': 4,\n",
    "                'precision': '16-mixed',\n",
    "                'strategy': 'ddp',\n",
    "                'max_epochs': 100,\n",
    "                'gradient_clip_val': 1.0,\n",
    "                'accumulate_grad_batches': 2,\n",
    "                'enable_progress_bar': True,\n",
    "                'log_every_n_steps': 50,\n",
    "                'check_val_every_n_epoch': 2,\n",
    "                'sync_batchnorm': True\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        'cpu_production': {\n",
    "            'description': 'CPU-only production training',\n",
    "            'config': {\n",
    "                'accelerator': 'cpu',\n",
    "                'devices': 1,\n",
    "                'precision': '32-true',\n",
    "                'strategy': 'auto',\n",
    "                'max_epochs': 50,\n",
    "                'gradient_clip_val': 1.0,\n",
    "                'accumulate_grad_batches': 4,\n",
    "                'enable_progress_bar': True,\n",
    "                'log_every_n_steps': 25,\n",
    "                'check_val_every_n_epoch': 1\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        'debugging': {\n",
    "            'description': 'Configuration for debugging issues',\n",
    "            'config': {\n",
    "                'accelerator': 'auto',\n",
    "                'devices': 1,\n",
    "                'precision': '32-true',  # Full precision for stability\n",
    "                'strategy': 'auto',\n",
    "                'max_epochs': 5,\n",
    "                'fast_dev_run': False,\n",
    "                'overfit_batches': 10,  # Overfit small subset\n",
    "                'limit_train_batches': 20,\n",
    "                'limit_val_batches': 10,\n",
    "                'enable_progress_bar': True,\n",
    "                'log_every_n_steps': 1,\n",
    "                'detect_anomaly': True,\n",
    "                'track_grad_norm': 2\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return configs\n",
    "\n",
    "def print_configuration_guide():\n",
    "    \"\"\"Print comprehensive configuration guide\"\"\"\n",
    "    \n",
    "    configs = get_production_configs()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"🚀 LIGHTNING CONFIGURATION GUIDE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for scenario, info in configs.items():\n",
    "        print(f\"\\n📋 {scenario.upper().replace('_', ' ')} CONFIGURATION\")\n",
    "        print(f\"Description: {info['description']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for key, value in info['config'].items():\n",
    "            print(f\"  {key:25} = {value}\")\n",
    "        \n",
    "        print(\"\\nUsage:\")\n",
    "        print(f\"  trainer = pl.Trainer(**{info['config']})\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"💡 Configuration Selection Guide:\")\n",
    "    print(\"  • Development: Fast iteration, debugging enabled\")\n",
    "    print(\"  • Single GPU: Production training on one GPU\")  \n",
    "    print(\"  • Multi GPU: Distributed training on multiple GPUs\")\n",
    "    print(\"  • CPU: Training without GPU acceleration\")\n",
    "    print(\"  • Debugging: Troubleshooting training issues\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Display configuration guide\n",
    "print_configuration_guide()\n",
    "\n",
    "# Test one of the configurations\n",
    "def test_production_config():\n",
    "    \"\"\"Test a production configuration\"\"\"\n",
    "    \n",
    "    configs = get_production_configs()\n",
    "    \n",
    "    # Test development config (safe for all systems)\n",
    "    dev_config = configs['development']['config']\n",
    "    \n",
    "    print(f\"\\n=== Testing Development Configuration ===\")\n",
    "    \n",
    "    model = DeviceAwareModel()\n",
    "    trainer = pl.Trainer(\n",
    "        **dev_config,\n",
    "        enable_checkpointing=False,\n",
    "        logger=False\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    final_acc = trainer.callback_metrics.get('val_acc', 0)\n",
    "    print(f\"✓ Development configuration test completed\")\n",
    "    print(f\"Final accuracy: {final_acc:.4f}\")\n",
    "\n",
    "test_production_config()\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered devices, precision, and training strategies in PyTorch Lightning:\n",
    "\n",
    "1. **Device Management**: Automatic detection and configuration of CPUs, GPUs, and MPS devices\n",
    "2. **Precision Strategies**: Comparing FP32, mixed precision, and BFloat16 training\n",
    "3. **Training Strategies**: Single device, data parallel, and distributed data parallel approaches  \n",
    "4. **Custom Configuration**: Building optimal configurations based on hardware and model requirements\n",
    "5. **Production Templates**: Ready-to-use configurations for different deployment scenarios\n",
    "\n",
    "Key device considerations:\n",
    "- **CPU**: Always available, good for small models and debugging\n",
    "- **GPU**: Best performance for most deep learning tasks\n",
    "- **MPS**: Apple Silicon acceleration, growing support\n",
    "- **Multi-GPU**: Significant speedup for larger models and datasets\n",
    "\n",
    "Precision trade-offs:\n",
    "- **FP32**: Maximum accuracy, higher memory usage\n",
    "- **Mixed Precision**: 1.5-2x speedup, minimal accuracy loss\n",
    "- **BFloat16**: Better numerical stability than FP16\n",
    "\n",
    "Strategy selection:\n",
    "- **Single Device**: Simplest setup, good for most cases\n",
    "- **Data Parallel**: Easy multi-GPU, but has limitations\n",
    "- **DDP**: Best multi-GPU strategy, more complex setup\n",
    "\n",
    "Production best practices:\n",
    "- Choose precision based on model sensitivity and hardware\n",
    "- Use DDP for multi-GPU production training\n",
    "- Enable gradient clipping for training stability\n",
    "- Monitor GPU utilization and memory usage\n",
    "- Test configurations thoroughly before deployment\n",
    "- Plan for debugging with appropriate fallback configs\n",
    "\n",
    "Next notebook: We'll explore distributed data parallel (DDP) training in detail."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
