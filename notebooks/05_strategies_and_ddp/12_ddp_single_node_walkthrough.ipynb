{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9a355f7",
   "metadata": {},
   "source": [
    "# File Location: notebooks/12_ddp_single_node_walkthrough.ipynb\n",
    "\n",
    "# Distributed Data Parallel (DDP) Single Node Walkthrough\n",
    "\n",
    "This notebook demonstrates how to implement Distributed Data Parallel training on a single node with multiple GPUs using PyTorch Lightning. We'll cover the setup, configuration, and best practices for multi-GPU training.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand DDP concepts and benefits\n",
    "- Implement multi-GPU training with Lightning\n",
    "- Handle data loading and synchronization\n",
    "- Monitor and optimize distributed training performance\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "# Check available GPUs\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "```\n",
    "\n",
    "## 1. Understanding DDP Basics\n",
    "\n",
    "```python\n",
    "# DDP Key Concepts Explanation\n",
    "class DDPConcepts:\n",
    "    \"\"\"\n",
    "    Key DDP concepts:\n",
    "    1. Process Groups: Groups of processes that communicate\n",
    "    2. Rank: Unique identifier for each process\n",
    "    3. World Size: Total number of processes\n",
    "    4. Local Rank: Process rank within a node\n",
    "    5. All-Reduce: Synchronization operation across processes\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def explain_ddp():\n",
    "        concepts = {\n",
    "            \"Data Parallelism\": \"Each GPU processes a different batch subset\",\n",
    "            \"Gradient Synchronization\": \"Gradients averaged across all GPUs\",\n",
    "            \"Model Replication\": \"Same model on each GPU\",\n",
    "            \"Communication Backend\": \"NCCL for GPU, Gloo for CPU\"\n",
    "        }\n",
    "        \n",
    "        for concept, explanation in concepts.items():\n",
    "            print(f\"{concept}: {explanation}\")\n",
    "\n",
    "DDPConcepts.explain_ddp()\n",
    "```\n",
    "\n",
    "## 2. Dataset Preparation for DDP\n",
    "\n",
    "```python\n",
    "class CIFAR10Dataset(Dataset):\n",
    "    \"\"\"Custom CIFAR10 dataset with DDP-friendly sampling\"\"\"\n",
    "    \n",
    "    def __init__(self, train=True, transform=None):\n",
    "        self.dataset = torchvision.datasets.CIFAR10(\n",
    "            root='./data', train=train, download=True, transform=transform\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "# Data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CIFAR10Dataset(train=True, transform=train_transform)\n",
    "val_dataset = CIFAR10Dataset(train=False, transform=val_transform)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "```\n",
    "\n",
    "## 3. Lightning Module for DDP\n",
    "\n",
    "```python\n",
    "class DDPResNet(pl.LightningModule):\n",
    "    \"\"\"ResNet model optimized for DDP training\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, learning_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Model architecture\n",
    "        self.backbone = torchvision.models.resnet18(pretrained=False)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.train_acc = pl.metrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = pl.metrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.train_acc(logits, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.val_acc(logits, y)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Scale learning rate by number of GPUs\n",
    "        scaled_lr = self.hparams.learning_rate * torch.cuda.device_count()\n",
    "        \n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(), \n",
    "            lr=scaled_lr,\n",
    "            momentum=0.9, \n",
    "            weight_decay=5e-4\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=200\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch'\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize model\n",
    "model = DDPResNet(num_classes=10, learning_rate=0.1)\n",
    "```\n",
    "\n",
    "## 4. Data Module for DDP\n",
    "\n",
    "```python\n",
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    \"\"\"Data module with DDP-optimized data loading\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size=128, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = CIFAR10Dataset(train=True, transform=train_transform)\n",
    "            self.val_dataset = CIFAR10Dataset(train=False, transform=val_transform)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,  # Lightning handles DDP sampling\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "# Initialize data module\n",
    "data_module = CIFAR10DataModule(batch_size=128, num_workers=4)\n",
    "```\n",
    "\n",
    "## 5. DDP Strategy Configuration\n",
    "\n",
    "```python\n",
    "# Configure DDP strategy\n",
    "ddp_strategy = DDPStrategy(\n",
    "    process_group_backend=\"nccl\",  # Use NCCL for GPU communication\n",
    "    find_unused_parameters=False,  # Set to True if you have unused parameters\n",
    "    gradient_as_bucket_view=True,  # Memory optimization\n",
    "    static_graph=True  # Optimization for static computation graphs\n",
    ")\n",
    "\n",
    "# Alternative: Auto-detect strategy\n",
    "# strategy = \"auto\" if torch.cuda.device_count() > 1 else None\n",
    "\n",
    "print(f\"Using DDP strategy: {ddp_strategy}\")\n",
    "```\n",
    "\n",
    "## 6. Callbacks for DDP Training\n",
    "\n",
    "```python\n",
    "# Model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='./checkpoints',\n",
    "    filename='ddp-cifar10-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=3,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    min_delta=0.001,\n",
    "    patience=10,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint_callback, early_stop_callback]\n",
    "```\n",
    "\n",
    "## 7. Trainer Configuration for DDP\n",
    "\n",
    "```python\n",
    "# Configure trainer for DDP\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=torch.cuda.device_count() if torch.cuda.is_available() else 1,\n",
    "    strategy=ddp_strategy if torch.cuda.device_count() > 1 else 'auto',\n",
    "    callbacks=callbacks,\n",
    "    log_every_n_steps=10,\n",
    "    val_check_interval=1.0,\n",
    "    precision=16,  # Mixed precision for faster training\n",
    "    gradient_clip_val=1.0,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"Trainer configuration:\")\n",
    "print(f\"  Accelerator: {trainer.accelerator}\")\n",
    "print(f\"  Devices: {trainer.num_devices}\")\n",
    "print(f\"  Strategy: {trainer.strategy}\")\n",
    "```\n",
    "\n",
    "## 8. Training with DDP\n",
    "\n",
    "```python\n",
    "# Start training\n",
    "print(\"Starting DDP training...\")\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Load best checkpoint for testing\n",
    "best_model = DDPResNet.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "print(f\"Best model loaded from: {checkpoint_callback.best_model_path}\")\n",
    "```\n",
    "\n",
    "## 9. DDP Performance Monitoring\n",
    "\n",
    "```python\n",
    "class DDPProfiler:\n",
    "    \"\"\"Utility class for monitoring DDP performance\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_gpu_memory():\n",
    "        \"\"\"Log GPU memory usage across all devices\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "                cached = torch.cuda.memory_reserved(i) / 1024**3\n",
    "                print(f\"GPU {i}: {allocated:.2f}GB allocated, {cached:.2f}GB cached\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_training_time(dataloader, num_epochs):\n",
    "        \"\"\"Estimate total training time\"\"\"\n",
    "        samples_per_epoch = len(dataloader.dataset)\n",
    "        batch_size = dataloader.batch_size\n",
    "        batches_per_epoch = len(dataloader)\n",
    "        \n",
    "        print(f\"Training estimation:\")\n",
    "        print(f\"  Samples per epoch: {samples_per_epoch}\")\n",
    "        print(f\"  Batch size: {batch_size}\")\n",
    "        print(f\"  Batches per epoch: {batches_per_epoch}\")\n",
    "        print(f\"  Total epochs: {num_epochs}\")\n",
    "\n",
    "# Monitor performance\n",
    "profiler = DDPProfiler()\n",
    "profiler.log_gpu_memory()\n",
    "profiler.estimate_training_time(data_module.train_dataloader(), 50)\n",
    "```\n",
    "\n",
    "## 10. DDP Troubleshooting and Best Practices\n",
    "\n",
    "```python\n",
    "class DDPTroubleshooting:\n",
    "    \"\"\"Common DDP issues and solutions\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_environment():\n",
    "        \"\"\"Check DDP environment setup\"\"\"\n",
    "        env_vars = ['MASTER_ADDR', 'MASTER_PORT', 'WORLD_SIZE', 'RANK']\n",
    "        \n",
    "        print(\"Environment variables:\")\n",
    "        for var in env_vars:\n",
    "            value = os.environ.get(var, 'Not set')\n",
    "            print(f\"  {var}: {value}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def best_practices():\n",
    "        \"\"\"Print DDP best practices\"\"\"\n",
    "        practices = [\n",
    "            \"Use sync_dist=True for logging metrics\",\n",
    "            \"Scale learning rate by number of GPUs\",\n",
    "            \"Use pin_memory=True in DataLoader\",\n",
    "            \"Set find_unused_parameters=False if possible\",\n",
    "            \"Use NCCL backend for GPU communication\",\n",
    "            \"Ensure batch size is divisible by number of GPUs\",\n",
    "            \"Use gradient clipping for stability\"\n",
    "        ]\n",
    "        \n",
    "        print(\"DDP Best Practices:\")\n",
    "        for i, practice in enumerate(practices, 1):\n",
    "            print(f\"{i}. {practice}\")\n",
    "\n",
    "troubleshooter = DDPTroubleshooting()\n",
    "troubleshooter.check_environment()\n",
    "troubleshooter.best_practices()\n",
    "```\n",
    "\n",
    "## 11. Performance Comparison: Single GPU vs DDP\n",
    "\n",
    "```python\n",
    "# Performance comparison utility\n",
    "def compare_training_performance():\n",
    "    \"\"\"Compare single GPU vs multi-GPU training\"\"\"\n",
    "    \n",
    "    single_gpu_time = 100  # Example: 100 minutes\n",
    "    multi_gpu_time = single_gpu_time / torch.cuda.device_count() * 1.1  # 10% overhead\n",
    "    \n",
    "    speedup = single_gpu_time / multi_gpu_time\n",
    "    efficiency = speedup / torch.cuda.device_count() * 100\n",
    "    \n",
    "    print(f\"Performance Comparison:\")\n",
    "    print(f\"  Single GPU time: {single_gpu_time:.1f} minutes\")\n",
    "    print(f\"  Multi-GPU time: {multi_gpu_time:.1f} minutes\")\n",
    "    print(f\"  Speedup: {speedup:.2f}x\")\n",
    "    print(f\"  Efficiency: {efficiency:.1f}%\")\n",
    "    \n",
    "    return speedup, efficiency\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    speedup, efficiency = compare_training_performance()\n",
    "```\n",
    "\n",
    "## 12. Testing DDP Model\n",
    "\n",
    "```python\n",
    "# Test the trained model\n",
    "def test_ddp_model(model, test_dataloader):\n",
    "    \"\"\"Test the DDP-trained model\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            x, y = batch\n",
    "            if torch.cuda.is_available():\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "            \n",
    "            outputs = model(x)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Create test dataloader\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Test the model\n",
    "if torch.cuda.is_available():\n",
    "    best_model = best_model.cuda()\n",
    "\n",
    "test_accuracy = test_ddp_model(best_model, test_dataloader)\n",
    "```\n",
    "\n",
    "# Summary\n",
    "\n",
    "This notebook demonstrated Distributed Data Parallel (DDP) training with PyTorch Lightning on a single node with multiple GPUs. Key takeaways:\n",
    "\n",
    "## Key Concepts Covered\n",
    "- **DDP Architecture**: Understanding process groups, ranks, and synchronization\n",
    "- **Data Loading**: Proper dataset handling for distributed training\n",
    "- **Model Implementation**: Lightning module optimized for DDP\n",
    "- **Strategy Configuration**: Setting up DDP strategy with optimal parameters\n",
    "- **Performance Monitoring**: Tracking GPU usage and training efficiency\n",
    "\n",
    "## Best Practices Implemented\n",
    "- Synchronized metric logging across processes\n",
    "- Learning rate scaling for multiple GPUs\n",
    "- Memory-efficient data loading with pin_memory\n",
    "- Mixed precision training for speed\n",
    "- Proper gradient clipping and checkpointing\n",
    "\n",
    "## Performance Benefits\n",
    "- Linear speedup with multiple GPUs (with minimal overhead)\n",
    "- Efficient memory utilization across devices\n",
    "- Automatic gradient synchronization\n",
    "- Fault tolerance with checkpointing\n",
    "\n",
    "## Next Steps\n",
    "- Experiment with multi-node DDP setups\n",
    "- Try different communication backends (NCCL vs Gloo)\n",
    "- Implement custom strategies for specific use cases\n",
    "- Explore advanced features like gradient compression\n",
    "\n",
    "The DDP approach significantly reduces training time while maintaining model quality, making it essential for large-scale deep learning projects."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
