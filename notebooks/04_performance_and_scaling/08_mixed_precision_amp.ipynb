{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d3b51e",
   "metadata": {},
   "source": [
    "# Mixed Precision Training and AMP\n",
    "\n",
    "**File Location:** `notebooks/04_performance_and_scaling/08_mixed_precision_amp.ipynb`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook covers mixed precision training using Automatic Mixed Precision (AMP) in PyTorch Lightning. Learn to accelerate training, reduce memory usage, and maintain model accuracy while leveraging modern GPU capabilities.\n",
    "\n",
    "## Mixed Precision Fundamentals\n",
    "\n",
    "### Understanding Mixed Precision\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class MixedPrecisionDemo(pl.LightningModule):\n",
    "    \"\"\"Model to demonstrate mixed precision training\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=512, hidden_size=1024, num_classes=10, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Larger model to see memory benefits\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.BatchNorm1d(hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "        # Track precision info\n",
    "        self.precision_info = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        \n",
    "        # Log precision information\n",
    "        if batch_idx % 50 == 0:\n",
    "            self._log_precision_info(x, logits, loss)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _log_precision_info(self, inputs, outputs, loss):\n",
    "        \"\"\"Log information about tensor precisions\"\"\"\n",
    "        info = {\n",
    "            'step': self.global_step,\n",
    "            'input_dtype': str(inputs.dtype),\n",
    "            'output_dtype': str(outputs.dtype),\n",
    "            'loss_dtype': str(loss.dtype),\n",
    "            'input_range': [inputs.min().item(), inputs.max().item()],\n",
    "            'output_range': [outputs.min().item(), outputs.max().item()],\n",
    "            'loss_value': loss.item()\n",
    "        }\n",
    "        self.precision_info.append(info)\n",
    "        \n",
    "        # Log to Lightning\n",
    "        self.log('input_is_fp16', inputs.dtype == torch.float16)\n",
    "        self.log('output_is_fp16', outputs.dtype == torch.float16)\n",
    "        self.log('loss_is_fp16', loss.dtype == torch.float16)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "# Create larger synthetic dataset to see memory benefits\n",
    "def create_large_dataset(num_samples=10000, input_size=512, num_classes=10):\n",
    "    torch.manual_seed(42)\n",
    "    X = torch.randn(num_samples, input_size)\n",
    "    weights = torch.randn(input_size)\n",
    "    logits = X @ weights\n",
    "    y = torch.div(logits - logits.min(), (logits.max() - logits.min()) / (num_classes - 1), rounding_mode='floor').long()\n",
    "    y = torch.clamp(y, 0, num_classes - 1)\n",
    "    return X, y\n",
    "\n",
    "X, y = create_large_dataset()\n",
    "dataset = TensorDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(\"✓ Large model and dataset created\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in MixedPrecisionDemo().parameters()):,}\")\n",
    "```\n",
    "\n",
    "### Training without Mixed Precision (Baseline)\n",
    "\n",
    "```python\n",
    "# Memory monitoring callback\n",
    "class MemoryMonitorCallback(Callback):\n",
    "    \"\"\"Monitor GPU memory usage during training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.memory_stats = []\n",
    "        \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if hasattr(self, 'batch_start_time'):\n",
    "            batch_time = time.time() - self.batch_start_time\n",
    "            self.batch_times.append(batch_time)\n",
    "            if batch_idx % 50 == 0:\n",
    "                pl_module.log('batch_time_ms', batch_time * 1000)\n",
    "    \n",
    "    def get_average_times(self):\n",
    "        avg_epoch = np.mean(self.epoch_times) if self.epoch_times else 0\n",
    "        avg_batch = np.mean(self.batch_times) if self.batch_times else 0\n",
    "        return avg_epoch, avg_batch\n",
    "\n",
    "print(\"=== Baseline Training (FP32) ===\")\n",
    "\n",
    "# Train without mixed precision\n",
    "model_fp32 = MixedPrecisionDemo()\n",
    "memory_callback = MemoryMonitorCallback()\n",
    "speed_callback = SpeedBenchmarkCallback()\n",
    "\n",
    "trainer_fp32 = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    precision=\"32-true\",  # Explicit FP32\n",
    "    callbacks=[memory_callback, speed_callback],\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False\n",
    ")\n",
    "\n",
    "print(\"Training with FP32 precision...\")\n",
    "start_time = time.time()\n",
    "trainer_fp32.fit(model_fp32, train_loader, val_loader)\n",
    "fp32_total_time = time.time() - start_time\n",
    "\n",
    "# Get baseline metrics\n",
    "fp32_peak_alloc, fp32_peak_reserved = memory_callback.get_peak_memory()\n",
    "fp32_avg_epoch, fp32_avg_batch = speed_callback.get_average_times()\n",
    "fp32_final_acc = trainer_fp32.callback_metrics.get('val_acc', 0)\n",
    "\n",
    "print(f\"✓ FP32 Training completed in {fp32_total_time:.2f}s\")\n",
    "print(f\"Peak memory: {fp32_peak_alloc:.2f}GB allocated, {fp32_peak_reserved:.2f}GB reserved\")\n",
    "print(f\"Final accuracy: {fp32_final_acc:.4f}\")\n",
    "```\n",
    "\n",
    "### Training with Mixed Precision (AMP)\n",
    "\n",
    "```python\n",
    "print(\"\\n=== Mixed Precision Training (AMP) ===\")\n",
    "\n",
    "# Train with automatic mixed precision\n",
    "model_amp = MixedPrecisionDemo()\n",
    "memory_callback_amp = MemoryMonitorCallback()\n",
    "speed_callback_amp = SpeedBenchmarkCallback()\n",
    "\n",
    "trainer_amp = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    precision=\"16-mixed\",  # Enable AMP\n",
    "    callbacks=[memory_callback_amp, speed_callback_amp],\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False\n",
    ")\n",
    "\n",
    "print(\"Training with mixed precision (AMP)...\")\n",
    "start_time = time.time()\n",
    "trainer_amp.fit(model_amp, train_loader, val_loader)\n",
    "amp_total_time = time.time() - start_time\n",
    "\n",
    "# Get AMP metrics\n",
    "amp_peak_alloc, amp_peak_reserved = memory_callback_amp.get_peak_memory()\n",
    "amp_avg_epoch, amp_avg_batch = speed_callback_amp.get_average_times()\n",
    "amp_final_acc = trainer_amp.callback_metrics.get('val_acc', 0)\n",
    "\n",
    "print(f\"✓ AMP Training completed in {amp_total_time:.2f}s\")\n",
    "print(f\"Peak memory: {amp_peak_alloc:.2f}GB allocated, {amp_peak_reserved:.2f}GB reserved\")\n",
    "print(f\"Final accuracy: {amp_final_acc:.4f}\")\n",
    "\n",
    "# Compare results\n",
    "print(f\"\\n=== Performance Comparison ===\")\n",
    "print(f\"Training time - FP32: {fp32_total_time:.2f}s, AMP: {amp_total_time:.2f}s\")\n",
    "print(f\"Speedup: {fp32_total_time/amp_total_time:.2f}x\")\n",
    "print(f\"Memory reduction: {(fp32_peak_alloc - amp_peak_alloc)/fp32_peak_alloc*100:.1f}%\")\n",
    "print(f\"Accuracy - FP32: {fp32_final_acc:.4f}, AMP: {amp_final_acc:.4f}\")\n",
    "print(f\"Accuracy difference: {amp_final_acc - fp32_final_acc:.4f}\")\n",
    "```\n",
    "\n",
    "## Advanced Mixed Precision Techniques\n",
    "\n",
    "### Custom AMP Configuration\n",
    "\n",
    "```python\n",
    "class AdvancedAMPModel(pl.LightningModule):\n",
    "    \"\"\"Model with advanced AMP configuration\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Model with different layer types\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LayerNorm(1024),  # LayerNorm works well with AMP\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        self.transformer_block = nn.TransformerEncoderLayer(\n",
    "            d_model=1024,\n",
    "            nhead=8,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        \n",
    "        # Track gradient scaling\n",
    "        self.grad_scale_history = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape for transformer (add sequence dimension)\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.unsqueeze(1)  # [batch, 1, features]\n",
    "        x = self.transformer_block(x)\n",
    "        x = x.squeeze(1)  # [batch, features]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        \n",
    "        # Log gradient scaling information\n",
    "        if batch_idx % 50 == 0:\n",
    "            self._log_gradient_info()\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _log_gradient_info(self):\n",
    "        \"\"\"Log gradient scaling information\"\"\"\n",
    "        if hasattr(self.trainer, 'precision_plugin'):\n",
    "            scaler = getattr(self.trainer.precision_plugin, 'scaler', None)\n",
    "            if scaler is not None:\n",
    "                scale = scaler.get_scale()\n",
    "                self.grad_scale_history.append(scale)\n",
    "                self.log('gradient_scale', scale)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, \n",
    "            max_lr=1e-3, \n",
    "            total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'step'\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Train advanced AMP model\n",
    "print(\"\\n=== Advanced AMP Configuration ===\")\n",
    "\n",
    "model_advanced = AdvancedAMPModel()\n",
    "\n",
    "trainer_advanced = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    precision=\"16-mixed\",\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False\n",
    ")\n",
    "\n",
    "print(\"Training advanced model with AMP...\")\n",
    "trainer_advanced.fit(model_advanced, train_loader, val_loader)\n",
    "\n",
    "print(f\"✓ Advanced AMP training completed\")\n",
    "print(f\"Gradient scale history: {len(model_advanced.grad_scale_history)} updates\")\n",
    "if model_advanced.grad_scale_history:\n",
    "    print(f\"Final gradient scale: {model_advanced.grad_scale_history[-1]:.0f}\")\n",
    "```\n",
    "\n",
    "### Manual Mixed Precision Control\n",
    "\n",
    "```python\n",
    "class ManualAMPModel(pl.LightningModule):\n",
    "    \"\"\"Model with manual AMP control for specific operations\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        # Manual precision control for specific operations\n",
    "        with torch.cuda.amp.autocast(enabled=False):  # Force FP32 for this block\n",
    "            # Compute some operations that need high precision\n",
    "            x_normalized = F.normalize(x, p=2, dim=1)\n",
    "        \n",
    "        # Regular forward pass with AMP\n",
    "        logits = self(x_normalized)\n",
    "        \n",
    "        # Loss computation (usually handled by AMP automatically)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        # Ensure validation is consistent\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = self(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"\\n=== Manual AMP Control ===\")\n",
    "\n",
    "model_manual = ManualAMPModel()\n",
    "\n",
    "trainer_manual = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    precision=\"16-mixed\",\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False\n",
    ")\n",
    "\n",
    "print(\"Training with manual AMP control...\")\n",
    "trainer_manual.fit(model_manual, train_loader, val_loader)\n",
    "print(\"✓ Manual AMP training completed\")\n",
    "```\n",
    "\n",
    "## Debugging Mixed Precision Issues\n",
    "\n",
    "### Loss Scaling and Gradient Issues\n",
    "\n",
    "```python\n",
    "class DebuggingAMPModel(pl.LightningModule):\n",
    "    \"\"\"Model for debugging AMP issues\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "        \n",
    "        # Debug tracking\n",
    "        self.loss_history = []\n",
    "        self.gradient_norms = []\n",
    "        self.nan_count = 0\n",
    "        self.inf_count = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Debug loss values\n",
    "        self.loss_history.append(loss.item())\n",
    "        \n",
    "        # Check for NaN/Inf\n",
    "        if torch.isnan(loss):\n",
    "            self.nan_count += 1\n",
    "            print(f\"⚠️ NaN loss detected at step {self.global_step}\")\n",
    "        \n",
    "        if torch.isinf(loss):\n",
    "            self.inf_count += 1\n",
    "            print(f\"⚠️ Inf loss detected at step {self.global_step}\")\n",
    "        \n",
    "        # Log debugging info\n",
    "        self.log('train_loss', loss, on_step=True)\n",
    "        self.log('nan_count', float(self.nan_count))\n",
    "        self.log('inf_count', float(self.inf_count))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_before_optimizer_step(self, optimizer, optimizer_idx):\n",
    "        # Check gradient norms before optimization step\n",
    "        total_norm = 0\n",
    "        nan_grads = 0\n",
    "        inf_grads = 0\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                \n",
    "                if torch.isnan(p.grad).any():\n",
    "                    nan_grads += 1\n",
    "                if torch.isinf(p.grad).any():\n",
    "                    inf_grads += 1\n",
    "        \n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        self.gradient_norms.append(total_norm)\n",
    "        \n",
    "        # Log gradient statistics\n",
    "        self.log('grad_norm', total_norm)\n",
    "        self.log('nan_grads', float(nan_grads))\n",
    "        self.log('inf_grads', float(inf_grads))\n",
    "        \n",
    "        # Warning for problematic gradients\n",
    "        if total_norm > 100:\n",
    "            print(f\"⚠️ Large gradient norm: {total_norm:.2f}\")\n",
    "        \n",
    "        if nan_grads > 0 or inf_grads > 0:\n",
    "            print(f\"⚠️ Problematic gradients: {nan_grads} NaN, {inf_grads} Inf\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        self.log('val_acc', acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Use different optimizers to test stability\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "print(\"\\n=== Debugging AMP Issues ===\")\n",
    "\n",
    "model_debug = DebuggingAMPModel()\n",
    "\n",
    "# Train with debugging enabled\n",
    "trainer_debug = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    precision=\"16-mixed\",\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False,\n",
    "    gradient_clip_val=1.0,  # Gradient clipping for stability\n",
    "    gradient_clip_algorithm=\"norm\"\n",
    ")\n",
    "\n",
    "print(\"Training with AMP debugging...\")\n",
    "trainer_debug.fit(model_debug, train_loader, val_loader)\n",
    "\n",
    "print(f\"✓ Debug training completed\")\n",
    "print(f\"NaN losses: {model_debug.nan_count}\")\n",
    "print(f\"Inf losses: {model_debug.inf_count}\")\n",
    "print(f\"Max gradient norm: {max(model_debug.gradient_norms) if model_debug.gradient_norms else 0:.4f}\")\n",
    "print(f\"Min gradient norm: {min(model_debug.gradient_norms) if model_debug.gradient_norms else 0:.4f}\")\n",
    "```\n",
    "\n",
    "## Best Practices and Recommendations\n",
    "\n",
    "### Production AMP Setup\n",
    "\n",
    "```python\n",
    "class ProductionAMPModel(pl.LightningModule):\n",
    "    \"\"\"Production-ready model with AMP best practices\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Model architecture optimized for AMP\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LayerNorm(1024),  # Preferred over BatchNorm for AMP\n",
    "            nn.GELU(),           # GELU works well with mixed precision\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "        \n",
    "        from torchmetrics import Accuracy, MetricCollection\n",
    "        metrics = MetricCollection({\n",
    "            'accuracy': Accuracy(task=\"multiclass\", num_classes=10),\n",
    "        })\n",
    "        \n",
    "        self.train_metrics = metrics.clone(prefix='train_')\n",
    "        self.val_metrics = metrics.clone(prefix='val_')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.head(features)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_metrics(preds, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log_dict(self.train_metrics, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_metrics(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log_dict(self.val_metrics, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Use AdamW with weight decay for better regularization\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), \n",
    "            lr=1e-3, \n",
    "            weight_decay=1e-4,\n",
    "            eps=1e-4  # Slightly larger eps for numerical stability\n",
    "        )\n",
    "        \n",
    "        # Warmup + cosine annealing scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=1e-3,\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "            pct_start=0.1  # 10% warmup\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'step'\n",
    "            }\n",
    "        }\n",
    "\n",
    "def create_production_trainer():\n",
    "    \"\"\"Create production-ready trainer with AMP\"\"\"\n",
    "    from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "    \n",
    "    return pl.Trainer(\n",
    "        # AMP configuration\n",
    "        precision=\"16-mixed\",\n",
    "        \n",
    "        # Training configuration  \n",
    "        max_epochs=20,\n",
    "        gradient_clip_val=1.0,\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(\n",
    "                monitor='val_accuracy',\n",
    "                mode='max',\n",
    "                save_top_k=3,\n",
    "                filename='{epoch:02d}-{val_accuracy:.4f}'\n",
    "            ),\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                mode='min'\n",
    "            )\n",
    "        ],\n",
    "        \n",
    "        # Logging\n",
    "        log_every_n_steps=50,\n",
    "        enable_checkpointing=False,  # For demo\n",
    "        logger=False,  # For demo\n",
    "        enable_progress_bar=True\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Production AMP Setup ===\")\n",
    "\n",
    "model_production = ProductionAMPModel()\n",
    "trainer_production = create_production_trainer()\n",
    "\n",
    "print(\"Training production model with AMP...\")\n",
    "trainer_production.fit(model_production, train_loader, val_loader)\n",
    "\n",
    "final_acc = trainer_production.callback_metrics.get('val_accuracy', 0)\n",
    "print(f\"✓ Production training completed with final accuracy: {final_acc:.4f}\")\n",
    "\n",
    "print(\"\\n=== AMP Best Practices Summary ===\")\n",
    "print(\"✅ Use LayerNorm instead of BatchNorm when possible\")\n",
    "print(\"✅ Use GELU activation for better numerical stability\") \n",
    "print(\"✅ Set larger eps values in optimizers (1e-4 instead of 1e-8)\")\n",
    "print(\"✅ Enable gradient clipping to prevent overflow\")\n",
    "print(\"✅ Monitor gradient scaling and adjust if needed\")\n",
    "print(\"✅ Use appropriate learning rate warmup\")\n",
    "print(\"✅ Prefer AdamW over SGD for mixed precision training\")\n",
    "print(\"✅ Test thoroughly and compare with FP32 baseline\")\n",
    "```\n",
    "\n",
    "## Platform-Specific Optimizations\n",
    "\n",
    "### Different Precision Modes\n",
    "\n",
    "```python\n",
    "# Test different precision modes available in PyTorch Lightning\n",
    "precision_modes = [\n",
    "    (\"32-true\", \"Full FP32 precision\"),\n",
    "    (\"16-mixed\", \"Automatic Mixed Precision with FP16\"),\n",
    "    (\"bf16-mixed\", \"Mixed precision with BFloat16\"),\n",
    "    (\"64-true\", \"Double precision FP64\")\n",
    "]\n",
    "\n",
    "class PrecisionTestModel(pl.LightningModule):\n",
    "    \"\"\"Simple model to test different precision modes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', self.val_acc)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Create smaller test dataset\n",
    "def create_test_dataset(num_samples=1000, input_size=128):\n",
    "    torch.manual_seed(42)\n",
    "    X = torch.randn(num_samples, input_size)\n",
    "    y = torch.randint(0, 10, (num_samples,))\n",
    "    return TensorDataset(X, y)\n",
    "\n",
    "test_dataset = create_test_dataset()\n",
    "train_size = int(0.8 * len(test_dataset))\n",
    "val_size = len(test_dataset) - train_size\n",
    "train_test, val_test = torch.utils.data.random_split(test_dataset, [train_size, val_size])\n",
    "\n",
    "train_test_loader = DataLoader(train_test, batch_size=64, shuffle=True)\n",
    "val_test_loader = DataLoader(val_test, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"=== Testing Different Precision Modes ===\")\n",
    "\n",
    "precision_results = {}\n",
    "\n",
    "for precision_mode, description in precision_modes:\n",
    "    print(f\"\\nTesting {precision_mode}: {description}\")\n",
    "    \n",
    "    try:\n",
    "        model = PrecisionTestModel()\n",
    "        \n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=2,\n",
    "            precision=precision_mode,\n",
    "            enable_checkpointing=False,\n",
    "            logger=False,\n",
    "            enable_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        trainer.fit(model, train_test_loader, val_test_loader)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Record results\n",
    "        final_acc = trainer.callback_metrics.get('val_acc', 0)\n",
    "        training_time = end_time - start_time\n",
    "        \n",
    "        precision_results[precision_mode] = {\n",
    "            'accuracy': float(final_acc),\n",
    "            'training_time': training_time,\n",
    "            'status': 'SUCCESS'\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ {precision_mode}: Accuracy={final_acc:.4f}, Time={training_time:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        precision_results[precision_mode] = {\n",
    "            'accuracy': 0.0,\n",
    "            'training_time': 0.0,\n",
    "            'status': f'FAILED: {str(e)[:50]}'\n",
    "        }\n",
    "        print(f\"❌ {precision_mode}: Failed - {e}\")\n",
    "\n",
    "# Summary of precision mode comparison\n",
    "print(f\"\\n=== Precision Mode Comparison ===\")\n",
    "for mode, results in precision_results.items():\n",
    "    if results['status'] == 'SUCCESS':\n",
    "        print(f\"{mode:12s}: Acc={results['accuracy']:.4f}, Time={results['training_time']:.2f}s\")\n",
    "    else:\n",
    "        print(f\"{mode:12s}: {results['status']}\")\n",
    "```\n",
    "\n",
    "### Hardware Detection and Optimization\n",
    "\n",
    "```python\n",
    "def detect_hardware_capabilities():\n",
    "    \"\"\"Detect hardware capabilities for mixed precision\"\"\"\n",
    "    info = {\n",
    "        'cuda_available': torch.cuda.is_available(),\n",
    "        'cuda_version': torch.version.cuda if torch.cuda.is_available() else None,\n",
    "        'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        'tensor_cores': False,\n",
    "        'bf16_support': False,\n",
    "        'amp_supported': False\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        info['gpu_name'] = gpu_name\n",
    "        \n",
    "        # Check for Tensor Core support (V100, T4, RTX series, A100, etc.)\n",
    "        tensor_core_gpus = ['V100', 'T4', 'RTX', 'A100', 'A6000', 'A5000', 'A4000']\n",
    "        info['tensor_cores'] = any(gpu in gpu_name for gpu in tensor_core_gpus)\n",
    "        \n",
    "        # Check compute capability\n",
    "        capability = torch.cuda.get_device_capability(0)\n",
    "        info['compute_capability'] = f\"{capability[0]}.{capability[1]}\"\n",
    "        \n",
    "        # Tensor Cores available from compute capability 7.0+\n",
    "        if capability[0] >= 7:\n",
    "            info['tensor_cores'] = True\n",
    "            info['amp_supported'] = True\n",
    "        \n",
    "        # BF16 support (A100 and newer, compute capability 8.0+)\n",
    "        if capability[0] >= 8:\n",
    "            info['bf16_support'] = True\n",
    "    \n",
    "    return info\n",
    "\n",
    "# Hardware detection\n",
    "print(\"=== Hardware Capability Detection ===\")\n",
    "hw_info = detect_hardware_capabilities()\n",
    "\n",
    "print(f\"CUDA Available: {hw_info['cuda_available']}\")\n",
    "if hw_info['cuda_available']:\n",
    "    print(f\"CUDA Version: {hw_info['cuda_version']}\")\n",
    "    print(f\"GPU Count: {hw_info['gpu_count']}\")\n",
    "    print(f\"GPU Name: {hw_info.get('gpu_name', 'Unknown')}\")\n",
    "    print(f\"Compute Capability: {hw_info.get('compute_capability', 'Unknown')}\")\n",
    "    print(f\"Tensor Cores: {hw_info['tensor_cores']}\")\n",
    "    print(f\"AMP Supported: {hw_info['amp_supported']}\")\n",
    "    print(f\"BF16 Supported: {hw_info['bf16_support']}\")\n",
    "    \n",
    "    # Recommendations based on hardware\n",
    "    print(f\"\\n=== Recommendations ===\")\n",
    "    if hw_info['tensor_cores']:\n",
    "        print(\"✓ Use precision='16-mixed' for optimal performance\")\n",
    "        if hw_info['bf16_support']:\n",
    "            print(\"✓ Consider precision='bf16-mixed' for numerical stability\")\n",
    "    else:\n",
    "        print(\"⚠ Mixed precision may not provide significant speedup\")\n",
    "        print(\"  Consider using precision='32-true' for compatibility\")\n",
    "else:\n",
    "    print(\"⚠ CUDA not available - mixed precision training not supported\")\n",
    "```\n",
    "\n",
    "## Memory Optimization Strategies\n",
    "\n",
    "### Memory-Efficient Training Patterns\n",
    "\n",
    "```python\n",
    "class MemoryEfficientModel(pl.LightningModule):\n",
    "    \"\"\"Model with memory optimization techniques\"\"\"\n",
    "    \n",
    "    def __init__(self, use_checkpointing=True, use_efficient_attention=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Large model to demonstrate memory usage\n",
    "        hidden_size = 2048\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size if i > 0 else 512, hidden_size),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.1),\n",
    "            ) for i in range(6)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_size, 10)\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Gradient checkpointing for memory efficiency\n",
    "        if self.hparams.use_checkpointing and self.training:\n",
    "            from torch.utils.checkpoint import checkpoint\n",
    "            \n",
    "            for layer in self.layers:\n",
    "                x = checkpoint(layer, x, use_reentrant=False)\n",
    "        else:\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "        \n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        # Memory monitoring\n",
    "        if torch.cuda.is_available() and batch_idx % 100 == 0:\n",
    "            memory_before = torch.cuda.memory_allocated() / 1024**3\n",
    "            \n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        if torch.cuda.is_available() and batch_idx % 100 == 0:\n",
    "            memory_after = torch.cuda.memory_allocated() / 1024**3\n",
    "            self.log('memory_usage_gb', memory_after)\n",
    "            self.log('memory_increase_mb', (memory_after - memory_before) * 1024)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Use more memory-efficient optimizer settings\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=1e-4,  # Lower LR for stability\n",
    "            weight_decay=1e-4,\n",
    "            eps=1e-4,  # Larger epsilon for mixed precision\n",
    "            betas=(0.9, 0.95)  # Slightly different betas\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "print(\"\\n=== Memory Optimization Strategies ===\")\n",
    "\n",
    "# Test with and without gradient checkpointing\n",
    "memory_configs = [\n",
    "    (\"Standard\", False, \"32-true\"),\n",
    "    (\"Standard + AMP\", False, \"16-mixed\"),\n",
    "    (\"Checkpointing + AMP\", True, \"16-mixed\")\n",
    "]\n",
    "\n",
    "memory_results = {}\n",
    "\n",
    "for config_name, use_checkpointing, precision in memory_configs:\n",
    "    print(f\"\\nTesting: {config_name}\")\n",
    "    \n",
    "    try:\n",
    "        model = MemoryEfficientModel(use_checkpointing=use_checkpointing)\n",
    "        memory_callback = MemoryMonitorCallback()\n",
    "        \n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=2,\n",
    "            precision=precision,\n",
    "            callbacks=[memory_callback],\n",
    "            enable_checkpointing=False,\n",
    "            logger=False,\n",
    "            enable_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        trainer.fit(model, train_test_loader, val_test_loader)\n",
    "        \n",
    "        peak_alloc, peak_reserved = memory_callback.get_peak_memory()\n",
    "        memory_results[config_name] = {\n",
    "            'peak_allocated': peak_alloc,\n",
    "            'peak_reserved': peak_reserved,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ Peak memory: {peak_alloc:.2f}GB allocated, {peak_reserved:.2f}GB reserved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        memory_results[config_name] = {\n",
    "            'peak_allocated': 0,\n",
    "            'peak_reserved': 0,\n",
    "            'success': False,\n",
    "            'error': str(e)[:100]\n",
    "        }\n",
    "        print(f\"❌ Failed: {e}\")\n",
    "\n",
    "# Memory optimization summary\n",
    "print(f\"\\n=== Memory Usage Comparison ===\")\n",
    "baseline_memory = None\n",
    "for config, results in memory_results.items():\n",
    "    if results['success']:\n",
    "        allocated = results['peak_allocated']\n",
    "        if baseline_memory is None:\n",
    "            baseline_memory = allocated\n",
    "            reduction = 0\n",
    "        else:\n",
    "            reduction = (baseline_memory - allocated) / baseline_memory * 100\n",
    "        \n",
    "        print(f\"{config:20s}: {allocated:.2f}GB ({reduction:+.1f}% vs baseline)\")\n",
    "```\n",
    "\n",
    "## Gradient Accumulation with Mixed Precision\n",
    "\n",
    "### Accumulation Strategies\n",
    "\n",
    "```python\n",
    "class GradientAccumulationModel(pl.LightningModule):\n",
    "    \"\"\"Model demonstrating gradient accumulation with mixed precision\"\"\"\n",
    "    \n",
    "    def __init__(self, effective_batch_size=512, actual_batch_size=64):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Calculate accumulation steps\n",
    "        self.accumulation_steps = effective_batch_size // actual_batch_size\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        \n",
    "        # Track gradient statistics\n",
    "        self.grad_norms = []\n",
    "        self.accumulated_losses = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Scale loss by accumulation steps for proper gradient averaging\n",
    "        scaled_loss = loss / self.accumulation_steps\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        \n",
    "        # Track accumulated loss\n",
    "        self.accumulated_losses.append(loss.item())\n",
    "        \n",
    "        # Log every accumulation cycle\n",
    "        if (batch_idx + 1) % self.accumulation_steps == 0:\n",
    "            avg_loss = sum(self.accumulated_losses[-self.accumulation_steps:]) / self.accumulation_steps\n",
    "            self.log('train_loss', avg_loss, on_step=True, prog_bar=True)\n",
    "            self.accumulated_losses.clear()\n",
    "        \n",
    "        self.log('train_acc', self.train_acc, on_epoch=True)\n",
    "        \n",
    "        return scaled_loss\n",
    "    \n",
    "    def on_before_optimizer_step(self, optimizer, optimizer_idx):\n",
    "        # Monitor gradient norms\n",
    "        total_norm = 0\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "        self.grad_norms.append(total_norm)\n",
    "        self.log('grad_norm', total_norm, on_step=True)\n",
    "        \n",
    "        # Check for gradient scaling issues\n",
    "        if hasattr(self.trainer, 'precision_plugin'):\n",
    "            scaler = getattr(self.trainer.precision_plugin, 'scaler', None)\n",
    "            if scaler is not None:\n",
    "                self.log('grad_scale', scaler.get_scale(), on_step=True)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Adjust learning rate for effective batch size\n",
    "        base_lr = 1e-3\n",
    "        scaled_lr = base_lr * (self.hparams.effective_batch_size / 64)  # Scale with batch size\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=scaled_lr,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "print(\"\\n=== Gradient Accumulation with Mixed Precision ===\")\n",
    "\n",
    "# Test different accumulation strategies\n",
    "accumulation_configs = [\n",
    "    (128, 64, 2),   # Small accumulation\n",
    "    (256, 64, 4),   # Medium accumulation  \n",
    "    (512, 64, 8),   # Large accumulation\n",
    "]\n",
    "\n",
    "for effective_batch, actual_batch, expected_steps in accumulation_configs:\n",
    "    print(f\"\\nTesting accumulation: {effective_batch} effective, {actual_batch} actual ({expected_steps} steps)\")\n",
    "    \n",
    "    model = GradientAccumulationModel(\n",
    "        effective_batch_size=effective_batch,\n",
    "        actual_batch_size=actual_batch\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=2,\n",
    "        precision=\"16-mixed\",\n",
    "        accumulate_grad_batches=expected_steps,\n",
    "        enable_checkpointing=False,\n",
    "        logger=False,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, train_test_loader, val_test_loader)\n",
    "    \n",
    "    avg_grad_norm = np.mean(model.grad_norms) if model.grad_norms else 0\n",
    "    print(f\"✓ Average gradient norm: {avg_grad_norm:.4f}\")\n",
    "    print(f\"✓ Accumulation steps configured: {model.accumulation_steps}\")\n",
    "```\n",
    "\n",
    "## Troubleshooting Common Issues\n",
    "\n",
    "### NaN/Inf Detection and Recovery\n",
    "\n",
    "```python\n",
    "class RobustAMPModel(pl.LightningModule):\n",
    "    \"\"\"Model with comprehensive NaN/Inf detection and recovery\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "        \n",
    "        # Tracking for debugging\n",
    "        self.loss_history = []\n",
    "        self.gradient_issues = {\n",
    "            'nan_losses': 0,\n",
    "            'inf_losses': 0,\n",
    "            'nan_gradients': 0,\n",
    "            'inf_gradients': 0,\n",
    "            'large_gradients': 0,\n",
    "            'scale_adjustments': 0\n",
    "        }\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Comprehensive loss validation\n",
    "        self._validate_loss(loss, batch_idx)\n",
    "        \n",
    "        self.loss_history.append(loss.item())\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _validate_loss(self, loss, batch_idx):\n",
    "        \"\"\"Validate loss for NaN/Inf issues\"\"\"\n",
    "        if torch.isnan(loss):\n",
    "            self.gradient_issues['nan_losses'] += 1\n",
    "            print(f\"🚨 NaN loss at step {self.global_step}, batch {batch_idx}\")\n",
    "            self.log('nan_losses', float(self.gradient_issues['nan_losses']))\n",
    "            \n",
    "        if torch.isinf(loss):\n",
    "            self.gradient_issues['inf_losses'] += 1\n",
    "            print(f\"🚨 Inf loss at step {self.global_step}, batch {batch_idx}\")\n",
    "            self.log('inf_losses', float(self.gradient_issues['inf_losses']))\n",
    "            \n",
    "        if loss.item() > 100:  # Unusually large loss\n",
    "            print(f\"⚠️ Large loss detected: {loss.item():.2f} at step {self.global_step}\")\n",
    "            \n",
    "    def on_before_optimizer_step(self, optimizer, optimizer_idx):\n",
    "        # Comprehensive gradient validation\n",
    "        self._validate_gradients()\n",
    "        \n",
    "    def _validate_gradients(self):\n",
    "        \"\"\"Validate gradients for numerical issues\"\"\"\n",
    "        total_norm = 0\n",
    "        nan_params = 0\n",
    "        inf_params = 0\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                # Check for NaN/Inf in gradients\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    nan_params += 1\n",
    "                    if nan_params <= 3:  # Limit spam\n",
    "                        print(f\"🚨 NaN gradient in {name}\")\n",
    "                        \n",
    "                if torch.isinf(param.grad).any():\n",
    "                    inf_params += 1\n",
    "                    if inf_params <= 3:  # Limit spam\n",
    "                        print(f\"🚨 Inf gradient in {name}\")\n",
    "                \n",
    "                # Calculate gradient norm\n",
    "                param_norm = param.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        \n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "        # Update tracking\n",
    "        if nan_params > 0:\n",
    "            self.gradient_issues['nan_gradients'] += 1\n",
    "        if inf_params > 0:\n",
    "            self.gradient_issues['inf_gradients'] += 1\n",
    "        if total_norm > 100:\n",
    "            self.gradient_issues['large_gradients'] += 1\n",
    "            print(f\"⚠️ Large gradient norm: {total_norm:.2f}\")\n",
    "        \n",
    "        # Log gradient statistics\n",
    "        self.log('grad_norm', total_norm)\n",
    "        self.log('nan_gradients', float(self.gradient_issues['nan_gradients']))\n",
    "        self.log('inf_gradients', float(self.gradient_issues['inf_gradients']))\n",
    "        \n",
    "        # Check gradient scaler\n",
    "        if hasattr(self.trainer, 'precision_plugin'):\n",
    "            scaler = getattr(self.trainer.precision_plugin, 'scaler', None)\n",
    "            if scaler is not None:\n",
    "                current_scale = scaler.get_scale()\n",
    "                self.log('gradient_scale', current_scale)\n",
    "                \n",
    "                # Detect scale adjustments (indicates gradient overflow)\n",
    "                if hasattr(self, '_last_scale'):\n",
    "                    if current_scale < self._last_scale:\n",
    "                        self.gradient_issues['scale_adjustments'] += 1\n",
    "                        print(f\"📉 Gradient scale reduced: {self._last_scale} -> {current_scale}\")\n",
    "                        \n",
    "                self._last_scale = current_scale\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Conservative optimizer settings for stability\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=1e-4,  # Lower learning rate\n",
    "            weight_decay=1e-4,\n",
    "            eps=1e-4,  # Larger epsilon for numerical stability\n",
    "            betas=(0.9, 0.95)  # More conservative momentum\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "    def get_debugging_summary(self):\n",
    "        \"\"\"Get summary of numerical issues encountered\"\"\"\n",
    "        return {\n",
    "            'total_steps': self.global_step,\n",
    "            'loss_history_length': len(self.loss_history),\n",
    "            'numerical_issues': self.gradient_issues.copy(),\n",
    "            'average_loss': np.mean(self.loss_history[-100:]) if self.loss_history else 0,\n",
    "            'loss_std': np.std(self.loss_history[-100:]) if len(self.loss_history) > 1 else 0\n",
    "        }\n",
    "\n",
    "print(\"\\n=== Robust AMP Training with Issue Detection ===\")\n",
    "\n",
    "# Train robust model with comprehensive debugging\n",
    "model_robust = RobustAMPModel()\n",
    "\n",
    "trainer_robust = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=1.0,  # Gradient clipping for stability\n",
    "    gradient_clip_algorithm=\"norm\",\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False\n",
    ")\n",
    "\n",
    "print(\"Training with comprehensive debugging...\")\n",
    "trainer_robust.fit(model_robust, train_test_loader, val_test_loader)\n",
    "\n",
    "# Get debugging summary\n",
    "debug_summary = model_robust.get_debugging_summary()\n",
    "\n",
    "print(f\"\\n=== Training Debug Summary ===\")\n",
    "print(f\"Total training steps: {debug_summary['total_steps']}\")\n",
    "print(f\"Average loss (last 100): {debug_summary['average_loss']:.4f}\")\n",
    "print(f\"Loss std deviation: {debug_summary['loss_std']:.4f}\")\n",
    "print(f\"\\nNumerical Issues:\")\n",
    "for issue, count in debug_summary['numerical_issues'].items():\n",
    "    if count > 0:\n",
    "        print(f\"  {issue}: {count}\")\n",
    "    \n",
    "if sum(debug_summary['numerical_issues'].values()) == 0:\n",
    "    print(\"✅ No numerical issues detected!\")\n",
    "else:\n",
    "    print(\"⚠️ Some numerical issues detected - see logs above\")\n",
    "```\n",
    "\n",
    "## Performance Profiling and Optimization\n",
    "\n",
    "### Detailed Performance Analysis\n",
    "\n",
    "```python\n",
    "class ProfilingAMPModel(pl.LightningModule):\n",
    "    \"\"\"Model with detailed performance profiling\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(512, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LayerNorm(1024), \n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "        \n",
    "        # Profiling data\n",
    "        self.profiling_data = {\n",
    "            'forward_times': [],\n",
    "            'backward_times': [],\n",
    "            'optimizer_times': [],\n",
    "            'memory_usage': []\n",
    "        }\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        # Profile forward pass\n",
    "        if batch_idx % 20 == 0:\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            forward_start = time.time()\n",
    "        \n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        if batch_idx % 20 == 0:\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            forward_time = time.time() - forward_start\n",
    "            self.profiling_data['forward_times'].append(forward_time)\n",
    "            \n",
    "            # Memory usage\n",
    "            if torch.cuda.is_available():\n",
    "                memory_mb = torch.cuda.memory_allocated() / 1024**2\n",
    "                self.profiling_data['memory_usage'].append(memory_mb)\n",
    "                self.log('memory_mb', memory_mb)\n",
    "            \n",
    "            self.log('forward_time_ms', forward_time * 1000)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_before_optimizer_step(self, optimizer, optimizer_idx):\n",
    "        # Profile optimizer step\n",
    "        if self.global_step % 20 == 0:\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            self._optimizer_start_time = time.time()\n",
    "    \n",
    "    def on_after_optimizer_step(self, optimizer, optimizer_idx):\n",
    "        # Record optimizer timing\n",
    "        if self.global_step % 20 == 0 and hasattr(self, '_optimizer_start_time'):\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            optimizer_time = time.time() - self._optimizer_start_time\n",
    "            self.profiling_data['optimizer_times'].append(optimizer_time)\n",
    "            self.log('optimizer_time_ms', optimizer_time * 1000)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        self.log('val_acc', acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def get_profiling_summary(self):\n",
    "        \"\"\"Get comprehensive profiling summary\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        for key, times in self.profiling_data.items():\n",
    "            if times:\n",
    "                summary[key] = {\n",
    "                    'mean_ms': np.mean(times) * 1000,\n",
    "                    'std_ms': np.std(times) * 1000,\n",
    "                    'min_ms': np.min(times) * 1000,\n",
    "                    'max_ms': np.max(times) * 1000,\n",
    "                    'count': len(times)\n",
    "                }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "print(\"\\n=== Performance Profiling ===\")\n",
    "\n",
    "# Profile different precision modes\n",
    "profiling_configs = [\n",
    "    (\"FP32\", \"32-true\"),\n",
    "    (\"AMP\", \"16-mixed\")\n",
    "]\n",
    "\n",
    "profiling_results = {}\n",
    "\n",
    "for config_name, precision in profiling_configs:\n",
    "    print(f\"\\nProfiling {config_name} training...\")\n",
    "    \n",
    "    model_profile = ProfilingAMPModel()\n",
    "    \n",
    "    trainer_profile = pl.Trainer(\n",
    "        max_epochs=2,\n",
    "        precision=precision,\n",
    "        enable_checkpointing=False,\n",
    "        logger=False,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    # Train with profiling\n",
    "    start_time = time.time()\n",
    "    trainer_profile.fit(model_profile, train_test_loader, val_test_loader)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Get profiling results\n",
    "    profile_summary = model_profile.get_profiling_summary()\n",
    "    profile_summary['total_time_sec'] = total_time\n",
    "    profiling_results[config_name] = profile_summary\n",
    "    \n",
    "    print(f\"✓ {config_name} profiling completed in {total_time:.2f}s\")\n",
    "\n",
    "# Compare profiling results\n",
    "print(f\"\\n=== Performance Comparison ===\")\n",
    "for config_name, results in profiling_results.items():\n",
    "    print(f\"\\n{config_name} Results:\")\n",
    "    print(f\"  Total time: {results['total_time_sec']:.2f}s\")\n",
    "    \n",
    "    if 'forward_times' in results:\n",
    "        forward = results['forward_times']\n",
    "        print(f\"  Forward pass: {forward['mean_ms']:.2f}±{forward['std_ms']:.2f}ms\")\n",
    "        \n",
    "    if 'optimizer_times' in results:\n",
    "        optimizer = results['optimizer_times']\n",
    "        print(f\"  Optimizer step: {optimizer['mean_ms']:.2f}±{optimizer['std_ms']:.2f}ms\")\n",
    "        \n",
    "    if 'memory_usage' in results:\n",
    "        memory = results['memory_usage']\n",
    "        print(f\"  Memory usage: {memory['mean_ms']:.0f}MB (avg)\")\n",
    "\n",
    "# Calculate speedup\n",
    "if 'FP32' in profiling_results and 'AMP' in profiling_results:\n",
    "    fp32_time = profiling_results['FP32']['total_time_sec']\n",
    "    amp_time = profiling_results['AMP']['total_time_sec']\n",
    "    speedup = fp32_time / amp_time\n",
    "    print(f\"\\n🚀 AMP Speedup: {speedup:.2f}x\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways and Best Practices\n",
    "\n",
    "This comprehensive notebook covered mixed precision training and AMP optimization in PyTorch Lightning:\n",
    "\n",
    "**Mixed Precision Fundamentals:**\n",
    "- **Memory Efficiency**: ~50% reduction in GPU memory usage with FP16/BF16\n",
    "- **Speed Improvements**: 1.5-2x training speedup on Tensor Core GPUs\n",
    "- **Maintained Accuracy**: Proper loss scaling preserves model performance\n",
    "- **Hardware Requirements**: Tensor Cores (V100+, RTX 20/30/40 series, A100+) for optimal benefits\n",
    "\n",
    "**Implementation Strategies:**\n",
    "- **Automatic Mixed Precision**: Lightning's `precision=\"16-mixed\"` handles complexity automatically\n",
    "- **BFloat16 Support**: `precision=\"bf16-mixed\"` on newer hardware for better numerical stability\n",
    "- **Manual Control**: Fine-grained precision control for specific operations when needed\n",
    "- **Gradient Accumulation**: Proper scaling with mixed precision for large effective batch sizes\n",
    "\n",
    "**Performance Optimization:**\n",
    "- **Memory Strategies**: Gradient checkpointing, efficient attention, model parallelism\n",
    "- **Hardware Detection**: Automatic capability detection and optimization recommendations\n",
    "- **Profiling Tools**: Comprehensive performance analysis and bottleneck identification\n",
    "- **Scaling Strategies**: Dynamic loss scaling and gradient clipping for numerical stability\n",
    "\n",
    "**Common Issues and Solutions:**\n",
    "- **NaN/Inf Values**: Comprehensive detection, logging, and recovery mechanisms\n",
    "- **Gradient Underflow**: Automatic loss scaling handles small gradient magnitudes\n",
    "- **Convergence Issues**: Conservative optimizer settings and learning rate schedules\n",
    "- **Memory Overflow**: Gradient accumulation and checkpointing for large models\n",
    "\n",
    "**Production Best Practices:**\n",
    "- **Architecture Choices**: LayerNorm over BatchNorm, GELU activations, larger epsilon values\n",
    "- **Training Stability**: Gradient clipping, warmup schedules, conservative learning rates\n",
    "- **Monitoring**: Comprehensive logging of loss values, gradient norms, and memory usage\n",
    "- **Validation**: Always compare with FP32 baseline for accuracy verification\n",
    "\n",
    "**Platform Considerations:**\n",
    "- **GPU Architecture**: Tensor Core utilization for maximum benefit\n",
    "- **CUDA Versions**: Compatibility with mixed precision features\n",
    "- **Memory Capacity**: Balance between model size and batch size\n",
    "- **Multi-GPU**: Proper scaling across distributed training setups\n",
    "\n",
    "**Development Workflow:**\n",
    "1. **Baseline Establishment**: Train with FP32 to establish accuracy targets\n",
    "2. **Hardware Verification**: Check Tensor Core availability and capabilities  \n",
    "3. **Gradual Implementation**: Start with `precision=\"16-mixed\"` and monitor closely\n",
    "4. **Performance Profiling**: Measure speedup and memory improvements\n",
    "5. **Stability Testing**: Extended training runs to verify convergence\n",
    "6. **Production Deployment**: Comprehensive monitoring and fallback mechanisms\n",
    "\n",
    "**Future Considerations:**\n",
    "- **8-bit Training**: Emerging quantization techniques for further memory reduction\n",
    "- **Model Compilation**: Integration with `torch.compile` for additional speedups\n",
    "- **Custom Precision**: Task-specific precision policies for optimal performance\n",
    "- **Hardware Evolution**: Adaptation to new GPU architectures and features\n",
    "\n",
    "Best practices:\n",
    "- Always compare with FP32 baseline for accuracy verification\n",
    "- Use LayerNorm over BatchNorm for better AMP compatibility\n",
    "- Enable gradient clipping for numerical stability\n",
    "- Monitor gradient scaling and loss values during training\n",
    "- Use appropriate learning rate schedules with warmup\n",
    "- Test extensively before production deployment\n",
    "\n",
    "Hardware requirements:\n",
    "- Tensor Cores (V100, T4, RTX 20/30/40 series, A100)\n",
    "- CUDA Compute Capability 7.0+\n",
    "- Sufficient GPU memory for model and gradients\n",
    "\n",
    "Mixed precision training is essential for modern deep learning workflows, providing significant performance benefits while maintaining model quality. The key is careful implementation with comprehensive monitoring and validation to ensure both speed and accuracy improvements."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
