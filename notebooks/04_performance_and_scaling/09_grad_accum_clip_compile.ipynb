{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe2f775",
   "metadata": {},
   "source": [
    "# Gradient Accumulation, Clipping, and Model Compilation\n",
    "\n",
    "**File Location:** `notebooks/04_performance_and_scaling/09_grad_accum_clip_compile.ipynb`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook covers advanced gradient techniques and model compilation for optimized training. Learn gradient accumulation for large effective batch sizes, gradient clipping for stable training, and torch.compile for significant speedups.\n",
    "\n",
    "## Gradient Accumulation\n",
    "\n",
    "### Understanding Gradient Accumulation\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class GradientAccumulationDemo(pl.LightningModule):\n",
    "    \"\"\"Demo model for gradient accumulation\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=256, hidden_size=512, num_classes=10, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "        # Track gradient accumulation stats\n",
    "        self.accum_steps_taken = []\n",
    "        self.effective_batch_sizes = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        \n",
    "        # Track effective batch size\n",
    "        accumulate_grad_batches = self.trainer.accumulate_grad_batches\n",
    "        current_batch_size = x.size(0)\n",
    "        effective_batch_size = current_batch_size * accumulate_grad_batches\n",
    "        \n",
    "        self.effective_batch_sizes.append(effective_batch_size)\n",
    "        \n",
    "        # Log information\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True)\n",
    "        self.log('effective_batch_size', effective_batch_size, on_step=False, on_epoch=True)\n",
    "        self.log('accumulate_steps', accumulate_grad_batches, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_before_optimizer_step(self, optimizer, optimizer_idx):\n",
    "        # Track when optimizer steps are actually taken\n",
    "        self.accum_steps_taken.append(self.global_step)\n",
    "        \n",
    "        # Log gradient norms\n",
    "        total_norm = 0\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "        self.log('grad_norm', total_norm, on_step=True, on_epoch=False)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Create dataset\n",
    "def create_dataset(num_samples=5000, input_size=256, num_classes=10):\n",
    "    torch.manual_seed(42)\n",
    "    X = torch.randn(num_samples, input_size)\n",
    "    weights = torch.randn(input_size)\n",
    "    logits = X @ weights\n",
    "    y = torch.div(logits - logits.min(), (logits.max() - logits.min()) / (num_classes - 1), rounding_mode='floor').long()\n",
    "    y = torch.clamp(y, 0, num_classes - 1)\n",
    "    return X, y\n",
    "\n",
    "X, y = create_dataset()\n",
    "dataset = TensorDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Different batch sizes to compare with gradient accumulation\n",
    "small_batch_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Small batches\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"‚úì Dataset and model setup completed\")\n",
    "```\n",
    "\n",
    "### Comparing Different Accumulation Strategies\n",
    "\n",
    "```python\n",
    "def compare_gradient_accumulation():\n",
    "    \"\"\"Compare different gradient accumulation settings\"\"\"\n",
    "    \n",
    "    configs = [\n",
    "        {\"name\": \"No Accumulation\", \"batch_size\": 64, \"accumulate\": 1},\n",
    "        {\"name\": \"2x Accumulation\", \"batch_size\": 32, \"accumulate\": 2}, \n",
    "        {\"name\": \"4x Accumulation\", \"batch_size\": 16, \"accumulate\": 4},\n",
    "        {\"name\": \"8x Accumulation\", \"batch_size\": 8, \"accumulate\": 8},\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\n=== {config['name']} ===\")\n",
    "        print(f\"Batch size: {config['batch_size']}, Accumulate: {config['accumulate']}\")\n",
    "        print(f\"Effective batch size: {config['batch_size'] * config['accumulate']}\")\n",
    "        \n",
    "        # Create dataloader with specific batch size\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "        \n",
    "        model = GradientAccumulationDemo()\n",
    "        \n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=3,\n",
    "            accumulate_grad_batches=config['accumulate'],\n",
    "            enable_checkpointing=False,\n",
    "            logger=False,\n",
    "            enable_progress_bar=False,\n",
    "            limit_train_batches=50,  # Limit for demo\n",
    "            limit_val_batches=20\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        final_acc = trainer.callback_metrics.get('val_acc', 0)\n",
    "        final_loss = trainer.callback_metrics.get('val_loss', float('inf'))\n",
    "        \n",
    "        results[config['name']] = {\n",
    "            'time': training_time,\n",
    "            'accuracy': final_acc,\n",
    "            'loss': final_loss,\n",
    "            'effective_batch_size': config['batch_size'] * config['accumulate'],\n",
    "            'optimizer_steps': len(model.accum_steps_taken)\n",
    "        }\n",
    "        \n",
    "        print(f\"Training time: {training_time:.2f}s\")\n",
    "        print(f\"Final accuracy: {final_acc:.4f}\")\n",
    "        print(f\"Optimizer steps taken: {len(model.accum_steps_taken)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison\n",
    "print(\"=== Gradient Accumulation Comparison ===\")\n",
    "accumulation_results = compare_gradient_accumulation()\n",
    "\n",
    "print(f\"\\n=== Results Summary ===\")\n",
    "for name, result in accumulation_results.items():\n",
    "    print(f\"{name:15} | Time: {result['time']:5.2f}s | Acc: {result['accuracy']:.4f} | Steps: {result['optimizer_steps']:3d} | Eff.BS: {result['effective_batch_size']:2d}\")\n",
    "```\n",
    "\n",
    "## Gradient Clipping Techniques\n",
    "\n",
    "### Understanding Gradient Clipping\n",
    "\n",
    "```python\n",
    "class GradientClippingDemo(pl.LightningModule):\n",
    "    \"\"\"Demo model for gradient clipping techniques\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Deeper model more prone to gradient issues\n",
    "        layers = []\n",
    "        input_size = 256\n",
    "        for i in range(6):  # 6 hidden layers\n",
    "            layers.extend([\n",
    "                nn.Linear(input_size if i == 0 else 512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            ])\n",
    "        layers.append(nn.Linear(512, 10))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        \n",
    "        # Track gradient statistics\n",
    "        self.grad_norms = []\n",
    "        self.clipped_grad_norms = []\n",
    "        self.clipping_ratios = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_before_optimizer_step(self, optimizer, optimizer_idx):\n",
    "        # Calculate gradient norm before clipping\n",
    "        total_norm = 0\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "        self.grad_norms.append(total_norm)\n",
    "        self.log('grad_norm_before_clip', total_norm, on_step=True)\n",
    "        \n",
    "        # If gradient clipping is enabled, calculate clipping ratio\n",
    "        if hasattr(self.trainer, 'gradient_clip_val') and self.trainer.gradient_clip_val is not None:\n",
    "            clip_val = self.trainer.gradient_clip_val\n",
    "            clipping_ratio = min(1.0, clip_val / (total_norm + 1e-6))\n",
    "            self.clipping_ratios.append(clipping_ratio)\n",
    "            \n",
    "            self.log('clipping_ratio', clipping_ratio, on_step=True)\n",
    "            self.log('gradient_clipped', float(clipping_ratio < 1.0), on_step=True)\n",
    "    \n",
    "    def on_after_optimizer_step(self, optimizer, optimizer_idx):\n",
    "        # Calculate gradient norm after clipping\n",
    "        total_norm = 0\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "        self.clipped_grad_norms.append(total_norm)\n",
    "        self.log('grad_norm_after_clip', total_norm, on_step=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "def compare_gradient_clipping():\n",
    "    \"\"\"Compare different gradient clipping strategies\"\"\"\n",
    "    \n",
    "    clipping_configs = [\n",
    "        {\"name\": \"No Clipping\", \"clip_val\": None, \"clip_algo\": None},\n",
    "        {\"name\": \"Norm Clip 0.5\", \"clip_val\": 0.5, \"clip_algo\": \"norm\"},\n",
    "        {\"name\": \"Norm Clip 1.0\", \"clip_val\": 1.0, \"clip_algo\": \"norm\"},\n",
    "        {\"name\": \"Norm Clip 2.0\", \"clip_val\": 2.0, \"clip_algo\": \"norm\"},\n",
    "        {\"name\": \"Value Clip 0.1\", \"clip_val\": 0.1, \"clip_algo\": \"value\"},\n",
    "    ]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    clipping_results = {}\n",
    "    \n",
    "    for config in clipping_configs:\n",
    "        print(f\"\\n=== {config['name']} ===\")\n",
    "        \n",
    "        model = GradientClippingDemo()\n",
    "        \n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=3,\n",
    "            gradient_clip_val=config['clip_val'],\n",
    "            gradient_clip_algorithm=config['clip_algo'],\n",
    "            enable_checkpointing=False,\n",
    "            logger=False,\n",
    "            enable_progress_bar=False,\n",
    "            limit_train_batches=50,\n",
    "            limit_val_batches=20\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        final_acc = trainer.callback_metrics.get('val_acc', 0)\n",
    "        final_loss = trainer.callback_metrics.get('val_loss', float('inf'))\n",
    "        \n",
    "        # Gradient statistics\n",
    "        avg_grad_norm = np.mean(model.grad_norms) if model.grad_norms else 0\n",
    "        max_grad_norm = np.max(model.grad_norms) if model.grad_norms else 0\n",
    "        clipping_frequency = np.mean([r < 1.0 for r in model.clipping_ratios]) if model.clipping_ratios else 0\n",
    "        \n",
    "        clipping_results[config['name']] = {\n",
    "            'time': training_time,\n",
    "            'accuracy': final_acc,\n",
    "            'loss': final_loss,\n",
    "            'avg_grad_norm': avg_grad_norm,\n",
    "            'max_grad_norm': max_grad_norm,\n",
    "            'clipping_freq': clipping_frequency\n",
    "        }\n",
    "        \n",
    "        print(f\"Final accuracy: {final_acc:.4f}\")\n",
    "        print(f\"Average gradient norm: {avg_grad_norm:.4f}\")\n",
    "        print(f\"Max gradient norm: {max_grad_norm:.4f}\")\n",
    "        print(f\"Clipping frequency: {clipping_frequency:.2%}\")\n",
    "    \n",
    "    return clipping_results\n",
    "\n",
    "print(\"\\n=== Gradient Clipping Comparison ===\")\n",
    "clipping_results = compare_gradient_clipping()\n",
    "\n",
    "print(f\"\\n=== Clipping Results Summary ===\")\n",
    "for name, result in clipping_results.items():\n",
    "    print(f\"{name:15} | Acc: {result['accuracy']:.4f} | AvgGrad: {result['avg_grad_norm']:6.3f} | MaxGrad: {result['max_grad_norm']:6.3f} | ClipFreq: {result['clipping_freq']:5.1%}\")\n",
    "```\n",
    "\n",
    "## Model Compilation with torch.compile\n",
    "\n",
    "### Basic Model Compilation\n",
    "\n",
    "```python\n",
    "class CompilationDemo(pl.LightningModule):\n",
    "    \"\"\"Demo model for torch.compile optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "\n",
    "def compare_compilation():\n",
    "    \"\"\"Compare training with and without torch.compile\"\"\"\n",
    "    \n",
    "    # Check if torch.compile is available (PyTorch 2.0+)\n",
    "    if not hasattr(torch, 'compile'):\n",
    "        print(\"‚ö†Ô∏è torch.compile not available. Please use PyTorch 2.0+\")\n",
    "        return\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    # Test different compilation modes\n",
    "    compilation_modes = [\n",
    "        {\"name\": \"No Compilation\", \"compile\": False, \"mode\": None},\n",
    "        {\"name\": \"Default Compile\", \"compile\": True, \"mode\": \"default\"},\n",
    "        {\"name\": \"Reduce Overhead\", \"compile\": True, \"mode\": \"reduce-overhead\"},\n",
    "        {\"name\": \"Max Autotune\", \"compile\": True, \"mode\": \"max-autotune\"},\n",
    "    ]\n",
    "    \n",
    "    compilation_results = {}\n",
    "    \n",
    "    for config in compilation_modes:\n",
    "        print(f\"\\n=== {config['name']} ===\")\n",
    "        \n",
    "        model = CompilationDemo()\n",
    "        \n",
    "        # Apply compilation if requested\n",
    "        if config['compile']:\n",
    "            print(f\"Compiling model with mode: {config['mode']}\")\n",
    "            model = torch.compile(model, mode=config['mode'])\n",
    "        \n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=2,  # Reduced for demo\n",
    "            enable_checkpointing=False,\n",
    "            logger=False,\n",
    "            enable_progress_bar=False,\n",
    "            limit_train_batches=100,  # More batches to see compilation benefits\n",
    "            limit_val_batches=30\n",
    "        )\n",
    "        \n",
    "        # Warmup run (compilation happens during first few iterations)\n",
    "        start_time = time.time()\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        final_acc = trainer.callback_metrics.get('val_acc', 0)\n",
    "        final_loss = trainer.callback_metrics.get('val_loss', float('inf'))\n",
    "        \n",
    "        compilation_results[config['name']] = {\n",
    "            'time': total_time,\n",
    "            'accuracy': final_acc,\n",
    "            'loss': final_loss\n",
    "        }\n",
    "        \n",
    "        print(f\"Training time: {total_time:.2f}s\")\n",
    "        print(f\"Final accuracy: {final_acc:.4f}\")\n",
    "    \n",
    "    return compilation_results\n",
    "\n",
    "# Test compilation if available\n",
    "print(\"\\n=== Model Compilation Comparison ===\")\n",
    "try:\n",
    "    compilation_results = compare_compilation()\n",
    "    if compilation_results:\n",
    "        print(f\"\\n=== Compilation Results Summary ===\")\n",
    "        baseline_time = compilation_results.get(\"No Compilation\", {}).get('time', 1)\n",
    "        \n",
    "        for name, result in compilation_results.items():\n",
    "            speedup = baseline_time / result['time'] if result['time'] > 0 else 1\n",
    "            print(f\"{name:18} | Time: {result['time']:5.2f}s | Speedup: {speedup:.2f}x | Acc: {result['accuracy']:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Compilation test failed: {e}\")\n",
    "```\n",
    "\n",
    "## Advanced Optimization Techniques\n",
    "\n",
    "### Combined Optimizations\n",
    "\n",
    "```python\n",
    "class OptimizedModel(pl.LightningModule):\n",
    "    \"\"\"Model with all optimization techniques combined\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Efficient model architecture\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LayerNorm(512),  # Better for mixed precision\n",
    "            nn.GELU(),          # Efficient activation\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "        from torchmetrics import Accuracy, MetricCollection\n",
    "        metrics = MetricCollection({\n",
    "            'accuracy': Accuracy(task=\"multiclass\", num_classes=10),\n",
    "        })\n",
    "        \n",
    "        self.train_metrics = metrics.clone(prefix='train_')\n",
    "        self.val_metrics = metrics.clone(prefix='val_')\n",
    "        \n",
    "        # Optimization tracking\n",
    "        self.optimization_stats = {\n",
    "            'grad_norms': [],\n",
    "            'training_times': [],\n",
    "            'memory_usage': []\n",
    "        }\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        # Time the forward pass\n",
    "        start_time = time.time()\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        forward_time = time.time() - start_time\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_metrics(preds, y)\n",
    "        \n",
    "        # Log performance metrics\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('forward_time_ms', forward_time * 1000, on_step=True)\n",
    "        self.log_dict(self.train_metrics, on_epoch=True)\n",
    "        \n",
    "        # Memory tracking\n",
    "        if torch.cuda.is_available() and batch_idx % 50 == 0:\n",
    "            memory_mb = torch.cuda.memory_allocated() / 1024**2\n",
    "            self.optimization_stats['memory_usage'].append(memory_mb)\n",
    "            self.log('memory_mb', memory_mb, on_step=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_metrics(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log_dict(self.val_metrics, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_before_optimizer_step(self, optimizer, optimizer_idx):\n",
    "        # Track gradient norms\n",
    "        total_norm = 0\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "        self.optimization_stats['grad_norms'].append(total_norm)\n",
    "        self.log('grad_norm', total_norm, on_step=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Optimized optimizer settings\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=1e-3,\n",
    "            weight_decay=1e-4,\n",
    "            eps=1e-4,  # Larger eps for mixed precision stability\n",
    "            betas=(0.9, 0.95)  # Slightly adjusted betas\n",
    "        )\n",
    "        \n",
    "        # Efficient learning rate schedule\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=1e-3,\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "            pct_start=0.05,  # Short warmup\n",
    "            div_factor=25,   # Initial lr = max_lr / div_factor\n",
    "            final_div_factor=1000  # Final lr = initial_lr / final_div_factor\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'step'\n",
    "            }\n",
    "        }\n",
    "\n",
    "def create_optimized_trainer():\n",
    "    \"\"\"Create trainer with all optimizations enabled\"\"\"\n",
    "    \n",
    "    return pl.Trainer(\n",
    "        # Performance optimizations\n",
    "        precision=\"16-mixed\",           # Mixed precision\n",
    "        accumulate_grad_batches=2,      # Gradient accumulation\n",
    "        gradient_clip_val=1.0,          # Gradient clipping\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        \n",
    "        # Training config\n",
    "        max_epochs=5,\n",
    "        \n",
    "        # Efficiency settings\n",
    "        enable_checkpointing=False,\n",
    "        logger=False,\n",
    "        enable_progress_bar=True,\n",
    "        log_every_n_steps=25\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Fully Optimized Training ===\")\n",
    "\n",
    "# Create optimized model and trainer\n",
    "optimized_model = OptimizedModel()\n",
    "\n",
    "# Apply compilation if available\n",
    "if hasattr(torch, 'compile'):\n",
    "    print(\"üöÄ Applying torch.compile...\")\n",
    "    optimized_model = torch.compile(optimized_model, mode=\"reduce-overhead\")\n",
    "\n",
    "optimized_trainer = create_optimized_trainer()\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(\"Training fully optimized model...\")\n",
    "start_time = time.time()\n",
    "optimized_trainer.fit(optimized_model, train_loader, val_loader)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "final_acc = optimized_trainer.callback_metrics.get('val_accuracy', 0)\n",
    "peak_memory = max(optimized_model.optimization_stats['memory_usage']) if optimized_model.optimization_stats['memory_usage'] else 0\n",
    "\n",
    "print(f\"‚úì Optimized training completed in {total_time:.2f}s\")\n",
    "print(f\"Final accuracy: {final_acc:.4f}\")\n",
    "print(f\"Peak memory usage: {peak_memory:.1f}MB\")\n",
    "print(f\"Average gradient norm: {np.mean(optimized_model.optimization_stats['grad_norms']):.4f}\")\n",
    "```\n",
    "\n",
    "## Best Practices Summary\n",
    "\n",
    "### Optimization Guidelines\n",
    "\n",
    "```python\n",
    "def print_optimization_guidelines():\n",
    "    \"\"\"Print comprehensive optimization guidelines\"\"\"\n",
    "    \n",
    "    guidelines = {\n",
    "        \"Gradient Accumulation\": {\n",
    "            \"‚úÖ Use when\": [\n",
    "                \"GPU memory is limited\",\n",
    "                \"Want larger effective batch sizes\",\n",
    "                \"Training very large models\"\n",
    "            ],\n",
    "            \"‚ö†Ô∏è Consider\": [\n",
    "                \"Batch normalization behavior changes\",\n",
    "                \"Optimizer steps happen less frequently\", \n",
    "                \"May need to adjust learning rates\"\n",
    "            ],\n",
    "            \"üîß Best practices\": [\n",
    "                \"accumulate_grad_batches=2-8 typically\",\n",
    "                \"Monitor effective batch size\",\n",
    "                \"Adjust LR schedule accordingly\"\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        \"Gradient Clipping\": {\n",
    "            \"‚úÖ Use when\": [\n",
    "                \"Training deep networks (>10 layers)\",\n",
    "                \"Gradient exploding problems\",\n",
    "                \"Using RNNs or transformers\"\n",
    "            ],\n",
    "            \"‚ö†Ô∏è Consider\": [\n",
    "                \"May slow convergence if too aggressive\",\n",
    "                \"Monitor clipping frequency\",\n",
    "                \"Different values for different optimizers\"\n",
    "            ],\n",
    "            \"üîß Best practices\": [\n",
    "                \"Start with clip_val=1.0\",\n",
    "                \"Use 'norm' clipping usually\",\n",
    "                \"Monitor gradient statistics\"\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        \"Model Compilation\": {\n",
    "            \"‚úÖ Use when\": [\n",
    "                \"PyTorch 2.0+ available\",\n",
    "                \"Model has many repeated operations\",\n",
    "                \"Training for many epochs\"\n",
    "            ],\n",
    "            \"‚ö†Ô∏è Consider\": [\n",
    "                \"First few iterations are slower\",\n",
    "                \"May increase memory usage\",\n",
    "                \"Debug mode disables optimizations\"\n",
    "            ],\n",
    "            \"üîß Best practices\": [\n",
    "                \"Use 'reduce-overhead' mode for training\",\n",
    "                \"Compile after model is finalized\",\n",
    "                \"Warmup with several batches\"\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        \"Combined Optimizations\": {\n",
    "            \"‚úÖ Recommended stack\": [\n",
    "                \"Mixed precision (16-mixed)\",\n",
    "                \"Gradient accumulation (2-4x)\",\n",
    "                \"Gradient clipping (norm, 1.0)\",\n",
    "                \"Model compilation (reduce-overhead)\",\n",
    "                \"AdamW optimizer with appropriate eps\"\n",
    "            ],\n",
    "            \"‚ö†Ô∏è Testing protocol\": [\n",
    "                \"Compare with baseline (FP32, no opts)\",\n",
    "                \"Monitor training stability\",\n",
    "                \"Verify final model accuracy\",\n",
    "                \"Profile memory usage\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üöÄ OPTIMIZATION BEST PRACTICES GUIDE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for category, info in guidelines.items():\n",
    "        print(f\"\\nüìä {category.upper()}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for section, items in info.items():\n",
    "            print(f\"\\n{section}\")\n",
    "            for item in items:\n",
    "                print(f\"  ‚Ä¢ {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üí° Remember: Always measure performance improvements!\")\n",
    "    print(\"üß™ Test each optimization individually before combining\")\n",
    "    print(\"üìà Monitor both speed AND accuracy when optimizing\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Print comprehensive guidelines\n",
    "print_optimization_guidelines()\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered advanced gradient techniques and model compilation:\n",
    "\n",
    "1. **Gradient Accumulation**: Simulate larger batch sizes by accumulating gradients across multiple mini-batches\n",
    "2. **Gradient Clipping**: Prevent gradient explosion using norm-based or value-based clipping\n",
    "3. **Model Compilation**: Use torch.compile for significant training speedups\n",
    "4. **Combined Optimizations**: Stack multiple techniques for maximum performance\n",
    "5. **Best Practices**: Guidelines for applying optimizations effectively\n",
    "\n",
    "Key optimization techniques:\n",
    "- **Gradient Accumulation**: 2-8x accumulation typical, monitor effective batch size\n",
    "- **Gradient Clipping**: Start with norm clipping at 1.0, adjust based on gradient statistics  \n",
    "- **Model Compilation**: Use \"reduce-overhead\" mode, expect warmup overhead\n",
    "- **Mixed Precision**: Combine with other optimizations for maximum benefit\n",
    "\n",
    "Performance benefits:\n",
    "- **Memory Efficiency**: Gradient accumulation enables training larger models\n",
    "- **Training Stability**: Gradient clipping prevents divergence in deep networks\n",
    "- **Speed Improvements**: Model compilation can provide 1.2-2x speedups\n",
    "- **Resource Utilization**: Better GPU utilization through optimized operations\n",
    "\n",
    "Practical considerations:\n",
    "- Test optimizations individually before combining\n",
    "- Monitor both training speed and model accuracy\n",
    "- Profile memory usage to avoid OOM errors\n",
    "- Adjust hyperparameters when changing batch dynamics\n",
    "- Use proper warmup periods for compiled models\n",
    "\n",
    "Next notebook: We'll explore profiling and performance tuning techniques."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
