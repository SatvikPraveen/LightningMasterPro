{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad40bd0",
   "metadata": {},
   "source": [
    "# Profiler and Performance Tuning\n",
    "\n",
    "**File Location:** `notebooks/04_performance_and_scaling/10_profiler_and_perf_tuning.ipynb`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook covers comprehensive performance profiling and tuning in PyTorch Lightning. Learn to identify bottlenecks, optimize data loading, tune hyperparameters for performance, and create production-ready training pipelines.\n",
    "\n",
    "## Lightning Profilers\n",
    "\n",
    "### Built-in Profiler Usage\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.profilers import SimpleProfiler, AdvancedProfiler, PyTorchProfiler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class ProfilingDemoModel(pl.LightningModule):\n",
    "    \"\"\"Model designed to demonstrate profiling capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Model with various operation types\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(32)\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 32, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape for conv1d: [batch, channels, length]\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        # Simulate some expensive operations\n",
    "        if batch_idx % 10 == 0:\n",
    "            # Expensive CPU operation\n",
    "            _ = torch.sin(torch.cos(x.cpu())).to(x.device)\n",
    "        \n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        \n",
    "        # Some metric computations\n",
    "        if batch_idx % 5 == 0:\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1).mean()\n",
    "            self.log('entropy', entropy, on_step=True)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Create dataset with some processing overhead\n",
    "def create_profiling_dataset(num_samples=2000, input_size=256, num_classes=10):\n",
    "    torch.manual_seed(42)\n",
    "    # Create more complex synthetic data\n",
    "    base_data = torch.randn(num_samples, input_size)\n",
    "    # Add some correlation structure\n",
    "    weights = torch.randn(input_size, input_size) * 0.1\n",
    "    X = base_data + torch.mm(base_data, weights)\n",
    "    \n",
    "    # Create targets with some noise\n",
    "    target_weights = torch.randn(input_size)\n",
    "    logits = X @ target_weights\n",
    "    y = torch.div(logits - logits.min(), (logits.max() - logits.min()) / (num_classes - 1), rounding_mode='floor').long()\n",
    "    y = torch.clamp(y, 0, num_classes - 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = create_profiling_dataset()\n",
    "dataset = TensorDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"‚úì Profiling demo setup completed\")\n",
    "```\n",
    "\n",
    "### Simple Profiler\n",
    "\n",
    "```python\n",
    "print(\"=== Simple Profiler ===\")\n",
    "\n",
    "# Simple profiler for basic timing information\n",
    "simple_profiler = SimpleProfiler(\n",
    "    dirpath=\"./profiling_results\",\n",
    "    filename=\"simple_profile\"\n",
    ")\n",
    "\n",
    "model = ProfilingDemoModel()\n",
    "trainer_simple = pl.Trainer(\n",
    "    max_epochs=2,\n",
    "    profiler=simple_profiler,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False,\n",
    "    limit_train_batches=30,\n",
    "    limit_val_batches=10\n",
    ")\n",
    "\n",
    "print(\"Training with Simple Profiler...\")\n",
    "trainer_simple.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Read and display profiling results\n",
    "profile_file = Path(\"./profiling_results/simple_profile.txt\")\n",
    "if profile_file.exists():\n",
    "    print(\"\\nüìä Simple Profiler Results:\")\n",
    "    with open(profile_file, 'r') as f:\n",
    "        content = f.read()\n",
    "        print(content[:1000] + \"...\" if len(content) > 1000 else content)\n",
    "else:\n",
    "    print(\"Profile file not found\")\n",
    "\n",
    "print(\"‚úì Simple profiling completed\")\n",
    "```\n",
    "\n",
    "### PyTorch Profiler (Advanced)\n",
    "\n",
    "```python\n",
    "print(\"\\n=== PyTorch Profiler ===\")\n",
    "\n",
    "# Advanced PyTorch profiler with detailed GPU/CPU analysis\n",
    "pytorch_profiler = PyTorchProfiler(\n",
    "    dirpath=\"./profiling_results\",\n",
    "    filename=\"pytorch_profile\",\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA] if torch.cuda.is_available() else [torch.profiler.ProfilerActivity.CPU],\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True,\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2)\n",
    ")\n",
    "\n",
    "model = ProfilingDemoModel()\n",
    "trainer_pytorch = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    profiler=pytorch_profiler,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False,\n",
    "    limit_train_batches=20\n",
    ")\n",
    "\n",
    "print(\"Training with PyTorch Profiler...\")\n",
    "trainer_pytorch.fit(model, train_loader, val_loader)\n",
    "print(\"‚úì PyTorch profiling completed - check tensorboard for detailed results\")\n",
    "```\n",
    "\n",
    "## Data Loading Optimization\n",
    "\n",
    "### DataLoader Performance Tuning\n",
    "\n",
    "```python\n",
    "import multiprocessing\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SlowDataset(Dataset):\n",
    "    \"\"\"Dataset with artificial processing delays\"\"\"\n",
    "    \n",
    "    def __init__(self, data, targets, processing_delay=0.001):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.delay = processing_delay\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Simulate preprocessing time\n",
    "        time.sleep(self.delay)\n",
    "        \n",
    "        # Some data transformations\n",
    "        x = self.data[idx]\n",
    "        y = self.targets[idx]\n",
    "        \n",
    "        # Simulate augmentation\n",
    "        if torch.rand(1) > 0.5:\n",
    "            x = x + torch.randn_like(x) * 0.01\n",
    "            \n",
    "        return x, y\n",
    "\n",
    "def benchmark_dataloader_configs():\n",
    "    \"\"\"Benchmark different DataLoader configurations\"\"\"\n",
    "    \n",
    "    # Create dataset with processing overhead\n",
    "    slow_dataset = SlowDataset(X[:800], y[:800], processing_delay=0.002)\n",
    "    \n",
    "    configs = [\n",
    "        {\"num_workers\": 0, \"pin_memory\": False, \"persistent_workers\": False},\n",
    "        {\"num_workers\": 2, \"pin_memory\": False, \"persistent_workers\": False},\n",
    "        {\"num_workers\": 4, \"pin_memory\": False, \"persistent_workers\": False},\n",
    "        {\"num_workers\": 2, \"pin_memory\": True, \"persistent_workers\": False},\n",
    "        {\"num_workers\": 2, \"pin_memory\": True, \"persistent_workers\": True},\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\n--- Config: {config} ---\")\n",
    "        \n",
    "        loader = DataLoader(\n",
    "            slow_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            **config\n",
    "        )\n",
    "        \n",
    "        # Benchmark loading time\n",
    "        start_time = time.time()\n",
    "        batch_times = []\n",
    "        \n",
    "        for i, batch in enumerate(loader):\n",
    "            batch_start = time.time()\n",
    "            # Simulate some processing\n",
    "            x, y = batch\n",
    "            _ = x.mean()\n",
    "            batch_end = time.time()\n",
    "            \n",
    "            batch_times.append(batch_end - batch_start)\n",
    "            \n",
    "            if i >= 20:  # Limit batches for demo\n",
    "                break\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        avg_batch_time = np.mean(batch_times)\n",
    "        \n",
    "        config_name = f\"workers_{config['num_workers']}_pin_{config['pin_memory']}_persist_{config['persistent_workers']}\"\n",
    "        results[config_name] = {\n",
    "            'total_time': total_time,\n",
    "            'avg_batch_time': avg_batch_time,\n",
    "            'batches_per_sec': 1 / avg_batch_time\n",
    "        }\n",
    "        \n",
    "        print(f\"Total time: {total_time:.2f}s, Avg batch: {avg_batch_time*1000:.1f}ms\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"=== DataLoader Optimization ===\")\n",
    "dataloader_results = benchmark_dataloader_configs()\n",
    "\n",
    "print(f\"\\nüìä DataLoader Benchmark Results:\")\n",
    "for config, result in dataloader_results.items():\n",
    "    print(f\"{config:25} | Total: {result['total_time']:5.2f}s | Batch: {result['avg_batch_time']*1000:5.1f}ms | Rate: {result['batches_per_sec']:5.1f} batch/s\")\n",
    "```\n",
    "\n",
    "### Memory and I/O Optimization\n",
    "\n",
    "```python\n",
    "class OptimizedDataset(Dataset):\n",
    "    \"\"\"Memory-optimized dataset with caching\"\"\"\n",
    "    \n",
    "    def __init__(self, data, targets, cache_size=100):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.cache = {}\n",
    "        self.cache_size = cache_size\n",
    "        self.access_count = {}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Check cache first\n",
    "        if idx in self.cache:\n",
    "            self.access_count[idx] = self.access_count.get(idx, 0) + 1\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        # Process data\n",
    "        x = self.data[idx]\n",
    "        y = self.targets[idx]\n",
    "        \n",
    "        # Apply transformations\n",
    "        x_processed = x + torch.randn_like(x) * 0.01\n",
    "        \n",
    "        # Cache if space available\n",
    "        if len(self.cache) < self.cache_size:\n",
    "            self.cache[idx] = (x_processed, y)\n",
    "        else:\n",
    "            # Replace least accessed item\n",
    "            least_accessed = min(self.cache.keys(), key=lambda k: self.access_count.get(k, 0))\n",
    "            del self.cache[least_accessed]\n",
    "            if least_accessed in self.access_count:\n",
    "                del self.access_count[least_accessed]\n",
    "            self.cache[idx] = (x_processed, y)\n",
    "        \n",
    "        self.access_count[idx] = 1\n",
    "        return x_processed, y\n",
    "\n",
    "def compare_dataset_implementations():\n",
    "    \"\"\"Compare different dataset implementations\"\"\"\n",
    "    \n",
    "    # Test datasets\n",
    "    regular_dataset = TensorDataset(X[:1000], y[:1000])\n",
    "    slow_dataset = SlowDataset(X[:1000], y[:1000], processing_delay=0.001)\n",
    "    optimized_dataset = OptimizedDataset(X[:1000], y[:1000], cache_size=200)\n",
    "    \n",
    "    datasets = [\n",
    "        (\"Regular TensorDataset\", regular_dataset),\n",
    "        (\"Slow Dataset\", slow_dataset),\n",
    "        (\"Optimized Dataset\", optimized_dataset)\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, dataset in datasets:\n",
    "        print(f\"\\n--- Testing {name} ---\")\n",
    "        \n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for i, batch in enumerate(loader):\n",
    "            if i >= 15:\n",
    "                break\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        results[name] = total_time\n",
    "        \n",
    "        print(f\"Time for 15 batches: {total_time:.3f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n=== Dataset Implementation Comparison ===\")\n",
    "dataset_results = compare_dataset_implementations()\n",
    "\n",
    "fastest_time = min(dataset_results.values())\n",
    "print(f\"\\nüìä Dataset Performance Comparison:\")\n",
    "for name, time_taken in dataset_results.items():\n",
    "    speedup = fastest_time / time_taken\n",
    "    print(f\"{name:20} | Time: {time_taken:.3f}s | Relative speed: {speedup:.2f}x\")\n",
    "```\n",
    "\n",
    "## Performance Monitoring and Metrics\n",
    "\n",
    "### Custom Performance Callbacks\n",
    "\n",
    "```python\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class PerformanceMonitorCallback(Callback):\n",
    "    \"\"\"Comprehensive performance monitoring callback\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.batch_times = []\n",
    "        self.epoch_times = []\n",
    "        self.memory_usage = []\n",
    "        self.gpu_utilization = []\n",
    "        self.step_times = {}\n",
    "        \n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        self.epoch_start_time = time.time()\n",
    "        \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        pl_module.log('epoch_duration', epoch_time)\n",
    "        \n",
    "    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):\n",
    "        self.batch_start_time = time.time()\n",
    "        \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        batch_time = time.time() - self.batch_start_time\n",
    "        self.batch_times.append(batch_time)\n",
    "        \n",
    "        # Log batch timing every N steps\n",
    "        if batch_idx % 20 == 0:\n",
    "            pl_module.log('batch_time_ms', batch_time * 1000, on_step=True)\n",
    "            \n",
    "            # Memory tracking\n",
    "            if torch.cuda.is_available():\n",
    "                memory_mb = torch.cuda.memory_allocated() / 1024**2\n",
    "                self.memory_usage.append(memory_mb)\n",
    "                pl_module.log('gpu_memory_mb', memory_mb, on_step=True)\n",
    "                \n",
    "                # GPU utilization (approximate)\n",
    "                utilization = min(100, batch_time * 100 / 0.05)  # Rough estimate\n",
    "                self.gpu_utilization.append(utilization)\n",
    "                pl_module.log('gpu_utilization_pct', utilization, on_step=True)\n",
    "    \n",
    "    def on_validation_start(self, trainer, pl_module):\n",
    "        self.val_start_time = time.time()\n",
    "        \n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        val_time = time.time() - self.val_start_time\n",
    "        pl_module.log('validation_duration', val_time)\n",
    "        \n",
    "        # Performance summary\n",
    "        if self.batch_times:\n",
    "            avg_batch_time = np.mean(self.batch_times[-100:])  # Last 100 batches\n",
    "            throughput = 32 / avg_batch_time  # Samples per second (assuming batch_size=32)\n",
    "            \n",
    "            pl_module.log('avg_batch_time_ms', avg_batch_time * 1000)\n",
    "            pl_module.log('throughput_samples_per_sec', throughput)\n",
    "        \n",
    "        if self.memory_usage:\n",
    "            peak_memory = max(self.memory_usage[-100:])\n",
    "            avg_memory = np.mean(self.memory_usage[-100:])\n",
    "            \n",
    "            pl_module.log('peak_memory_mb', peak_memory)\n",
    "            pl_module.log('avg_memory_mb', avg_memory)\n",
    "    \n",
    "    def get_performance_summary(self):\n",
    "        \"\"\"Get comprehensive performance summary\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        if self.batch_times:\n",
    "            summary['avg_batch_time'] = np.mean(self.batch_times)\n",
    "            summary['batch_time_std'] = np.std(self.batch_times)\n",
    "            summary['min_batch_time'] = np.min(self.batch_times)\n",
    "            summary['max_batch_time'] = np.max(self.batch_times)\n",
    "        \n",
    "        if self.epoch_times:\n",
    "            summary['avg_epoch_time'] = np.mean(self.epoch_times)\n",
    "            summary['total_training_time'] = sum(self.epoch_times)\n",
    "        \n",
    "        if self.memory_usage:\n",
    "            summary['peak_memory_mb'] = np.max(self.memory_usage)\n",
    "            summary['avg_memory_mb'] = np.mean(self.memory_usage)\n",
    "        \n",
    "        if self.gpu_utilization:\n",
    "            summary['avg_gpu_utilization'] = np.mean(self.gpu_utilization)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "class ResourceUsageCallback(Callback):\n",
    "    \"\"\"Monitor CPU and system resource usage\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            import psutil\n",
    "            self.psutil = psutil\n",
    "            self.cpu_percentages = []\n",
    "            self.memory_percentages = []\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è psutil not available for system monitoring\")\n",
    "            self.psutil = None\n",
    "    \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if self.psutil and batch_idx % 50 == 0:\n",
    "            cpu_percent = self.psutil.cpu_percent(interval=None)\n",
    "            memory_info = self.psutil.virtual_memory()\n",
    "            \n",
    "            self.cpu_percentages.append(cpu_percent)\n",
    "            self.memory_percentages.append(memory_info.percent)\n",
    "            \n",
    "            pl_module.log('cpu_usage_pct', cpu_percent, on_step=True)\n",
    "            pl_module.log('system_memory_pct', memory_info.percent, on_step=True)\n",
    "    \n",
    "    def get_resource_summary(self):\n",
    "        if not self.psutil:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'avg_cpu_usage': np.mean(self.cpu_percentages) if self.cpu_percentages else 0,\n",
    "            'peak_cpu_usage': np.max(self.cpu_percentages) if self.cpu_percentages else 0,\n",
    "            'avg_memory_usage': np.mean(self.memory_percentages) if self.memory_percentages else 0,\n",
    "            'peak_memory_usage': np.max(self.memory_percentages) if self.memory_percentages else 0\n",
    "        }\n",
    "\n",
    "# Test performance monitoring\n",
    "print(\"\\n=== Performance Monitoring ===\")\n",
    "\n",
    "perf_callback = PerformanceMonitorCallback()\n",
    "resource_callback = ResourceUsageCallback()\n",
    "\n",
    "model = ProfilingDemoModel()\n",
    "trainer_perf = pl.Trainer(\n",
    "    max_epochs=2,\n",
    "    callbacks=[perf_callback, resource_callback],\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False,\n",
    "    limit_train_batches=50,\n",
    "    limit_val_batches=15\n",
    ")\n",
    "\n",
    "print(\"Training with performance monitoring...\")\n",
    "trainer_perf.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Get performance summaries\n",
    "perf_summary = perf_callback.get_performance_summary()\n",
    "resource_summary = resource_callback.get_resource_summary()\n",
    "\n",
    "print(\"\\nüìä Performance Summary:\")\n",
    "for key, value in perf_summary.items():\n",
    "    if 'time' in key:\n",
    "        print(f\"{key}: {value:.3f}s\")\n",
    "    elif 'memory' in key:\n",
    "        print(f\"{key}: {value:.1f}MB\")\n",
    "    else:\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "\n",
    "print(\"\\nüñ•Ô∏è Resource Usage Summary:\")\n",
    "for key, value in resource_summary.items():\n",
    "    print(f\"{key}: {value:.1f}%\")\n",
    "```\n",
    "\n",
    "## Production Performance Optimization\n",
    "\n",
    "### Comprehensive Optimization Pipeline\n",
    "\n",
    "```python\n",
    "class ProductionOptimizedModel(pl.LightningModule):\n",
    "    \"\"\"Production model with all performance optimizations\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Optimized architecture\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(), \n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "        # Efficient metrics\n",
    "        from torchmetrics import Accuracy\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Efficient accuracy computation\n",
    "        with torch.no_grad():\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            self.train_acc(preds, y)\n",
    "        \n",
    "        # Minimal logging for performance\n",
    "        if batch_idx % 25 == 0:\n",
    "            self.log('train_loss', loss, on_step=True, prog_bar=True)\n",
    "        \n",
    "        self.log('train_acc', self.train_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Optimized optimizer settings\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=2e-3,  # Higher LR for faster convergence\n",
    "            weight_decay=1e-4,\n",
    "            eps=1e-4,  # Better for mixed precision\n",
    "            betas=(0.9, 0.95)\n",
    "        )\n",
    "        \n",
    "        # Efficient scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=2e-3,\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "            pct_start=0.05,\n",
    "            div_factor=25,\n",
    "            final_div_factor=1000\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'step'\n",
    "            }\n",
    "        }\n",
    "\n",
    "def create_production_trainer():\n",
    "    \"\"\"Create production-optimized trainer\"\"\"\n",
    "    \n",
    "    return pl.Trainer(\n",
    "        # Core performance settings\n",
    "        precision=\"16-mixed\",\n",
    "        accumulate_grad_batches=2,\n",
    "        gradient_clip_val=1.0,\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        \n",
    "        # Training efficiency\n",
    "        max_epochs=5,\n",
    "        log_every_n_steps=50,  # Reduce logging overhead\n",
    "        \n",
    "        # Validation efficiency\n",
    "        check_val_every_n_epoch=1,\n",
    "        num_sanity_val_steps=2,  # Minimal sanity check\n",
    "        \n",
    "        # System optimization\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=False,  # Skip for production\n",
    "        \n",
    "        # Callbacks for monitoring\n",
    "        callbacks=[PerformanceMonitorCallback()],\n",
    "        \n",
    "        # Disable for demo\n",
    "        enable_checkpointing=False,\n",
    "        logger=False\n",
    "    )\n",
    "\n",
    "def create_optimized_dataloader(dataset, batch_size=64, is_training=True):\n",
    "    \"\"\"Create performance-optimized DataLoader\"\"\"\n",
    "    \n",
    "    num_workers = min(4, multiprocessing.cpu_count())\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=is_training,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        drop_last=is_training,  # For training stability\n",
    "        prefetch_factor=2  # Prefetch more batches\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Production Optimization Pipeline ===\")\n",
    "\n",
    "# Create optimized components\n",
    "prod_model = ProductionOptimizedModel()\n",
    "\n",
    "# Apply compilation if available\n",
    "if hasattr(torch, 'compile'):\n",
    "    print(\"üöÄ Applying torch.compile for production...\")\n",
    "    prod_model = torch.compile(prod_model, mode=\"reduce-overhead\")\n",
    "\n",
    "# Optimized data loading\n",
    "prod_train_loader = create_optimized_dataloader(train_dataset, batch_size=64, is_training=True)\n",
    "prod_val_loader = create_optimized_dataloader(val_dataset, batch_size=64, is_training=False)\n",
    "\n",
    "# Production trainer\n",
    "prod_trainer = create_production_trainer()\n",
    "\n",
    "print(\"üè≠ Starting production-optimized training...\")\n",
    "start_time = time.time()\n",
    "prod_trainer.fit(prod_model, prod_train_loader, prod_val_loader)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Get final performance metrics\n",
    "final_acc = prod_trainer.callback_metrics.get('val_acc', 0)\n",
    "perf_callback = [cb for cb in prod_trainer.callbacks if isinstance(cb, PerformanceMonitorCallback)][0]\n",
    "perf_summary = perf_callback.get_performance_summary()\n",
    "\n",
    "print(f\"\\nüéØ Production Training Results:\")\n",
    "print(f\"Total training time: {total_time:.2f}s\")\n",
    "print(f\"Final validation accuracy: {final_acc:.4f}\")\n",
    "print(f\"Average batch time: {perf_summary.get('avg_batch_time', 0)*1000:.1f}ms\")\n",
    "print(f\"Peak memory usage: {perf_summary.get('peak_memory_mb', 0):.1f}MB\")\n",
    "\n",
    "# Throughput calculation\n",
    "total_samples = len(train_dataset) * prod_trainer.current_epoch\n",
    "throughput = total_samples / total_time\n",
    "print(f\"Training throughput: {throughput:.0f} samples/second\")\n",
    "```\n",
    "\n",
    "## Performance Best Practices\n",
    "\n",
    "### Optimization Checklist\n",
    "\n",
    "```python\n",
    "def print_performance_checklist():\n",
    "    \"\"\"Comprehensive performance optimization checklist\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"üöÄ Model Architecture\": [\n",
    "            \"Use LayerNorm instead of BatchNorm for mixed precision\",\n",
    "            \"Prefer GELU over ReLU for better numerical properties\", \n",
    "            \"Minimize model complexity without sacrificing accuracy\",\n",
    "            \"Use efficient attention mechanisms (if applicable)\",\n",
    "            \"Avoid unnecessary model.eval()/model.train() switches\"\n",
    "        ],\n",
    "        \n",
    "        \"‚ö° Training Configuration\": [\n",
    "            \"Enable mixed precision (precision='16-mixed')\",\n",
    "            \"Use gradient accumulation for effective larger batches\",\n",
    "            \"Apply gradient clipping (clip_val=1.0 typically)\",\n",
    "            \"Optimize learning rate schedule (OneCycleLR often best)\",\n",
    "            \"Use AdamW with appropriate eps for mixed precision\"\n",
    "        ],\n",
    "        \n",
    "        \"üíæ Data Loading\": [\n",
    "            \"Use multiple workers (2-8 typically optimal)\",\n",
    "            \"Enable pin_memory for GPU training\",\n",
    "            \"Use persistent_workers=True for repeated epochs\",\n",
    "            \"Prefetch data with prefetch_factor=2-4\",\n",
    "            \"Optimize batch size for your GPU memory\"\n",
    "        ],\n",
    "        \n",
    "        \"üîß System Optimization\": [\n",
    "            \"Compile model with torch.compile (PyTorch 2.0+)\",\n",
    "            \"Use appropriate CUDA versions and drivers\", \n",
    "            \"Optimize CPU/GPU data transfer\",\n",
    "            \"Monitor and eliminate CPU/GPU bottlenecks\",\n",
    "            \"Use NVMe SSDs for fast data loading\"\n",
    "        ],\n",
    "        \n",
    "        \"üìä Monitoring\": [\n",
    "            \"Profile regularly with PyTorchProfiler\",\n",
    "            \"Monitor GPU utilization and memory\",\n",
    "            \"Track batch processing times\",\n",
    "            \"Log performance metrics during training\",\n",
    "            \"Set up alerts for performance degradation\"\n",
    "        ],\n",
    "        \n",
    "        \"üéØ Production Deployment\": [\n",
    "            \"Reduce logging frequency in production\",\n",
    "            \"Minimize validation frequency if appropriate\",\n",
    "            \"Use efficient checkpointing strategies\",\n",
    "            \"Implement proper error handling and recovery\",\n",
    "            \"Plan for scaling and distributed training\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üöÄ PERFORMANCE OPTIMIZATION CHECKLIST\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for category, items in checklist.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        print(\"-\" * 50)\n",
    "        for item in items:\n",
    "            print(f\"  ‚ñ° {item}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"üí° Key Performance Metrics to Monitor:\")\n",
    "    print(\"  ‚Ä¢ Batch processing time (target: <100ms for small models)\")\n",
    "    print(\"  ‚Ä¢ GPU utilization (target: >80%)\")\n",
    "    print(\"  ‚Ä¢ Memory usage (target: <90% of available)\")\n",
    "    print(\"  ‚Ä¢ Training throughput (samples/second)\")\n",
    "    print(\"  ‚Ä¢ Time to convergence\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "print_performance_checklist()\n",
    "\n",
    "# Performance optimization summary\n",
    "print(f\"\\nüìà Performance Optimization Summary:\")\n",
    "print(f\"‚úì Mixed precision training can provide 1.5-2x speedup\")\n",
    "print(f\"‚úì Proper data loading can improve throughput by 2-5x\")\n",
    "print(f\"‚úì Model compilation can provide 1.2-1.8x speedup\") \n",
    "print(f\"‚úì Gradient accumulation enables training larger models\")\n",
    "print(f\"‚úì Combined optimizations can provide 3-10x overall speedup\")\n",
    "print(f\"\\nüéØ Always measure performance improvements systematically!\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered comprehensive performance profiling and optimization:\n",
    "\n",
    "1. **Lightning Profilers**: Simple, Advanced, and PyTorch profilers for identifying bottlenecks\n",
    "2. **Data Loading Optimization**: num_workers, pin_memory, persistent_workers tuning\n",
    "3. **Performance Monitoring**: Custom callbacks for tracking training metrics\n",
    "4. **Resource Usage**: CPU, memory, and GPU utilization monitoring\n",
    "5. **Production Optimization**: Complete optimization pipeline for deployment\n",
    "6. **Best Practices**: Comprehensive checklist for performance optimization\n",
    "\n",
    "Key profiling tools:\n",
    "- **SimpleProfiler**: Basic timing information for quick analysis\n",
    "- **PyTorchProfiler**: Detailed GPU/CPU profiling with TensorBoard integration\n",
    "- **Custom Callbacks**: Real-time performance monitoring during training\n",
    "\n",
    "Data loading optimizations:\n",
    "- **num_workers**: 2-8 workers typically optimal\n",
    "- **pin_memory**: Essential for GPU training\n",
    "- **persistent_workers**: Reduces worker initialization overhead\n",
    "- **prefetch_factor**: 2-4 for better pipeline utilization\n",
    "\n",
    "Performance monitoring metrics:\n",
    "- **Batch Time**: Target <100ms for efficient training\n",
    "- **GPU Utilization**: Aim for >80% utilization\n",
    "- **Memory Usage**: Keep <90% of available memory\n",
    "- **Throughput**: Track samples processed per second\n",
    "\n",
    "Production considerations:\n",
    "- Combine multiple optimizations for maximum benefit\n",
    "- Profile regularly to identify new bottlenecks\n",
    "- Monitor performance degradation over time\n",
    "- Plan for scaling and distributed training\n",
    "- Implement proper error handling and recovery\n",
    "\n",
    "Expected performance gains:\n",
    "- Mixed precision: 1.5-2x speedup\n",
    "- Data loading optimization: 2-5x improvement\n",
    "- Model compilation: 1.2-1.8x speedup\n",
    "- Combined optimizations: 3-10x overall improvement\n",
    "\n",
    "Next notebook: We'll explore multi-GPU strategies and distributed training."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
