{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889412f7",
   "metadata": {},
   "source": [
    "# File Location: notebooks/07_evaluation_export_predict/16_test_predict_loops.ipynb\n",
    "\n",
    "# Test and Prediction Loops Implementation\n",
    "\n",
    "This notebook explores advanced test and prediction loops in PyTorch Lightning, including custom evaluation strategies, batch prediction handling, and comprehensive model testing workflows.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement custom test and prediction loops\n",
    "- Handle various evaluation scenarios and metrics\n",
    "- Build batch prediction systems with proper memory management\n",
    "- Create comprehensive model testing pipelines\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from pytorch_lightning.loops.base import Loop\n",
    "from pytorch_lightning.loops import EvaluationLoop\n",
    "import os\n",
    "import json\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Lightning version: {pl.__version__}\")\n",
    "```\n",
    "\n",
    "## 1. Understanding Test and Prediction Loops\n",
    "\n",
    "```python\n",
    "class TestPredictLoopConcepts:\n",
    "    \"\"\"\n",
    "    Test and Prediction Loop Concepts:\n",
    "    \n",
    "    1. Test Loops: Evaluation on test sets with metrics computation\n",
    "    2. Prediction Loops: Inference on new data without ground truth\n",
    "    3. Batch Processing: Efficient handling of large datasets\n",
    "    4. Memory Management: Preventing OOM during inference\n",
    "    5. Result Aggregation: Collecting and organizing outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def explain_differences():\n",
    "        differences = {\n",
    "            \"Test Loop\": {\n",
    "                \"Purpose\": \"Evaluate model performance with ground truth\",\n",
    "                \"Outputs\": \"Metrics, losses, predictions\",\n",
    "                \"Use Case\": \"Final model evaluation\",\n",
    "                \"Memory\": \"Stores metrics and some predictions\"\n",
    "            },\n",
    "            \"Prediction Loop\": {\n",
    "                \"Purpose\": \"Generate predictions on new data\",\n",
    "                \"Outputs\": \"Predictions, confidence scores\",\n",
    "                \"Use Case\": \"Production inference\",\n",
    "                \"Memory\": \"Optimized for large-scale inference\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for loop_type, details in differences.items():\n",
    "            print(f\"{loop_type}:\")\n",
    "            for aspect, description in details.items():\n",
    "                print(f\"  {aspect}: {description}\")\n",
    "            print()\n",
    "\n",
    "TestPredictLoopConcepts.explain_differences()\n",
    "```\n",
    "\n",
    "## 2. Advanced Test Loop Implementation\n",
    "\n",
    "```python\n",
    "class DetailedTestLoop(Loop):\n",
    "    \"\"\"Custom test loop with comprehensive evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, save_predictions=True, compute_metrics=True):\n",
    "        super().__init__()\n",
    "        self.save_predictions = save_predictions\n",
    "        self.compute_metrics = compute_metrics\n",
    "        \n",
    "        # Results storage\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        self.logits = []\n",
    "        self.losses = []\n",
    "        self.sample_indices = []\n",
    "        \n",
    "        # Batch tracking\n",
    "        self.current_batch = 0\n",
    "        self.total_batches = 0\n",
    "        \n",
    "        # Metrics storage\n",
    "        self.batch_metrics = []\n",
    "        self.class_metrics = {}\n",
    "        \n",
    "    @property\n",
    "    def done(self) -> bool:\n",
    "        return self.current_batch >= self.total_batches\n",
    "    \n",
    "    def setup(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Setup the test loop\"\"\"\n",
    "        # Get test dataloader\n",
    "        if hasattr(self.trainer, 'test_dataloaders'):\n",
    "            self.dataloader = self.trainer.test_dataloaders[0]\n",
    "        else:\n",
    "            raise ValueError(\"No test dataloader found\")\n",
    "        \n",
    "        self.total_batches = len(self.dataloader)\n",
    "        self.dataloader_iter = iter(self.dataloader)\n",
    "        \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset loop state\"\"\"\n",
    "        self.current_batch = 0\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        self.logits = []\n",
    "        self.losses = []\n",
    "        self.sample_indices = []\n",
    "        self.batch_metrics = []\n",
    "        \n",
    "        if hasattr(self, 'dataloader_iter'):\n",
    "            del self.dataloader_iter\n",
    "    \n",
    "    def advance(self) -> None:\n",
    "        \"\"\"Process one test batch\"\"\"\n",
    "        try:\n",
    "            batch = next(self.dataloader_iter)\n",
    "            \n",
    "            # Extract data\n",
    "            if len(batch) == 2:\n",
    "                x, y = batch\n",
    "            elif len(batch) == 3:\n",
    "                x, y, indices = batch\n",
    "                self.sample_indices.extend(indices.cpu().numpy() if hasattr(indices, 'cpu') else indices)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected batch format: {len(batch)} elements\")\n",
    "            \n",
    "            # Move to device\n",
    "            if torch.cuda.is_available():\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                logits = self.trainer.lightning_module(x)\n",
    "                loss = F.cross_entropy(logits, y, reduction='none')\n",
    "                \n",
    "                # Store results\n",
    "                self.logits.extend(logits.cpu())\n",
    "                self.targets.extend(y.cpu())\n",
    "                self.losses.extend(loss.cpu())\n",
    "                \n",
    "                # Get predictions\n",
    "                _, preds = torch.max(logits, 1)\n",
    "                self.predictions.extend(preds.cpu())\n",
    "                \n",
    "                # Compute batch metrics if requested\n",
    "                if self.compute_metrics:\n",
    "                    batch_acc = (preds == y).float().mean().item()\n",
    "                    batch_loss = loss.mean().item()\n",
    "                    \n",
    "                    self.batch_metrics.append({\n",
    "                        'batch_idx': self.current_batch,\n",
    "                        'accuracy': batch_acc,\n",
    "                        'loss': batch_loss,\n",
    "                        'samples': len(y)\n",
    "                    })\n",
    "            \n",
    "            self.current_batch += 1\n",
    "            \n",
    "        except StopIteration:\n",
    "            pass\n",
    "    \n",
    "    def on_run_end(self) -> None:\n",
    "        \"\"\"Compute final metrics and save results\"\"\"\n",
    "        if not self.predictions:\n",
    "            print(\"No predictions collected\")\n",
    "            return\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        predictions = np.array([p.item() if hasattr(p, 'item') else p for p in self.predictions])\n",
    "        targets = np.array([t.item() if hasattr(t, 'item') else t for t in self.targets])\n",
    "        losses = np.array([l.item() if hasattr(l, 'item') else l for l in self.losses])\n",
    "        \n",
    "        # Compute overall metrics\n",
    "        overall_accuracy = (predictions == targets).mean()\n",
    "        overall_loss = losses.mean()\n",
    "        \n",
    "        print(f\"Test Results:\")\n",
    "        print(f\"  Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "        print(f\"  Overall Loss: {overall_loss:.4f}\")\n",
    "        print(f\"  Total Samples: {len(predictions)}\")\n",
    "        \n",
    "        # Per-class metrics\n",
    "        unique_classes = np.unique(targets)\n",
    "        class_accuracies = {}\n",
    "        \n",
    "        for cls in unique_classes:\n",
    "            mask = targets == cls\n",
    "            if mask.sum() > 0:\n",
    "                cls_acc = (predictions[mask] == targets[mask]).mean()\n",
    "                class_accuracies[int(cls)] = cls_acc\n",
    "                print(f\"  Class {cls} Accuracy: {cls_acc:.4f} ({mask.sum()} samples)\")\n",
    "        \n",
    "        self.class_metrics = class_accuracies\n",
    "        \n",
    "        # Store results in trainer's lightning module\n",
    "        if hasattr(self.trainer.lightning_module, 'test_results'):\n",
    "            self.trainer.lightning_module.test_results.update({\n",
    "                'predictions': predictions,\n",
    "                'targets': targets,\n",
    "                'losses': losses,\n",
    "                'overall_accuracy': overall_accuracy,\n",
    "                'overall_loss': overall_loss,\n",
    "                'class_accuracies': class_accuracies,\n",
    "                'batch_metrics': self.batch_metrics\n",
    "            })\n",
    "        else:\n",
    "            self.trainer.lightning_module.test_results = {\n",
    "                'predictions': predictions,\n",
    "                'targets': targets,\n",
    "                'losses': losses,\n",
    "                'overall_accuracy': overall_accuracy,\n",
    "                'overall_loss': overall_loss,\n",
    "                'class_accuracies': class_accuracies,\n",
    "                'batch_metrics': self.batch_metrics\n",
    "            }\n",
    "\n",
    "print(\"Detailed test loop implementation complete!\")\n",
    "```\n",
    "\n",
    "## 3. Custom Prediction Loop\n",
    "\n",
    "```python\n",
    "class BatchPredictionLoop(Loop):\n",
    "    \"\"\"Custom prediction loop for efficient batch inference\"\"\"\n",
    "    \n",
    "    def __init__(self, return_logits=True, return_probabilities=True, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.return_logits = return_logits\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Results storage\n",
    "        self.predictions = []\n",
    "        self.logits_list = []\n",
    "        self.probabilities_list = []\n",
    "        self.features_list = []\n",
    "        \n",
    "        # Processing state\n",
    "        self.current_batch = 0\n",
    "        self.total_batches = 0\n",
    "        \n",
    "        # Memory management\n",
    "        self.max_memory_mb = 1000  # Maximum memory usage in MB\n",
    "        \n",
    "    @property\n",
    "    def done(self) -> bool:\n",
    "        return self.current_batch >= self.total_batches\n",
    "    \n",
    "    def setup(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Setup the prediction loop\"\"\"\n",
    "        # Get prediction dataloader\n",
    "        if hasattr(self.trainer, 'predict_dataloaders'):\n",
    "            self.dataloader = self.trainer.predict_dataloaders[0]\n",
    "        else:\n",
    "            raise ValueError(\"No prediction dataloader found\")\n",
    "        \n",
    "        self.total_batches = len(self.dataloader)\n",
    "        self.dataloader_iter = iter(self.dataloader)\n",
    "        \n",
    "        # Estimate memory usage\n",
    "        self._estimate_memory_usage()\n",
    "        \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset loop state\"\"\"\n",
    "        self.current_batch = 0\n",
    "        self.predictions = []\n",
    "        self.logits_list = []\n",
    "        self.probabilities_list = []\n",
    "        self.features_list = []\n",
    "        \n",
    "        if hasattr(self, 'dataloader_iter'):\n",
    "            del self.dataloader_iter\n",
    "    \n",
    "    def _estimate_memory_usage(self):\n",
    "        \"\"\"Estimate memory usage for the prediction loop\"\"\"\n",
    "        sample_batch = next(iter(self.dataloader))\n",
    "        if isinstance(sample_batch, (list, tuple)):\n",
    "            x = sample_batch[0]\n",
    "        else:\n",
    "            x = sample_batch\n",
    "        \n",
    "        # Estimate memory per batch\n",
    "        batch_size = x.size(0)\n",
    "        memory_per_sample = x.numel() * 4 / (1024 * 1024)  # 4 bytes per float32, convert to MB\n",
    "        estimated_memory = memory_per_sample * batch_size * self.total_batches\n",
    "        \n",
    "        print(f\"Estimated memory usage: {estimated_memory:.2f} MB\")\n",
    "        \n",
    "        if estimated_memory > self.max_memory_mb:\n",
    "            print(f\"Warning: Estimated memory usage exceeds limit ({self.max_memory_mb} MB)\")\n",
    "            print(\"Consider processing in chunks or reducing batch size\")\n",
    "    \n",
    "    def advance(self) -> None:\n",
    "        \"\"\"Process one prediction batch\"\"\"\n",
    "        try:\n",
    "            batch = next(self.dataloader_iter)\n",
    "            \n",
    "            # Handle different batch formats\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                x = batch[0]\n",
    "            else:\n",
    "                x = batch\n",
    "            \n",
    "            # Move to device\n",
    "            if torch.cuda.is_available():\n",
    "                x = x.cuda()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                # Get model output\n",
    "                logits = self.trainer.lightning_module(x)\n",
    "                \n",
    "                # Get predictions\n",
    "                _, preds = torch.max(logits, 1)\n",
    "                self.predictions.extend(preds.cpu().numpy())\n",
    "                \n",
    "                # Store logits if requested\n",
    "                if self.return_logits:\n",
    "                    self.logits_list.extend(logits.cpu().numpy())\n",
    "                \n",
    "                # Store probabilities if requested\n",
    "                if self.return_probabilities:\n",
    "                    probs = F.softmax(logits, dim=1)\n",
    "                    self.probabilities_list.extend(probs.cpu().numpy())\n",
    "                \n",
    "                # Extract features if model supports it\n",
    "                if hasattr(self.trainer.lightning_module, 'extract_features'):\n",
    "                    features = self.trainer.lightning_module.extract_features(x)\n",
    "                    self.features_list.extend(features.cpu().numpy())\n",
    "            \n",
    "            self.current_batch += 1\n",
    "            \n",
    "            # Memory management: periodically clear GPU cache\n",
    "            if self.current_batch % 10 == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "        except StopIteration:\n",
    "            pass\n",
    "    \n",
    "    def on_run_end(self) -> None:\n",
    "        \"\"\"Organize and save prediction results\"\"\"\n",
    "        results = {\n",
    "            'predictions': np.array(self.predictions),\n",
    "            'num_samples': len(self.predictions)\n",
    "        }\n",
    "        \n",
    "        if self.return_logits and self.logits_list:\n",
    "            results['logits'] = np.array(self.logits_list)\n",
    "        \n",
    "        if self.return_probabilities and self.probabilities_list:\n",
    "            results['probabilities'] = np.array(self.probabilities_list)\n",
    "        \n",
    "        if self.features_list:\n",
    "            results['features'] = np.array(self.features_list)\n",
    "        \n",
    "        # Store in lightning module\n",
    "        self.trainer.lightning_module.prediction_results = results\n",
    "        \n",
    "        print(f\"Prediction completed:\")\n",
    "        print(f\"  Processed {len(self.predictions)} samples\")\n",
    "        print(f\"  Predictions shape: {results['predictions'].shape}\")\n",
    "        \n",
    "        if 'probabilities' in results:\n",
    "            print(f\"  Probabilities shape: {results['probabilities'].shape}\")\n",
    "        \n",
    "        if 'features' in results:\n",
    "            print(f\"  Features shape: {results['features'].shape}\")\n",
    "\n",
    "print(\"Custom prediction loop implementation complete!\")\n",
    "```\n",
    "\n",
    "## 4. Enhanced Lightning Module for Testing\n",
    "\n",
    "```python\n",
    "class TestPredictModel(pl.LightningModule):\n",
    "    \"\"\"Lightning module optimized for testing and prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Model architecture with feature extraction capability\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_acc = pl.metrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = pl.metrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = pl.metrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "        # Results storage\n",
    "        self.test_results = {}\n",
    "        self.prediction_results = {}\n",
    "        \n",
    "        # Custom loops\n",
    "        self.custom_test_loop = DetailedTestLoop(save_predictions=True)\n",
    "        self.custom_predict_loop = BatchPredictionLoop(return_logits=True, return_probabilities=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "        \"\"\"Extract features without classification\"\"\"\n",
    "        return self.feature_extractor(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.train_acc(logits, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.val_acc(logits, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"Enhanced test step with detailed logging\"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # Compute metrics\n",
    "        self.test_acc(logits, y)\n",
    "        \n",
    "        # Get predictions and probabilities\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('test_acc', self.test_acc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # Return detailed results\n",
    "        return {\n",
    "            'test_loss': loss,\n",
    "            'predictions': preds,\n",
    "            'targets': y,\n",
    "            'probabilities': probs,\n",
    "            'logits': logits\n",
    "        }\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        \"\"\"Enhanced prediction step\"\"\"\n",
    "        # Handle different batch formats\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            x = batch[0]\n",
    "        else:\n",
    "            x = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.extract_features(x)\n",
    "        \n",
    "        return {\n",
    "            'predictions': preds,\n",
    "            'probabilities': probs,\n",
    "            'logits': logits,\n",
    "            'features': features\n",
    "        }\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_test_summary(self):\n",
    "        \"\"\"Get comprehensive test summary\"\"\"\n",
    "        if not self.test_results:\n",
    "            return \"No test results available\"\n",
    "        \n",
    "        summary = [\"Test Results Summary:\", \"=\" * 30]\n",
    "        summary.append(f\"Overall Accuracy: {self.test_results.get('overall_accuracy', 0):.4f}\")\n",
    "        summary.append(f\"Overall Loss: {self.test_results.get('overall_loss', 0):.4f}\")\n",
    "        summary.append(f\"Total Samples: {len(self.test_results.get('predictions', []))}\")\n",
    "        \n",
    "        if 'class_accuracies' in self.test_results:\n",
    "            summary.append(\"\\nPer-Class Accuracies:\")\n",
    "            for cls, acc in self.test_results['class_accuracies'].items():\n",
    "                summary.append(f\"  Class {cls}: {acc:.4f}\")\n",
    "        \n",
    "        return \"\\n\".join(summary)\n",
    "\n",
    "# Initialize model\n",
    "model = TestPredictModel(num_classes=10, learning_rate=0.001)\n",
    "```\n",
    "\n",
    "## 5. Data Module for Testing and Prediction\n",
    "\n",
    "```python\n",
    "class TestPredictDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Data module with comprehensive test and prediction support\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size=64, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        # Transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = torchvision.datasets.MNIST('./data', train=True, transform=self.transform, download=True)\n",
    "            self.val_dataset = torchvision.datasets.MNIST('./data', train=False, transform=self.transform, download=True)\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_dataset = torchvision.datasets.MNIST('./data', train=False, transform=self.transform, download=True)\n",
    "        \n",
    "        if stage == 'predict' or stage is None:\n",
    "            # For prediction, we can use the same test dataset (without labels in practice)\n",
    "            self.predict_dataset = torchvision.datasets.MNIST('./data', train=False, transform=self.transform, download=True)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "# Initialize data module\n",
    "data_module = TestPredictDataModule(batch_size=64, num_workers=4)\n",
    "```\n",
    "\n",
    "## 6. Training and Testing Pipeline\n",
    "\n",
    "```python\n",
    "# Training\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    log_every_n_steps=50,\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=[\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            monitor='val_acc',\n",
    "            mode='max',\n",
    "            save_top_k=1,\n",
    "            filename='best-model-{epoch:02d}-{val_acc:.2f}'\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Training model...\")\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Testing with standard Lightning test\n",
    "print(\"\\nStandard Lightning test:\")\n",
    "test_results = trainer.test(model, data_module, verbose=True)\n",
    "\n",
    "# Testing with custom test loop\n",
    "print(\"\\nCustom detailed test:\")\n",
    "model.custom_test_loop.trainer = trainer\n",
    "data_module.setup('test')\n",
    "trainer.test_dataloaders = [data_module.test_dataloader()]\n",
    "model.custom_test_loop.run()\n",
    "\n",
    "# Print detailed test results\n",
    "print(model.get_test_summary())\n",
    "\n",
    "# Prediction with standard Lightning predict\n",
    "print(\"\\nStandard Lightning predict:\")\n",
    "predictions = trainer.predict(model, data_module)\n",
    "\n",
    "# Print prediction summary\n",
    "if predictions:\n",
    "    print(f\"Prediction batches: {len(predictions)}\")\n",
    "    if predictions[0] and 'predictions' in predictions[0]:\n",
    "        total_predictions = sum(len(batch['predictions']) for batch in predictions)\n",
    "        print(f\"Total predictions: {total_predictions}\")\n",
    "\n",
    "print(\"Testing and prediction pipeline completed!\")\n",
    "```\n",
    "\n",
    "## 7. Advanced Evaluation Metrics and Visualization\n",
    "\n",
    "```python\n",
    "class AdvancedEvaluator:\n",
    "    \"\"\"Advanced evaluation tools for test and prediction results\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def plot_confusion_matrix(self, normalize=True):\n",
    "        \"\"\"Plot detailed confusion matrix\"\"\"\n",
    "        if not hasattr(self.model, 'test_results') or 'predictions' not in self.model.test_results:\n",
    "            print(\"No test results available\")\n",
    "            return\n",
    "        \n",
    "        predictions = self.model.test_results['predictions']\n",
    "        targets = self.model.test_results['targets']\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(targets, predictions)\n",
    "        \n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "            fmt = '.2f'\n",
    "        else:\n",
    "            title = 'Confusion Matrix'\n",
    "            fmt = 'd'\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues',\n",
    "                    xticklabels=range(10), yticklabels=range(10))\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_prediction_confidence(self):\n",
    "        \"\"\"Analyze prediction confidence and calibration\"\"\"\n",
    "        if not hasattr(self.model, 'prediction_results') or 'probabilities' not in self.model.prediction_results:\n",
    "            print(\"No probability predictions available\")\n",
    "            return\n",
    "        \n",
    "        probs = self.model.prediction_results['probabilities']\n",
    "        predictions = self.model.prediction_results['predictions']\n",
    "        \n",
    "        # Get max probabilities (confidence scores)\n",
    "        max_probs = np.max(probs, axis=1)\n",
    "        \n",
    "        # Create confidence histogram\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Confidence distribution\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.hist(max_probs, bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Prediction Confidence')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Confidence Score Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Confidence by class\n",
    "        plt.subplot(1, 3, 2)\n",
    "        for class_idx in range(min(10, probs.shape[1])):\n",
    "            class_mask = predictions == class_idx\n",
    "            if class_mask.sum() > 0:\n",
    "                class_confidences = max_probs[class_mask]\n",
    "                plt.hist(class_confidences, bins=20, alpha=0.5, label=f'Class {class_idx}')\n",
    "        \n",
    "        plt.xlabel('Prediction Confidence')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Confidence by Predicted Class')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Confidence statistics\n",
    "        plt.subplot(1, 3, 3)\n",
    "        class_avg_conf = []\n",
    "        class_labels = []\n",
    "        \n",
    "        for class_idx in range(min(10, probs.shape[1])):\n",
    "            class_mask = predictions == class_idx\n",
    "            if class_mask.sum() > 0:\n",
    "                avg_conf = max_probs[class_mask].mean()\n",
    "                class_avg_conf.append(avg_conf)\n",
    "                class_labels.append(f'Class {class_idx}')\n",
    "        \n",
    "        plt.bar(class_labels, class_avg_conf, alpha=0.7)\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Average Confidence')\n",
    "        plt.title('Average Confidence by Class')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"Confidence Statistics:\")\n",
    "        print(f\"  Mean confidence: {max_probs.mean():.4f}\")\n",
    "        print(f\"  Std confidence: {max_probs.std():.4f}\")\n",
    "        print(f\"  Min confidence: {max_probs.min():.4f}\")\n",
    "        print(f\"  Max confidence: {max_probs.max():.4f}\")\n",
    "    \n",
    "    def error_analysis(self):\n",
    "        \"\"\"Detailed error analysis\"\"\"\n",
    "        if not hasattr(self.model, 'test_results'):\n",
    "            print(\"No test results available\")\n",
    "            return\n",
    "        \n",
    "        predictions = self.model.test_results['predictions']\n",
    "        targets = self.model.test_results['targets']\n",
    "        losses = self.model.test_results['losses']\n",
    "        \n",
    "        # Find misclassified samples\n",
    "        errors = predictions != targets\n",
    "        error_indices = np.where(errors)[0]\n",
    "        \n",
    "        print(f\"Error Analysis:\")\n",
    "        print(f\"  Total errors: {errors.sum()}\")\n",
    "        print(f\"  Error rate: {errors.mean():.4f}\")\n",
    "        \n",
    "        # Most common error types\n",
    "        error_pairs = list(zip(targets[errors], predictions[errors]))\n",
    "        from collections import Counter\n",
    "        common_errors = Counter(error_pairs).most_common(10)\n",
    "        \n",
    "        print(f\"\\nMost Common Errors:\")\n",
    "        for (true_label, pred_label), count in common_errors:\n",
    "            print(f\"  {true_label} -> {pred_label}: {count} times\")\n",
    "        \n",
    "        # High-loss samples\n",
    "        high_loss_indices = np.argsort(losses)[-10:]\n",
    "        print(f\"\\nHigh-Loss Samples:\")\n",
    "        for idx in high_loss_indices:\n",
    "            print(f\"  Sample {idx}: True={targets[idx]}, Pred={predictions[idx]}, Loss={losses[idx]:.4f}\")\n",
    "    \n",
    "    def generate_classification_report(self):\n",
    "        \"\"\"Generate comprehensive classification report\"\"\"\n",
    "        if not hasattr(self.model, 'test_results'):\n",
    "            print(\"No test results available\")\n",
    "            return\n",
    "        \n",
    "        predictions = self.model.test_results['predictions']\n",
    "        targets = self.model.test_results['targets']\n",
    "        \n",
    "        # Generate report\n",
    "        report = classification_report(targets, predictions, target_names=[f'Class {i}' for i in range(10)])\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        \n",
    "        # Save to file\n",
    "        with open('classification_report.txt', 'w') as f:\n",
    "            f.write(report)\n",
    "        print(\"Report saved to classification_report.txt\")\n",
    "\n",
    "# Run evaluations\n",
    "evaluator = AdvancedEvaluator(model)\n",
    "evaluator.plot_confusion_matrix(normalize=True)\n",
    "evaluator.error_analysis()\n",
    "evaluator.generate_classification_report()\n",
    "\n",
    "# Run prediction analysis if we have prediction results\n",
    "if hasattr(model, 'prediction_results'):\n",
    "    evaluator.analyze_prediction_confidence()\n",
    "```\n",
    "\n",
    "## 8. Batch Prediction Pipeline\n",
    "\n",
    "```python\n",
    "class ProductionPredictionPipeline:\n",
    "    \"\"\"Production-ready prediction pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, model, batch_size=64):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.model.eval()\n",
    "        \n",
    "    def predict_from_arrays(self, data_arrays):\n",
    "        \"\"\"Predict from numpy arrays\"\"\"\n",
    "        # Convert to tensor dataset\n",
    "        tensor_data = torch.FloatTensor(data_arrays)\n",
    "        dataset = torch.utils.data.TensorDataset(tensor_data)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                x = batch[0]\n",
    "                if torch.cuda.is_available():\n",
    "                    x = x.cuda()\n",
    "                \n",
    "                logits = self.model(x)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                \n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                probabilities.extend(probs.cpu().numpy())\n",
    "        \n",
    "        return np.array(predictions), np.array(probabilities)\n",
    "    \n",
    "    def predict_with_uncertainty(self, data_arrays, num_samples=10):\n",
    "        \"\"\"Predict with uncertainty estimation using Monte Carlo dropout\"\"\"\n",
    "        # Enable dropout for uncertainty estimation\n",
    "        def enable_dropout(model):\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, nn.Dropout):\n",
    "                    module.train()\n",
    "        \n",
    "        enable_dropout(self.model)\n",
    "        \n",
    "        tensor_data = torch.FloatTensor(data_arrays)\n",
    "        dataset = torch.utils.data.TensorDataset(tensor_data)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        all_predictions = []\n",
    "        \n",
    "        # Multiple forward passes\n",
    "        for sample in range(num_samples):\n",
    "            sample_predictions = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in dataloader:\n",
    "                    x = batch[0]\n",
    "                    if torch.cuda.is_available():\n",
    "                        x = x.cuda()\n",
    "                    \n",
    "                    logits = self.model(x)\n",
    "                    probs = F.softmax(logits, dim=1)\n",
    "                    sample_predictions.extend(probs.cpu().numpy())\n",
    "            \n",
    "            all_predictions.append(np.array(sample_predictions))\n",
    "        \n",
    "        # Compute statistics\n",
    "        all_predictions = np.stack(all_predictions)  # (num_samples, num_data, num_classes)\n",
    "        \n",
    "        mean_predictions = all_predictions.mean(axis=0)\n",
    "        std_predictions = all_predictions.std(axis=0)\n",
    "        final_predictions = np.argmax(mean_predictions, axis=1)\n",
    "        \n",
    "        # Epistemic uncertainty (model uncertainty)\n",
    "        epistemic_uncertainty = std_predictions.mean(axis=1)\n",
    "        \n",
    "        # Aleatoric uncertainty (data uncertainty) \n",
    "        aleatoric_uncertainty = -np.sum(mean_predictions * np.log(mean_predictions + 1e-8), axis=1)\n",
    "        \n",
    "        self.model.eval()  # Reset to eval mode\n",
    "        \n",
    "        return {\n",
    "            'predictions': final_predictions,\n",
    "            'probabilities': mean_predictions,\n",
    "            'epistemic_uncertainty': epistemic_uncertainty,\n",
    "            'aleatoric_uncertainty': aleatoric_uncertainty,\n",
    "            'prediction_std': std_predictions\n",
    "        }\n",
    "    \n",
    "    def save_predictions(self, predictions, probabilities, output_path='predictions.json'):\n",
    "        \"\"\"Save predictions in JSON format\"\"\"\n",
    "        results = {\n",
    "            'predictions': predictions.tolist() if hasattr(predictions, 'tolist') else predictions,\n",
    "            'probabilities': probabilities.tolist() if hasattr(probabilities, 'tolist') else probabilities,\n",
    "            'metadata': {\n",
    "                'num_samples': len(predictions),\n",
    "                'num_classes': len(probabilities[0]) if len(probabilities) > 0 else 0,\n",
    "                'batch_size': self.batch_size\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "pipeline = ProductionPredictionPipeline(model, batch_size=64)\n",
    "\n",
    "# Generate some dummy data for demonstration\n",
    "dummy_data = np.random.randn(100, 1, 28, 28).astype(np.float32)\n",
    "\n",
    "# Standard prediction\n",
    "preds, probs = pipeline.predict_from_arrays(dummy_data)\n",
    "print(f\"Standard predictions shape: {preds.shape}\")\n",
    "print(f\"Probabilities shape: {probs.shape}\")\n",
    "\n",
    "# Uncertainty estimation\n",
    "uncertainty_results = pipeline.predict_with_uncertainty(dummy_data[:20], num_samples=5)  # Smaller sample for demo\n",
    "print(f\"Uncertainty predictions shape: {uncertainty_results['predictions'].shape}\")\n",
    "print(f\"Epistemic uncertainty mean: {uncertainty_results['epistemic_uncertainty'].mean():.4f}\")\n",
    "\n",
    "# Save predictions\n",
    "pipeline.save_predictions(preds, probs, 'mnist_predictions.json')\n",
    "```\n",
    "\n",
    "# Summary\n",
    "\n",
    "This notebook demonstrated comprehensive test and prediction loop implementations in PyTorch Lightning. Key concepts and implementations covered:\n",
    "\n",
    "## Core Loop Implementations\n",
    "- **Detailed Test Loops**: Custom evaluation with comprehensive metrics collection\n",
    "- **Batch Prediction Loops**: Memory-efficient inference for large datasets\n",
    "- **Result Management**: Organized storage and retrieval of evaluation results\n",
    "- **Performance Monitoring**: Real-time tracking of evaluation progress\n",
    "\n",
    "## Advanced Testing Features\n",
    "- **Per-Class Metrics**: Detailed accuracy analysis by class\n",
    "- **Error Analysis**: Identification and categorization of model mistakes  \n",
    "- **Confusion Matrix**: Visual representation of classification performance\n",
    "- **Statistical Reporting**: Comprehensive classification reports with precision/recall\n",
    "\n",
    "## Production Prediction Capabilities\n",
    "- **Batch Processing**: Efficient handling of large-scale inference\n",
    "- **Uncertainty Estimation**: Monte Carlo dropout for prediction confidence\n",
    "- **Memory Management**: Smart memory usage and cleanup strategies\n",
    "- **Result Serialization**: JSON export for downstream applications\n",
    "\n",
    "## Evaluation Tools and Visualization\n",
    "- **Confidence Analysis**: Distribution and calibration of prediction confidence\n",
    "- **Error Categorization**: Common failure modes and high-loss sample identification\n",
    "- **Visual Diagnostics**: Confusion matrices and performance visualizations\n",
    "- **Production Pipeline**: End-to-end inference workflow for deployment\n",
    "\n",
    "## Key Benefits\n",
    "- **Comprehensive Evaluation**: Beyond simple accuracy metrics\n",
    "- **Production Ready**: Scalable inference pipelines\n",
    "- **Debugging Support**: Detailed error analysis capabilities\n",
    "- **Flexibility**: Customizable loops for specific requirements\n",
    "\n",
    "## Next Steps\n",
    "- Integrate with MLOps platforms for automated evaluation\n",
    "- Implement A/B testing frameworks for model comparison  \n",
    "- Add support for regression and multi-label classification\n",
    "- Develop real-time prediction serving capabilities\n",
    "\n",
    "The test and prediction loop framework provides the foundation for robust model evaluation and deployment in production environments."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
