{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a55c04",
   "metadata": {},
   "source": [
    "# File Location: notebooks/07_evaluation_export_predict/17_onnx_torchscript_export.ipynb\n",
    "\n",
    "# ONNX and TorchScript Export for Production Deployment\n",
    "\n",
    "This notebook covers model export strategies using ONNX and TorchScript for production deployment, including optimization techniques, cross-platform compatibility, and performance benchmarking.\n",
    "\n",
    "## Learning Objectives\n",
    "- Export PyTorch Lightning models to ONNX and TorchScript formats\n",
    "- Optimize models for different deployment scenarios\n",
    "- Implement cross-platform compatibility testing\n",
    "- Benchmark performance across different export formats\n",
    "- Handle dynamic shapes and batch processing\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ONNX and optimization imports\n",
    "try:\n",
    "    import onnx\n",
    "    import onnxruntime as ort\n",
    "    ONNX_AVAILABLE = True\n",
    "    print(f\"ONNX version: {onnx.__version__}\")\n",
    "    print(f\"ONNX Runtime version: {ort.__version__}\")\n",
    "except ImportError:\n",
    "    ONNX_AVAILABLE = False\n",
    "    print(\"ONNX not available. Install with: pip install onnx onnxruntime\")\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Lightning version: {pl.__version__}\")\n",
    "```\n",
    "\n",
    "## 1. Export-Ready Lightning Module\n",
    "\n",
    "```python\n",
    "class ExportableModel(pl.LightningModule):\n",
    "    \"\"\"Lightning module optimized for export to different formats\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, input_shape=(1, 28, 28), learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Model architecture - designed for export compatibility\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((7, 7))\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_acc = pl.metrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = pl.metrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass optimized for export\"\"\"\n",
    "        # Ensure input is the right type and shape\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if len(x.shape) == 3:  # Add batch dimension\n",
    "            x = x.unsqueeze(0)\n",
    "        \n",
    "        features = self.features(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "    \n",
    "    def predict_with_softmax(self, x):\n",
    "        \"\"\"Forward pass with softmax - useful for ONNX export\"\"\"\n",
    "        logits = self.forward(x)\n",
    "        return F.softmax(logits, dim=1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.train_acc(logits, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.val_acc(logits, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def export_summary(self):\n",
    "        \"\"\"Print model summary for export validation\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"Model Export Summary:\")\n",
    "        print(f\"  Input shape: {self.input_shape}\")\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"  Model size (MB): {total_params * 4 / 1024 / 1024:.2f}\")\n",
    "\n",
    "# Initialize model\n",
    "model = ExportableModel(num_classes=10, input_shape=(1, 28, 28))\n",
    "model.export_summary()\n",
    "```\n",
    "\n",
    "## 2. Data Module for Export Testing\n",
    "\n",
    "```python\n",
    "class ExportDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Data module for export testing\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size=64, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = torchvision.datasets.MNIST('./data', train=True, transform=self.transform, download=True)\n",
    "            self.val_dataset = torchvision.datasets.MNIST('./data', train=False, transform=self.transform, download=True)\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_dataset = torchvision.datasets.MNIST('./data', train=False, transform=self.transform, download=True)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "# Train the model first\n",
    "data_module = ExportDataModule(batch_size=128)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False\n",
    ")\n",
    "\n",
    "print(\"Training model for export...\")\n",
    "trainer.fit(model, data_module)\n",
    "print(f\"Training completed. Final validation accuracy: {trainer.callback_metrics.get('val_acc', 'N/A')}\")\n",
    "```\n",
    "\n",
    "## 3. TorchScript Export Implementation\n",
    "\n",
    "```python\n",
    "class TorchScriptExporter:\n",
    "    \"\"\"Handle TorchScript export with various optimization options\"\"\"\n",
    "    \n",
    "    def __init__(self, model: pl.LightningModule):\n",
    "        self.model = model\n",
    "        self.model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    def export_scripted(self, example_input: torch.Tensor, save_path: str = \"model_scripted.pt\") -> torch.jit.ScriptModule:\n",
    "        \"\"\"Export model using torch.jit.script\"\"\"\n",
    "        try:\n",
    "            print(\"Exporting with torch.jit.script...\")\n",
    "            scripted_model = torch.jit.script(self.model)\n",
    "            \n",
    "            # Test the scripted model\n",
    "            with torch.no_grad():\n",
    "                scripted_output = scripted_model(example_input)\n",
    "                original_output = self.model(example_input)\n",
    "                \n",
    "                # Check outputs match\n",
    "                if torch.allclose(scripted_output, original_output, atol=1e-6):\n",
    "                    print(\"✓ Script export successful - outputs match\")\n",
    "                else:\n",
    "                    print(\"⚠ Warning: Script export outputs differ slightly\")\n",
    "            \n",
    "            # Save the model\n",
    "            torch.jit.save(scripted_model, save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "            \n",
    "            return scripted_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Script export failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def export_traced(self, example_input: torch.Tensor, save_path: str = \"model_traced.pt\") -> torch.jit.TracedModule:\n",
    "        \"\"\"Export model using torch.jit.trace\"\"\"\n",
    "        try:\n",
    "            print(\"Exporting with torch.jit.trace...\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                traced_model = torch.jit.trace(self.model, example_input)\n",
    "                \n",
    "                # Test the traced model\n",
    "                traced_output = traced_model(example_input)\n",
    "                original_output = self.model(example_input)\n",
    "                \n",
    "                # Check outputs match\n",
    "                if torch.allclose(traced_output, original_output, atol=1e-6):\n",
    "                    print(\"✓ Trace export successful - outputs match\")\n",
    "                else:\n",
    "                    print(\"⚠ Warning: Trace export outputs differ slightly\")\n",
    "            \n",
    "            # Save the model\n",
    "            torch.jit.save(traced_model, save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "            \n",
    "            return traced_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trace export failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def optimize_torchscript(self, scripted_model: torch.jit.ScriptModule, example_input: torch.Tensor) -> torch.jit.ScriptModule:\n",
    "        \"\"\"Apply TorchScript optimizations\"\"\"\n",
    "        print(\"Applying TorchScript optimizations...\")\n",
    "        \n",
    "        try:\n",
    "            # Freeze the model for inference optimization\n",
    "            optimized_model = torch.jit.freeze(scripted_model)\n",
    "            \n",
    "            # Apply graph optimizations\n",
    "            optimized_model = torch.jit.optimize_for_inference(optimized_model)\n",
    "            \n",
    "            # Warmup\n",
    "            with torch.no_grad():\n",
    "                for _ in range(10):\n",
    "                    _ = optimized_model(example_input)\n",
    "            \n",
    "            print(\"✓ TorchScript optimization completed\")\n",
    "            return optimized_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"TorchScript optimization failed: {str(e)}\")\n",
    "            return scripted_model\n",
    "    \n",
    "    def compare_torchscript_methods(self, example_input: torch.Tensor) -> Dict[str, Any]:\n",
    "        \"\"\"Compare different TorchScript export methods\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Original model\n",
    "        print(\"\\n=== TorchScript Export Comparison ===\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            for _ in range(100):\n",
    "                original_output = self.model(example_input)\n",
    "            original_time = time.time() - start_time\n",
    "        \n",
    "        results['original'] = {\n",
    "            'time': original_time,\n",
    "            'output_shape': original_output.shape\n",
    "        }\n",
    "        \n",
    "        # Script export\n",
    "        scripted_model = self.export_scripted(example_input, \"model_scripted.pt\")\n",
    "        if scripted_model:\n",
    "            with torch.no_grad():\n",
    "                start_time = time.time()\n",
    "                for _ in range(100):\n",
    "                    scripted_output = scripted_model(example_input)\n",
    "                scripted_time = time.time() - start_time\n",
    "            \n",
    "            results['scripted'] = {\n",
    "                'time': scripted_time,\n",
    "                'speedup': original_time / scripted_time,\n",
    "                'output_shape': scripted_output.shape\n",
    "            }\n",
    "        \n",
    "        # Trace export\n",
    "        traced_model = self.export_traced(example_input, \"model_traced.pt\")\n",
    "        if traced_model:\n",
    "            with torch.no_grad():\n",
    "                start_time = time.time()\n",
    "                for _ in range(100):\n",
    "                    traced_output = traced_model(example_input)\n",
    "                traced_time = time.time() - start_time\n",
    "            \n",
    "            results['traced'] = {\n",
    "                'time': traced_time,\n",
    "                'speedup': original_time / traced_time,\n",
    "                'output_shape': traced_output.shape\n",
    "            }\n",
    "            \n",
    "            # Optimized traced model\n",
    "            optimized_model = self.optimize_torchscript(traced_model, example_input)\n",
    "            with torch.no_grad():\n",
    "                start_time = time.time()\n",
    "                for _ in range(100):\n",
    "                    optimized_output = optimized_model(example_input)\n",
    "                optimized_time = time.time() - start_time\n",
    "            \n",
    "            results['optimized'] = {\n",
    "                'time': optimized_time,\n",
    "                'speedup': original_time / optimized_time,\n",
    "                'output_shape': optimized_output.shape\n",
    "            }\n",
    "        \n",
    "        # Print comparison\n",
    "        print(\"\\nTorchScript Performance Comparison:\")\n",
    "        for method, metrics in results.items():\n",
    "            if method == 'original':\n",
    "                print(f\"{method:12}: {metrics['time']:.4f}s (baseline)\")\n",
    "            else:\n",
    "                print(f\"{method:12}: {metrics['time']:.4f}s ({metrics['speedup']:.2f}x speedup)\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test TorchScript export\n",
    "example_input = torch.randn(1, 1, 28, 28)\n",
    "torchscript_exporter = TorchScriptExporter(model)\n",
    "torchscript_results = torchscript_exporter.compare_torchscript_methods(example_input)\n",
    "```\n",
    "\n",
    "## 4. ONNX Export Implementation\n",
    "\n",
    "```python\n",
    "class ONNXExporter:\n",
    "    \"\"\"Handle ONNX export with optimization options\"\"\"\n",
    "    \n",
    "    def __init__(self, model: pl.LightningModule):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "    \n",
    "    def export_onnx(self, example_input: torch.Tensor, save_path: str = \"model.onnx\", \n",
    "                   dynamic_axes: Optional[Dict] = None, opset_version: int = 11) -> bool:\n",
    "        \"\"\"Export model to ONNX format\"\"\"\n",
    "        if not ONNX_AVAILABLE:\n",
    "            print(\"ONNX not available. Please install onnx and onnxruntime\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            print(f\"Exporting to ONNX (opset version {opset_version})...\")\n",
    "            \n",
    "            # Default dynamic axes for batch size flexibility\n",
    "            if dynamic_axes is None:\n",
    "                dynamic_axes = {\n",
    "                    'input': {0: 'batch_size'},\n",
    "                    'output': {0: 'batch_size'}\n",
    "                }\n",
    "            \n",
    "            # Export to ONNX\n",
    "            torch.onnx.export(\n",
    "                self.model,\n",
    "                example_input,\n",
    "                save_path,\n",
    "                export_params=True,\n",
    "                opset_version=opset_version,\n",
    "                do_constant_folding=True,  # Optimize constant folding\n",
    "                input_names=['input'],\n",
    "                output_names=['output'],\n",
    "                dynamic_axes=dynamic_axes,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Verify the exported model\n",
    "            if self._verify_onnx_model(save_path, example_input):\n",
    "                print(f\"✓ ONNX export successful - model saved to {save_path}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"✗ ONNX export verification failed\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ONNX export failed: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _verify_onnx_model(self, model_path: str, example_input: torch.Tensor) -> bool:\n",
    "        \"\"\"Verify ONNX model correctness\"\"\"\n",
    "        try:\n",
    "            # Load and check ONNX model\n",
    "            onnx_model = onnx.load(model_path)\n",
    "            onnx.checker.check_model(onnx_model)\n",
    "            \n",
    "            # Create ONNX Runtime session\n",
    "            ort_session = ort.InferenceSession(model_path)\n",
    "            \n",
    "            # Run inference\n",
    "            ort_inputs = {ort_session.get_inputs()[0].name: example_input.numpy()}\n",
    "            ort_outputs = ort_session.run(None, ort_inputs)\n",
    "            \n",
    "            # Compare with original PyTorch output\n",
    "            with torch.no_grad():\n",
    "                torch_output = self.model(example_input)\n",
    "            \n",
    "            # Check if outputs are close\n",
    "            if np.allclose(torch_output.numpy(), ort_outputs[0], atol=1e-5):\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Warning: ONNX outputs differ from PyTorch outputs\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ONNX verification failed: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def optimize_onnx(self, model_path: str, optimized_path: str = \"model_optimized.onnx\") -> bool:\n",
    "        \"\"\"Apply ONNX optimization\"\"\"\n",
    "        if not ONNX_AVAILABLE:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            print(\"Applying ONNX optimizations...\")\n",
    "            \n",
    "            # Available optimization levels: 'basic', 'extended', 'layout', 'all'\n",
    "            sess_options = ort.SessionOptions()\n",
    "            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "            \n",
    "            # Load and optimize\n",
    "            providers = ['CPUExecutionProvider']\n",
    "            if torch.cuda.is_available():\n",
    "                providers.insert(0, 'CUDAExecutionProvider')\n",
    "            \n",
    "            session = ort.InferenceSession(model_path, sess_options, providers=providers)\n",
    "            \n",
    "            print(f\"✓ ONNX model optimized for providers: {session.get_providers()}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ONNX optimization failed: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def benchmark_onnx_vs_pytorch(self, model_path: str, example_input: torch.Tensor, num_runs: int = 100) -> Dict[str, float]:\n",
    "        \"\"\"Benchmark ONNX vs PyTorch performance\"\"\"\n",
    "        if not ONNX_AVAILABLE:\n",
    "            return {}\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # PyTorch benchmark\n",
    "        print(\"Benchmarking PyTorch model...\")\n",
    "        with torch.no_grad():\n",
    "            # Warmup\n",
    "            for _ in range(10):\n",
    "                _ = self.model(example_input)\n",
    "            \n",
    "            # Timing\n",
    "            start_time = time.time()\n",
    "            for _ in range(num_runs):\n",
    "                pytorch_output = self.model(example_input)\n",
    "            pytorch_time = time.time() - start_time\n",
    "        \n",
    "        results['pytorch'] = pytorch_time\n",
    "        \n",
    "        # ONNX benchmark\n",
    "        try:\n",
    "            print(\"Benchmarking ONNX model...\")\n",
    "            \n",
    "            # Setup ONNX Runtime\n",
    "            providers = ['CPUExecutionProvider']\n",
    "            if torch.cuda.is_available():\n",
    "                providers.insert(0, 'CUDAExecutionProvider')\n",
    "            \n",
    "            ort_session = ort.InferenceSession(model_path, providers=providers)\n",
    "            ort_input_name = ort_session.get_inputs()[0].name\n",
    "            ort_inputs = {ort_input_name: example_input.numpy()}\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(10):\n",
    "                _ = ort_session.run(None, ort_inputs)\n",
    "            \n",
    "            # Timing\n",
    "            start_time = time.time()\n",
    "            for _ in range(num_runs):\n",
    "                onnx_outputs = ort_session.run(None, ort_inputs)\n",
    "            onnx_time = time.time() - start_time\n",
    "            \n",
    "            results['onnx'] = onnx_time\n",
    "            results['speedup'] = pytorch_time / onnx_time\n",
    "            \n",
    "            # Verify outputs still match\n",
    "            if np.allclose(pytorch_output.numpy(), onnx_outputs[0], atol=1e-5):\n",
    "                print(\"✓ ONNX outputs match PyTorch outputs\")\n",
    "            else:\n",
    "                print(\"⚠ Warning: ONNX outputs differ from PyTorch outputs\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ONNX benchmarking failed: {str(e)}\")\n",
    "            return results\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nBenchmark Results ({num_runs} runs):\")\n",
    "        print(f\"PyTorch: {pytorch_time:.4f}s\")\n",
    "        print(f\"ONNX:    {onnx_time:.4f}s\")\n",
    "        print(f\"Speedup: {results.get('speedup', 0):.2f}x\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test ONNX export if available\n",
    "if ONNX_AVAILABLE:\n",
    "    onnx_exporter = ONNXExporter(model)\n",
    "    \n",
    "    # Export with different configurations\n",
    "    example_input = torch.randn(1, 1, 28, 28)\n",
    "    \n",
    "    # Basic export\n",
    "    success = onnx_exporter.export_onnx(example_input, \"model_basic.onnx\")\n",
    "    \n",
    "    if success:\n",
    "        # Benchmark performance\n",
    "        benchmark_results = onnx_exporter.benchmark_onnx_vs_pytorch(\"model_basic.onnx\", example_input)\n",
    "        \n",
    "        # Export with dynamic shapes\n",
    "        dynamic_axes = {\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        }\n",
    "        onnx_exporter.export_onnx(example_input, \"model_dynamic.onnx\", dynamic_axes=dynamic_axes)\n",
    "else:\n",
    "    print(\"Skipping ONNX export - ONNX not available\")\n",
    "```\n",
    "\n",
    "## 5. Cross-Platform Deployment Testing\n",
    "\n",
    "```python\n",
    "class CrossPlatformTester:\n",
    "    \"\"\"Test exported models across different platforms and configurations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.test_results = {}\n",
    "    \n",
    "    def test_model_formats(self, original_model, test_input):\n",
    "        \"\"\"Test all exported model formats\"\"\"\n",
    "        print(\"=== Cross-Platform Model Testing ===\")\n",
    "        \n",
    "        results = {\n",
    "            'pytorch': self._test_pytorch_model(original_model, test_input),\n",
    "            'torchscript_traced': self._test_torchscript_model(\"model_traced.pt\", test_input),\n",
    "            'torchscript_scripted': self._test_torchscript_model(\"model_scripted.pt\", test_input),\n",
    "        }\n",
    "        \n",
    "        if ONNX_AVAILABLE:\n",
    "            results['onnx'] = self._test_onnx_model(\"model_basic.onnx\", test_input)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _test_pytorch_model(self, model, test_input):\n",
    "        \"\"\"Test original PyTorch model\"\"\"\n",
    "        try:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                start_time = time.time()\n",
    "                output = model(test_input)\n",
    "                inference_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'inference_time': inference_time,\n",
    "                'output_shape': output.shape,\n",
    "                'model_size_mb': sum(p.numel() * 4 for p in model.parameters()) / 1024 / 1024\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'success': False, 'error': str(e)}\n",
    "    \n",
    "    def _test_torchscript_model(self, model_path, test_input):\n",
    "        \"\"\"Test TorchScript model\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(model_path):\n",
    "                return {'success': False, 'error': 'Model file not found'}\n",
    "            \n",
    "            model = torch.jit.load(model_path)\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                start_time = time.time()\n",
    "                output = model(test_input)\n",
    "                inference_time = time.time() - start_time\n",
    "            \n",
    "            file_size = os.path.getsize(model_path) / 1024 / 1024  # MB\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'inference_time': inference_time,\n",
    "                'output_shape': output.shape,\n",
    "                'file_size_mb': file_size\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'success': False, 'error': str(e)}\n",
    "    \n",
    "    def _test_onnx_model(self, model_path, test_input):\n",
    "        \"\"\"Test ONNX model\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(model_path):\n",
    "                return {'success': False, 'error': 'Model file not found'}\n",
    "            \n",
    "            ort_session = ort.InferenceSession(model_path)\n",
    "            ort_inputs = {ort_session.get_inputs()[0].name: test_input.numpy()}\n",
    "            \n",
    "            start_time = time.time()\n",
    "            ort_outputs = ort_session.run(None, ort_inputs)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            file_size = os.path.getsize(model_path) / 1024 / 1024  # MB\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'inference_time': inference_time,\n",
    "                'output_shape': ort_outputs[0].shape,\n",
    "                'file_size_mb': file_size\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'success': False, 'error': str(e)}\n",
    "    \n",
    "    def test_batch_sizes(self, model_paths, batch_sizes=[1, 4, 16, 32, 64]):\n",
    "        \"\"\"Test different batch sizes for exported models\"\"\"\n",
    "        print(\"\\n=== Batch Size Performance Testing ===\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            test_input = torch.randn(batch_size, 1, 28, 28)\n",
    "            batch_results = {}\n",
    "            \n",
    "            # Test TorchScript\n",
    "            if os.path.exists(\"model_traced.pt\"):\n",
    "                try:\n",
    "                    model = torch.jit.load(\"model_traced.pt\")\n",
    "                    with torch.no_grad():\n",
    "                        start_time = time.time()\n",
    "                        _ = model(test_input)\n",
    "                        batch_results['torchscript'] = time.time() - start_time\n",
    "                except:\n",
    "                    batch_results['torchscript'] = None\n",
    "            \n",
    "            # Test ONNX\n",
    "            if ONNX_AVAILABLE and os.path.exists(\"model_dynamic.onnx\"):\n",
    "                try:\n",
    "                    ort_session = ort.InferenceSession(\"model_dynamic.onnx\")\n",
    "                    ort_inputs = {ort_session.get_inputs()[0].name: test_input.numpy()}\n",
    "                    start_time = time.time()\n",
    "                    _ = ort_session.run(None, ort_inputs)\n",
    "                    batch_results['onnx'] = time.time() - start_time\n",
    "                except:\n",
    "                    batch_results['onnx'] = None\n",
    "            \n",
    "            results[batch_size] = batch_results\n",
    "            print(f\"Batch size {batch_size:2d}: TorchScript={batch_results.get('torchscript', 'N/A'):.4f}s, \"\n",
    "                  f\"ONNX={batch_results.get('onnx', 'N/A'):.4f}s\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_deployment_report(self, test_results):\n",
    "        \"\"\"Generate comprehensive deployment report\"\"\"\n",
    "        report = {\n",
    "            'summary': {},\n",
    "            'recommendations': [],\n",
    "            'compatibility': {},\n",
    "            'performance': test_results\n",
    "        }\n",
    "        \n",
    "        # Analyze results\n",
    "        successful_formats = [fmt for fmt, result in test_results.items() if result.get('success', False)]\n",
    "        \n",
    "        if successful_formats:\n",
    "            # Find fastest format\n",
    "            fastest_format = min(successful_formats, \n",
    "                                key=lambda x: test_results[x].get('inference_time', float('inf')))\n",
    "            \n",
    "            # Find smallest format\n",
    "            size_key = 'file_size_mb' if 'file_size_mb' in test_results[fastest_format] else 'model_size_mb'\n",
    "            smallest_format = min(successful_formats,\n",
    "                                 key=lambda x: test_results[x].get(size_key, float('inf')))\n",
    "            \n",
    "            report['summary'] = {\n",
    "                'successful_formats': successful_formats,\n",
    "                'fastest_format': fastest_format,\n",
    "                'smallest_format': smallest_format,\n",
    "                'fastest_time': test_results[fastest_format].get('inference_time', 0),\n",
    "                'smallest_size': test_results[smallest_format].get(size_key, 0)\n",
    "            }\n",
    "            \n",
    "            # Recommendations\n",
    "            if 'onnx' in successful_formats:\n",
    "                report['recommendations'].append(\"ONNX: Best for cross-platform deployment\")\n",
    "            if 'torchscript_traced' in successful_formats:\n",
    "                report['recommendations'].append(\"TorchScript Traced: Best for PyTorch ecosystem\")\n",
    "            if fastest_format == 'onnx':\n",
    "                report['recommendations'].append(\"ONNX provides fastest inference\")\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Run cross-platform testing\n",
    "tester = CrossPlatformTester()\n",
    "test_input = torch.randn(1, 1, 28, 28)\n",
    "cross_platform_results = tester.test_model_formats(model, test_input)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== Cross-Platform Test Results ===\")\n",
    "for format_name, result in cross_platform_results.items():\n",
    "    if result['success']:\n",
    "        size_key = 'file_size_mb' if 'file_size_mb' in result else 'model_size_mb'\n",
    "        print(f\"{format_name:20}: ✓ {result['inference_time']:.4f}s, {result.get(size_key, 0):.2f}MB\")\n",
    "    else:\n",
    "        print(f\"{format_name:20}: ✗ {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "# Test batch performance\n",
    "batch_results = tester.test_batch_sizes({})\n",
    "\n",
    "# Generate deployment report\n",
    "deployment_report = tester.generate_deployment_report(cross_platform_results)\n",
    "print(f\"\\n=== Deployment Recommendations ===\")\n",
    "for rec in deployment_report['recommendations']:\n",
    "    print(f\"• {rec}\")\n",
    "```\n",
    "\n",
    "## 6. Production Deployment Pipeline\n",
    "\n",
    "```python\n",
    "class ProductionDeploymentPipeline:\n",
    "    \"\"\"Complete pipeline for production model deployment\"\"\"\n",
    "    \n",
    "    def __init__(self, model, model_name=\"mnist_classifier\"):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.deployment_artifacts = {}\n",
    "        \n",
    "    def prepare_for_deployment(self, example_input, target_platforms=['torchscript', 'onnx']):\n",
    "        \"\"\"Prepare all deployment artifacts\"\"\"\n",
    "        print(f\"=== Preparing {self.model_name} for Production Deployment ===\")\n",
    "        \n",
    "        artifacts = {}\n",
    "        \n",
    "        # Create deployment directory\n",
    "        deploy_dir = f\"deployment_{self.model_name}\"\n",
    "        os.makedirs(deploy_dir, exist_ok=True)\n",
    "        \n",
    "        # Export to different formats\n",
    "        if 'torchscript' in target_platforms:\n",
    "            artifacts.update(self._export_torchscript(example_input, deploy_dir))\n",
    "        \n",
    "        if 'onnx' in target_platforms and ONNX_AVAILABLE:\n",
    "            artifacts.update(self._export_onnx(example_input, deploy_dir))\n",
    "        \n",
    "        # Generate metadata and documentation\n",
    "        artifacts['metadata'] = self._generate_metadata(example_input, deploy_dir)\n",
    "        artifacts['readme'] = self._generate_readme(deploy_dir)\n",
    "        \n",
    "        self.deployment_artifacts = artifacts\n",
    "        return artifacts\n",
    "    \n",
    "    def _export_torchscript(self, example_input, deploy_dir):\n",
    "        \"\"\"Export TorchScript models\"\"\"\n",
    "        artifacts = {}\n",
    "        \n",
    "        # Traced model\n",
    "        try:\n",
    "            traced_model = torch.jit.trace(self.model, example_input)\n",
    "            traced_path = os.path.join(deploy_dir, f\"{self.model_name}_traced.pt\")\n",
    "            torch.jit.save(traced_model, traced_path)\n",
    "            artifacts['torchscript_traced'] = traced_path\n",
    "            print(f\"✓ TorchScript traced model saved to {traced_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ TorchScript traced export failed: {e}\")\n",
    "        \n",
    "        # Optimized model\n",
    "        try:\n",
    "            if 'torchscript_traced' in artifacts:\n",
    "                optimized_model = torch.jit.optimize_for_inference(traced_model)\n",
    "                optimized_path = os.path.join(deploy_dir, f\"{self.model_name}_optimized.pt\")\n",
    "                torch.jit.save(optimized_model, optimized_path)\n",
    "                artifacts['torchscript_optimized'] = optimized_path\n",
    "                print(f\"✓ Optimized TorchScript model saved to {optimized_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ TorchScript optimization failed: {e}\")\n",
    "        \n",
    "        return artifacts\n",
    "    \n",
    "    def _export_onnx(self, example_input, deploy_dir):\n",
    "        \"\"\"Export ONNX models\"\"\"\n",
    "        artifacts = {}\n",
    "        \n",
    "        # Standard ONNX export\n",
    "        try:\n",
    "            onnx_path = os.path.join(deploy_dir, f\"{self.model_name}.onnx\")\n",
    "            torch.onnx.export(\n",
    "                self.model, example_input, onnx_path,\n",
    "                export_params=True, opset_version=11, do_constant_folding=True,\n",
    "                input_names=['input'], output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            artifacts['onnx'] = onnx_path\n",
    "            print(f\"✓ ONNX model saved to {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ ONNX export failed: {e}\")\n",
    "        \n",
    "        return artifacts\n",
    "    \n",
    "    def _generate_metadata(self, example_input, deploy_dir):\n",
    "        \"\"\"Generate model metadata\"\"\"\n",
    "        metadata = {\n",
    "            'model_name': self.model_name,\n",
    "            'input_shape': list(example_input.shape),\n",
    "            'input_dtype': str(example_input.dtype),\n",
    "            'num_classes': self.model.hparams.num_classes,\n",
    "            'pytorch_version': torch.__version__,\n",
    "            'export_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'model_parameters': sum(p.numel() for p in self.model.parameters()),\n",
    "            'model_size_mb': sum(p.numel() * 4 for p in self.model.parameters()) / 1024 / 1024,\n",
    "            'preprocessing': {\n",
    "                'normalization': {\n",
    "                    'mean': [0.1307],\n",
    "                    'std': [0.3081]\n",
    "                },\n",
    "                'input_range': [0, 1]\n",
    "            },\n",
    "            'postprocessing': {\n",
    "                'output_type': 'logits',\n",
    "                'apply_softmax': True,\n",
    "                'class_names': [f'class_{i}' for i in range(10)]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(deploy_dir, 'model_metadata.json')\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"✓ Model metadata saved to {metadata_path}\")\n",
    "        return metadata_path\n",
    "    \n",
    "    def _generate_readme(self, deploy_dir):\n",
    "        \"\"\"Generate deployment README\"\"\"\n",
    "        readme_content = f\"\"\"# {self.model_name.upper()} - Production Deployment\n",
    "```\n",
    "\n",
    "## Model Information\n",
    "- **Model Name**: {self.model_name}\n",
    "- **Task**: MNIST Digit Classification\n",
    "- **Input Shape**: (batch_size, 1, 28, 28)\n",
    "- **Output Shape**: (batch_size, 10)\n",
    "- **Parameters**: {sum(p.numel() for p in self.model.parameters()):,}\n",
    "\n",
    "## Available Formats\n",
    "- **TorchScript**: Optimized for PyTorch ecosystem\n",
    "- **ONNX**: Cross-platform deployment\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "### TorchScript\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model = torch.jit.load('{self.model_name}_traced.pt')\n",
    "model.eval()\n",
    "\n",
    "# Prepare input (1x1x28x28 tensor)\n",
    "input_tensor = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    predictions = torch.softmax(output, dim=1)\n",
    "```\n",
    "\n",
    "### ONNX\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load model\n",
    "session = ort.InferenceSession('{self.model_name}.onnx')\n",
    "\n",
    "# Prepare input\n",
    "input_data = np.random.randn(1, 1, 28, 28).astype(np.float32)\n",
    "\n",
    "# Inference\n",
    "outputs = session.run(None, {{'input': input_data}})\n",
    "predictions = outputs[0]\n",
    "```\n",
    "\n",
    "## Preprocessing\n",
    "1. Normalize input images: `(x - 0.1307) / 0.3081`\n",
    "2. Ensure input shape is (batch_size, 1, 28, 28)\n",
    "3. Input should be float32 tensors in range [0, 1]\n",
    "\n",
    "## Postprocessing\n",
    "1. Apply softmax to get probabilities\n",
    "2. Use argmax to get predicted class\n",
    "3. Classes are numbered 0-9 representing digits\n",
    "\n",
    "## Performance Notes\n",
    "- TorchScript: Optimized for CPU inference\n",
    "- ONNX: Better cross-platform compatibility\n",
    "- Batch processing recommended for throughput\n",
    "\n",
    "## Files\n",
    "- `{self.model_name}_traced.pt`: TorchScript traced model\n",
    "- `{self.model_name}_optimized.pt`: Optimized TorchScript model  \n",
    "- `{self.model_name}.onnx`: ONNX model\n",
    "- `model_metadata.json`: Model configuration and metadata\n",
    "\"\"\"\n",
    "\n",
    "```python        \n",
    "        readme_path = os.path.join(deploy_dir, 'README.md')\n",
    "        with open(readme_path, 'w') as f:\n",
    "            f.write(readme_content)\n",
    "        \n",
    "        print(f\"✓ README generated at {readme_path}\")\n",
    "        return readme_path\n",
    "    \n",
    "    def validate_deployment(self, num_test_samples=100):\n",
    "        \"\"\"Validate all deployment artifacts\"\"\"\n",
    "        print(f\"\\n=== Validating Deployment Artifacts ===\")\n",
    "        \n",
    "        validation_results = {}\n",
    "        test_input = torch.randn(num_test_samples, 1, 28, 28)\n",
    "        \n",
    "        # Get reference output from original model\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            reference_output = self.model(test_input)\n",
    "        \n",
    "        # Validate TorchScript models\n",
    "        for artifact_name, artifact_path in self.deployment_artifacts.items():\n",
    "            if artifact_name.startswith('torchscript') and artifact_path.endswith('.pt'):\n",
    "                validation_results[artifact_name] = self._validate_torchscript(\n",
    "                    artifact_path, test_input, reference_output\n",
    "                )\n",
    "        \n",
    "        # Validate ONNX models\n",
    "        if 'onnx' in self.deployment_artifacts:\n",
    "            validation_results['onnx'] = self._validate_onnx(\n",
    "                self.deployment_artifacts['onnx'], test_input, reference_output\n",
    "            )\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    def _validate_torchscript(self, model_path, test_input, reference_output):\n",
    "        \"\"\"Validate TorchScript model\"\"\"\n",
    "        try:\n",
    "            model = torch.jit.load(model_path)\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(test_input)\n",
    "            \n",
    "            # Check output similarity\n",
    "            max_diff = torch.max(torch.abs(output - reference_output)).item()\n",
    "            avg_diff = torch.mean(torch.abs(output - reference_output)).item()\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'max_difference': max_diff,\n",
    "                'avg_difference': avg_diff,\n",
    "                'outputs_match': max_diff < 1e-5\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'success': False, 'error': str(e)}\n",
    "    \n",
    "    def _validate_onnx(self, model_path, test_input, reference_output):\n",
    "        \"\"\"Validate ONNX model\"\"\"\n",
    "        try:\n",
    "            session = ort.InferenceSession(model_path)\n",
    "            ort_inputs = {session.get_inputs()[0].name: test_input.numpy()}\n",
    "            ort_outputs = session.run(None, ort_inputs)\n",
    "            \n",
    "            onnx_output = torch.from_numpy(ort_outputs[0])\n",
    "            \n",
    "            # Check output similarity\n",
    "            max_diff = torch.max(torch.abs(onnx_output - reference_output)).item()\n",
    "            avg_diff = torch.mean(torch.abs(onnx_output - reference_output)).item()\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'max_difference': max_diff,\n",
    "                'avg_difference': avg_diff,\n",
    "                'outputs_match': max_diff < 1e-4\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'success': False, 'error': str(e)}\n",
    "\n",
    "# Create deployment pipeline\n",
    "pipeline = ProductionDeploymentPipeline(model, \"mnist_classifier_v1\")\n",
    "\n",
    "# Prepare deployment artifacts\n",
    "example_input = torch.randn(1, 1, 28, 28)\n",
    "deployment_artifacts = pipeline.prepare_for_deployment(example_input, ['torchscript', 'onnx'])\n",
    "\n",
    "# Validate deployment\n",
    "validation_results = pipeline.validate_deployment(num_test_samples=50)\n",
    "\n",
    "print(f\"\\n=== Deployment Validation Results ===\")\n",
    "for artifact_name, result in validation_results.items():\n",
    "    if result['success']:\n",
    "        status = \"✓ PASS\" if result['outputs_match'] else \"⚠ DIFF\"\n",
    "        print(f\"{artifact_name:20}: {status} (max_diff: {result['max_difference']:.2e})\")\n",
    "    else:\n",
    "        print(f\"{artifact_name:20}: ✗ FAIL ({result.get('error', 'Unknown error')})\")\n",
    "```\n",
    "\n",
    "## 7. Performance Benchmarking Suite\n",
    "\n",
    "```python\n",
    "class ModelBenchmarkSuite:\n",
    "    \"\"\"Comprehensive benchmarking for exported models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    def run_comprehensive_benchmark(self, model_paths, test_configs):\n",
    "        \"\"\"Run comprehensive benchmarks across all models and configurations\"\"\"\n",
    "        print(\"=== Comprehensive Model Benchmarking ===\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for config_name, config in test_configs.items():\n",
    "            print(f\"\\nTesting configuration: {config_name}\")\n",
    "            config_results = {}\n",
    "            \n",
    "            batch_size = config.get('batch_size', 1)\n",
    "            num_runs = config.get('num_runs', 100)\n",
    "            warmup_runs = config.get('warmup_runs', 10)\n",
    "            \n",
    "            test_input = torch.randn(batch_size, 1, 28, 28)\n",
    "            \n",
    "            # Benchmark each model format\n",
    "            for model_name, model_path in model_paths.items():\n",
    "                if os.path.exists(model_path):\n",
    "                    config_results[model_name] = self._benchmark_single_model(\n",
    "                        model_path, test_input, num_runs, warmup_runs, model_name\n",
    "                    )\n",
    "                else:\n",
    "                    config_results[model_name] = {'error': 'Model file not found'}\n",
    "            \n",
    "            results[config_name] = config_results\n",
    "        \n",
    "        self.results = results\n",
    "        return results\n",
    "    \n",
    "    def _benchmark_single_model(self, model_path, test_input, num_runs, warmup_runs, model_type):\n",
    "        \"\"\"Benchmark a single model\"\"\"\n",
    "        try:\n",
    "            if model_type.startswith('torchscript'):\n",
    "                return self._benchmark_torchscript(model_path, test_input, num_runs, warmup_runs)\n",
    "            elif model_type == 'onnx' and ONNX_AVAILABLE:\n",
    "                return self._benchmark_onnx(model_path, test_input, num_runs, warmup_runs)\n",
    "            else:\n",
    "                return {'error': f'Unsupported model type: {model_type}'}\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def _benchmark_torchscript(self, model_path, test_input, num_runs, warmup_runs):\n",
    "        \"\"\"Benchmark TorchScript model\"\"\"\n",
    "        model = torch.jit.load(model_path)\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Warmup\n",
    "            for _ in range(warmup_runs):\n",
    "                _ = model(test_input)\n",
    "            \n",
    "            # Benchmark\n",
    "            times = []\n",
    "            for _ in range(num_runs):\n",
    "                start_time = time.time()\n",
    "                output = model(test_input)\n",
    "                times.append(time.time() - start_time)\n",
    "        \n",
    "        return {\n",
    "            'mean_time': np.mean(times),\n",
    "            'std_time': np.std(times),\n",
    "            'min_time': np.min(times),\n",
    "            'max_time': np.max(times),\n",
    "            'throughput': test_input.shape[0] / np.mean(times),  # samples/second\n",
    "            'file_size_mb': os.path.getsize(model_path) / 1024 / 1024\n",
    "        }\n",
    "    \n",
    "    def _benchmark_onnx(self, model_path, test_input, num_runs, warmup_runs):\n",
    "        \"\"\"Benchmark ONNX model\"\"\"\n",
    "        session = ort.InferenceSession(model_path)\n",
    "        ort_inputs = {session.get_inputs()[0].name: test_input.numpy()}\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(warmup_runs):\n",
    "            _ = session.run(None, ort_inputs)\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            outputs = session.run(None, ort_inputs)\n",
    "            times.append(time.time() - start_time)\n",
    "        \n",
    "        return {\n",
    "            'mean_time': np.mean(times),\n",
    "            'std_time': np.std(times),\n",
    "            'min_time': np.min(times),\n",
    "            'max_time': np.max(times),\n",
    "            'throughput': test_input.shape[0] / np.mean(times),\n",
    "            'file_size_mb': os.path.getsize(model_path) / 1024 / 1024\n",
    "        }\n",
    "    \n",
    "    def generate_benchmark_report(self, save_path=\"benchmark_report.json\"):\n",
    "        \"\"\"Generate comprehensive benchmark report\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No benchmark results available\")\n",
    "            return\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary = {\n",
    "            'fastest_model': {},\n",
    "            'most_efficient': {},\n",
    "            'smallest_model': {},\n",
    "            'detailed_results': self.results\n",
    "        }\n",
    "        \n",
    "        # Find best performing models across configurations\n",
    "        for config_name, config_results in self.results.items():\n",
    "            fastest_time = float('inf')\n",
    "            fastest_model = None\n",
    "            \n",
    "            smallest_size = float('inf')\n",
    "            smallest_model = None\n",
    "            \n",
    "            highest_throughput = 0\n",
    "            most_efficient = None\n",
    "            \n",
    "            for model_name, result in config_results.items():\n",
    "                if 'error' not in result:\n",
    "                    # Check fastest\n",
    "                    if result['mean_time'] < fastest_time:\n",
    "                        fastest_time = result['mean_time']\n",
    "                        fastest_model = model_name\n",
    "                    \n",
    "                    # Check smallest\n",
    "                    if result['file_size_mb'] < smallest_size:\n",
    "                        smallest_size = result['file_size_mb']\n",
    "                        smallest_model = model_name\n",
    "                    \n",
    "                    # Check most efficient (highest throughput)\n",
    "                    if result['throughput'] > highest_throughput:\n",
    "                        highest_throughput = result['throughput']\n",
    "                        most_efficient = model_name\n",
    "            \n",
    "            summary['fastest_model'][config_name] = {\n",
    "                'model': fastest_model,\n",
    "                'time': fastest_time\n",
    "            }\n",
    "            summary['smallest_model'][config_name] = {\n",
    "                'model': smallest_model,\n",
    "                'size_mb': smallest_size\n",
    "            }\n",
    "            summary['most_efficient'][config_name] = {\n",
    "                'model': most_efficient,\n",
    "                'throughput': highest_throughput\n",
    "            }\n",
    "        \n",
    "        # Save report\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"✓ Benchmark report saved to {save_path}\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n=== Benchmark Summary ===\")\n",
    "        for config_name in self.results.keys():\n",
    "            print(f\"\\nConfiguration: {config_name}\")\n",
    "            if config_name in summary['fastest_model']:\n",
    "                fastest = summary['fastest_model'][config_name]\n",
    "                print(f\"  Fastest: {fastest['model']} ({fastest['time']:.4f}s)\")\n",
    "            if config_name in summary['most_efficient']:\n",
    "                efficient = summary['most_efficient'][config_name]\n",
    "                print(f\"  Most Efficient: {efficient['model']} ({efficient['throughput']:.1f} samples/s)\")\n",
    "            if config_name in summary['smallest_model']:\n",
    "                smallest = summary['smallest_model'][config_name]\n",
    "                print(f\"  Smallest: {smallest['model']} ({smallest['size_mb']:.2f} MB)\")\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Run comprehensive benchmarks\n",
    "benchmark_suite = ModelBenchmarkSuite()\n",
    "\n",
    "# Define test configurations\n",
    "test_configs = {\n",
    "    'single_inference': {'batch_size': 1, 'num_runs': 100, 'warmup_runs': 10},\n",
    "    'small_batch': {'batch_size': 4, 'num_runs': 100, 'warmup_runs': 10},\n",
    "    'medium_batch': {'batch_size': 16, 'num_runs': 50, 'warmup_runs': 5},\n",
    "    'large_batch': {'batch_size': 64, 'num_runs': 20, 'warmup_runs': 5}\n",
    "}\n",
    "\n",
    "# Model paths (update based on actual deployment artifacts)\n",
    "model_paths = {\n",
    "    'torchscript_traced': f'deployment_mnist_classifier_v1/mnist_classifier_v1_traced.pt',\n",
    "    'torchscript_optimized': f'deployment_mnist_classifier_v1/mnist_classifier_v1_optimized.pt',\n",
    "    'onnx': f'deployment_mnist_classifier_v1/mnist_classifier_v1.onnx'\n",
    "}\n",
    "\n",
    "# Filter existing models\n",
    "existing_models = {name: path for name, path in model_paths.items() if os.path.exists(path)}\n",
    "\n",
    "if existing_models:\n",
    "    benchmark_results = benchmark_suite.run_comprehensive_benchmark(existing_models, test_configs)\n",
    "    benchmark_summary = benchmark_suite.generate_benchmark_report()\n",
    "else:\n",
    "    print(\"No deployment models found for benchmarking\")\n",
    "```\n",
    "\n",
    "# Summary\n",
    "\n",
    "This notebook provided comprehensive model export strategies for production deployment using ONNX and TorchScript. Key implementations and concepts covered:\n",
    "\n",
    "## Export Format Implementations\n",
    "- **TorchScript Tracing**: JIT compilation for production inference\n",
    "- **TorchScript Scripting**: Full model serialization with control flow\n",
    "- **ONNX Export**: Cross-platform deployment format\n",
    "- **Model Optimization**: Performance tuning for each export format\n",
    "\n",
    "## Production Deployment Pipeline  \n",
    "- **Multi-Format Export**: Automated export to multiple formats\n",
    "- **Validation Testing**: Ensuring output consistency across formats\n",
    "- **Metadata Generation**: Complete model documentation and configuration\n",
    "- **Deployment Artifacts**: Production-ready model packages\n",
    "\n",
    "## Performance Analysis and Benchmarking\n",
    "- **Cross-Platform Testing**: Compatibility verification across formats\n",
    "- **Batch Size Optimization**: Performance scaling analysis\n",
    "- **Memory Usage Monitoring**: Efficient resource utilization\n",
    "- **Throughput Benchmarking**: Real-world performance metrics\n",
    "\n",
    "## Key Benefits Achieved\n",
    "- **Production Readiness**: Complete deployment pipeline with validation\n",
    "- **Cross-Platform Compatibility**: Models deployable across different environments\n",
    "- **Performance Optimization**: Format-specific optimizations for speed\n",
    "- **Documentation**: Comprehensive usage guides and metadata\n",
    "\n",
    "## Export Format Comparison\n",
    "- **TorchScript**: Best for PyTorch ecosystem, preserves Python semantics\n",
    "- **ONNX**: Superior cross-platform support, broader runtime options\n",
    "- **Optimization Trade-offs**: Speed vs. compatibility considerations\n",
    "- **File Size**: Efficient model serialization and compression\n",
    "\n",
    "## Next Steps\n",
    "- Integrate with cloud deployment platforms (AWS, GCP, Azure)\n",
    "- Add support for quantization and pruning\n",
    "- Implement model serving with FastAPI/TorchServe\n",
    "- Develop continuous integration for model validation\n",
    "\n",
    "The export framework provides a robust foundation for deploying PyTorch Lightning models in production environments with optimal performance and reliability."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
