{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1426b4a6",
   "metadata": {},
   "source": [
    "# Checkpointing and Early Stopping\n",
    "\n",
    "**File Location:** `notebooks/03_callbacks_and_checkpointing/06_checkpoint_earlystop.ipynb`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook covers PyTorch Lightning's checkpointing and early stopping mechanisms. Learn to save model states, resume training, implement intelligent stopping criteria, and manage model versioning for robust ML workflows.\n",
    "\n",
    "## Model Checkpointing Fundamentals\n",
    "\n",
    "### Basic Checkpointing Setup\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Simple model for checkpointing demo\n",
    "class CheckpointModel(pl.LightningModule):\n",
    "    def __init__(self, input_size=20, hidden_size=64, num_classes=5, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Metrics for monitoring\n",
    "        from torchmetrics import Accuracy\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        # Add scheduler for more interesting training dynamics\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "                'frequency': 1,\n",
    "                'interval': 'epoch'\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create synthetic data\n",
    "def create_training_data(num_samples=5000, input_size=20, num_classes=5):\n",
    "    torch.manual_seed(42)\n",
    "    X = torch.randn(num_samples, input_size)\n",
    "    # Create targets with some noise to make training more realistic\n",
    "    weights = torch.randn(input_size)\n",
    "    logits = X @ weights\n",
    "    y = torch.div(logits - logits.min(), (logits.max() - logits.min()) / (num_classes - 1), rounding_mode='floor').long()\n",
    "    y = torch.clamp(y, 0, num_classes - 1)\n",
    "    return X, y\n",
    "\n",
    "X, y = create_training_data()\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"✓ Basic setup completed\")\n",
    "```\n",
    "\n",
    "### Standard Checkpointing Configuration\n",
    "\n",
    "```python\n",
    "# Create temporary directory for checkpoints\n",
    "checkpoint_dir = Path(tempfile.mkdtemp(prefix=\"lightning_checkpoints_\"))\n",
    "print(f\"Checkpoint directory: {checkpoint_dir}\")\n",
    "\n",
    "# Basic checkpoint callback - saves best model\n",
    "basic_checkpoint = ModelCheckpoint(\n",
    "    dirpath=checkpoint_dir / \"basic\",\n",
    "    filename=\"best-model-{epoch:02d}-{val_loss:.2f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,  # Keep only the best model\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Train with basic checkpointing\n",
    "model = CheckpointModel()\n",
    "logger = TensorBoardLogger(checkpoint_dir / \"logs\", name=\"basic_checkpoint\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[basic_checkpoint],\n",
    "    logger=logger,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Training with basic checkpointing...\")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# List saved checkpoints\n",
    "checkpoint_files = list((checkpoint_dir / \"basic\").glob(\"*.ckpt\"))\n",
    "print(f\"✓ Training completed. Saved checkpoints: {len(checkpoint_files)}\")\n",
    "for ckpt in checkpoint_files:\n",
    "    print(f\"  {ckpt.name}\")\n",
    "```\n",
    "\n",
    "### Advanced Checkpointing Strategies\n",
    "\n",
    "```python\n",
    "# Multiple checkpoint callbacks for different purposes\n",
    "callbacks = [\n",
    "    # Best model based on validation loss\n",
    "    ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir / \"best_loss\",\n",
    "        filename=\"best-loss-{epoch:02d}-{val_loss:.3f}\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        verbose=True\n",
    "    ),\n",
    "    \n",
    "    # Best model based on validation accuracy\n",
    "    ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir / \"best_acc\",\n",
    "        filename=\"best-acc-{epoch:02d}-{val_acc:.3f}\",\n",
    "        monitor=\"val_acc\", \n",
    "        mode=\"max\",\n",
    "        save_top_k=1,\n",
    "        verbose=True\n",
    "    ),\n",
    "    \n",
    "    # Keep top 3 models based on validation loss\n",
    "    ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir / \"top3\",\n",
    "        filename=\"top3-{epoch:02d}-{val_loss:.3f}\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\", \n",
    "        save_top_k=3,\n",
    "        verbose=False\n",
    "    ),\n",
    "    \n",
    "    # Regular epoch checkpoints (every 5 epochs)\n",
    "    ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir / \"periodic\",\n",
    "        filename=\"epoch-{epoch:02d}\",\n",
    "        every_n_epochs=5,\n",
    "        save_top_k=-1,  # Keep all\n",
    "        verbose=False\n",
    "    ),\n",
    "    \n",
    "    # Last checkpoint (always keep the most recent)\n",
    "    ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir / \"last\",\n",
    "        filename=\"last-model\",\n",
    "        save_last=True,\n",
    "        verbose=False\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train with multiple checkpointing strategies\n",
    "model = CheckpointModel(learning_rate=2e-3)  # Slightly different config\n",
    "logger = TensorBoardLogger(checkpoint_dir / \"logs\", name=\"advanced_checkpoint\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    enable_progress_bar=False  # Reduce output for demo\n",
    ")\n",
    "\n",
    "print(\"Training with advanced checkpointing strategies...\")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Analyze saved checkpoints\n",
    "print(\"\\n=== Checkpoint Analysis ===\")\n",
    "for callback in callbacks:\n",
    "    if hasattr(callback, 'dirpath'):\n",
    "        checkpoint_files = list(Path(callback.dirpath).glob(\"*.ckpt\"))\n",
    "        print(f\"{Path(callback.dirpath).name}: {len(checkpoint_files)} checkpoints\")\n",
    "        \n",
    "        for ckpt in sorted(checkpoint_files):\n",
    "            # Load checkpoint to inspect metadata\n",
    "            checkpoint = torch.load(ckpt, map_location='cpu')\n",
    "            epoch = checkpoint.get('epoch', 'unknown')\n",
    "            val_loss = checkpoint.get('val_loss', 'unknown')\n",
    "            print(f\"  {ckpt.name} (epoch: {epoch}, val_loss: {val_loss})\")\n",
    "\n",
    "print(\"✓ Advanced checkpointing completed\")\n",
    "```\n",
    "\n",
    "## Early Stopping Implementation\n",
    "\n",
    "### Basic Early Stopping\n",
    "\n",
    "```python\n",
    "# Simple early stopping callback\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.001,  # Minimum change to qualify as an improvement\n",
    "    patience=5,       # Number of epochs to wait for improvement\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# Model that might benefit from early stopping\n",
    "model = CheckpointModel(learning_rate=5e-4)  # Lower LR for more stable training\n",
    "\n",
    "# Combined checkpointing and early stopping\n",
    "combined_callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir / \"early_stop\",\n",
    "        filename=\"early-stop-{epoch:02d}-{val_loss:.3f}\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        verbose=True\n",
    "    ),\n",
    "    early_stop_callback\n",
    "]\n",
    "\n",
    "logger = TensorBoardLogger(checkpoint_dir / \"logs\", name=\"early_stopping\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # High max epochs, but early stopping will prevent overfitting\n",
    "    callbacks=combined_callbacks,\n",
    "    logger=logger,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Training with early stopping...\")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "print(f\"Training stopped at epoch: {trainer.current_epoch}\")\n",
    "print(f\"Reason: {early_stop_callback.stopped_epoch}\")\n",
    "print(\"✓ Early stopping demo completed\")\n",
    "```\n",
    "\n",
    "### Advanced Early Stopping Strategies\n",
    "\n",
    "```python\n",
    "# Multiple early stopping criteria\n",
    "advanced_callbacks = [\n",
    "    # Stop if validation loss doesn't improve\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0.001,\n",
    "        patience=7,\n",
    "        verbose=True,\n",
    "        mode=\"min\",\n",
    "        check_finite=True  # Stop if metric becomes NaN/Inf\n",
    "    ),\n",
    "    \n",
    "    # Stop if validation accuracy doesn't improve (different patience)\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_acc\",\n",
    "        min_delta=0.005,\n",
    "        patience=10,\n",
    "        verbose=True,\n",
    "        mode=\"max\",\n",
    "        check_finite=True\n",
    "    ),\n",
    "    \n",
    "    # Best model checkpoint\n",
    "    ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir / \"advanced_early_stop\",\n",
    "        filename=\"best-{epoch:02d}-{val_loss:.3f}-{val_acc:.3f}\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        verbose=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Model with potential for overfitting\n",
    "class OverfittingModel(CheckpointModel):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Larger, more complex model prone to overfitting\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.hparams.input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),  # Less dropout = more overfitting potential\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.hparams.num_classes)\n",
    "        )\n",
    "\n",
    "model = OverfittingModel(learning_rate=1e-3)\n",
    "logger = TensorBoardLogger(checkpoint_dir / \"logs\", name=\"advanced_early_stopping\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=advanced_callbacks,\n",
    "    logger=logger,\n",
    "    enable_progress_bar=False\n",
    ")\n",
    "\n",
    "print(\"Training with advanced early stopping...\")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "print(f\"Final training epoch: {trainer.current_epoch}\")\n",
    "for callback in advanced_callbacks:\n",
    "    if isinstance(callback, EarlyStopping):\n",
    "        print(f\"Early stopping ({callback.monitor}): stopped at epoch {callback.stopped_epoch}\")\n",
    "\n",
    "print(\"✓ Advanced early stopping demo completed\")\n",
    "```\n",
    "\n",
    "## Model Loading and Resuming Training\n",
    "\n",
    "### Loading from Checkpoints\n",
    "\n",
    "```python\n",
    "# Function to find the best checkpoint\n",
    "def find_best_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Find the best checkpoint based on validation loss\"\"\"\n",
    "    checkpoint_files = list(checkpoint_dir.glob(\"**/*.ckpt\"))\n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_checkpoint = None\n",
    "    \n",
    "    for ckpt_path in checkpoint_files:\n",
    "        try:\n",
    "            checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "            val_loss = checkpoint.get('val_loss', float('inf'))\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_checkpoint = ckpt_path\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return best_checkpoint\n",
    "\n",
    "# Find and load best checkpoint\n",
    "best_checkpoint_path = find_best_checkpoint(checkpoint_dir)\n",
    "print(f\"Best checkpoint: {best_checkpoint_path}\")\n",
    "\n",
    "if best_checkpoint_path:\n",
    "    # Method 1: Load model from checkpoint\n",
    "    loaded_model = CheckpointModel.load_from_checkpoint(best_checkpoint_path)\n",
    "    print(f\"✓ Loaded model from checkpoint\")\n",
    "    print(f\"Model hyperparameters: {loaded_model.hparams}\")\n",
    "    \n",
    "    # Method 2: Resume training from checkpoint\n",
    "    print(\"\\nResuming training from checkpoint...\")\n",
    "    \n",
    "    # Create new trainer for resumed training\n",
    "    resume_callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=checkpoint_dir / \"resumed\",\n",
    "            filename=\"resumed-{epoch:02d}-{val_loss:.3f}\",\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=3,\n",
    "            verbose=True\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=5,\n",
    "            verbose=True,\n",
    "            mode=\"min\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    resume_trainer = pl.Trainer(\n",
    "        max_epochs=25,  # Continue training for more epochs\n",
    "        callbacks=resume_callbacks,\n",
    "        logger=TensorBoardLogger(checkpoint_dir / \"logs\", name=\"resumed_training\"),\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    # Resume from checkpoint\n",
    "    resume_trainer.fit(\n",
    "        loaded_model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        ckpt_path=best_checkpoint_path\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Resumed training completed\")\n",
    "```\n",
    "\n",
    "### Checkpoint Inspection and Analysis\n",
    "\n",
    "```python\n",
    "def analyze_checkpoint(checkpoint_path):\n",
    "    \"\"\"Analyze checkpoint contents\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    print(f\"=== Checkpoint Analysis: {checkpoint_path.name} ===\")\n",
    "    print(f\"Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"Global step: {checkpoint.get('global_step', 'N/A')}\")\n",
    "    print(f\"Validation loss: {checkpoint.get('val_loss', 'N/A')}\")\n",
    "    print(f\"Validation accuracy: {checkpoint.get('val_acc', 'N/A')}\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    if 'hyper_parameters' in checkpoint:\n",
    "        print(\"Hyperparameters:\")\n",
    "        for key, value in checkpoint['hyper_parameters'].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Model state\n",
    "    state_dict = checkpoint.get('state_dict', {})\n",
    "    print(f\"Model parameters: {len(state_dict)} tensors\")\n",
    "    \n",
    "    # Optimizer state\n",
    "    if 'optimizer_states' in checkpoint:\n",
    "        print(f\"Optimizer states: {len(checkpoint['optimizer_states'])}\")\n",
    "    \n",
    "    # Learning rate schedulers\n",
    "    if 'lr_schedulers' in checkpoint:\n",
    "        print(f\"LR schedulers: {len(checkpoint['lr_schedulers'])}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Analyze several checkpoints\n",
    "checkpoint_files = list(checkpoint_dir.glob(\"**/*.ckpt\"))[:3]  # Analyze first 3\n",
    "for ckpt_path in checkpoint_files:\n",
    "    try:\n",
    "        analyze_checkpoint(ckpt_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {ckpt_path}: {e}\")\n",
    "```\n",
    "\n",
    "## Custom Checkpoint Callbacks\n",
    "\n",
    "### Custom Checkpoint Strategy\n",
    "\n",
    "```python\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class CustomCheckpointCallback(Callback):\n",
    "    \"\"\"Custom checkpoint callback with advanced logic\"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir, save_every_n_epochs=5, save_on_improvement=True):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.save_every_n_epochs = save_every_n_epochs\n",
    "        self.save_on_improvement = save_on_improvement\n",
    "        self.best_val_loss = float('inf')\n",
    "        \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        current_epoch = trainer.current_epoch\n",
    "        \n",
    "        # Get current validation loss\n",
    "        val_loss = trainer.callback_metrics.get('val_loss', float('inf'))\n",
    "        val_acc = trainer.callback_metrics.get('val_acc', 0.0)\n",
    "        \n",
    "        # Save every N epochs\n",
    "        if (current_epoch + 1) % self.save_every_n_epochs == 0:\n",
    "            checkpoint_path = self.save_dir / f\"periodic_epoch_{current_epoch:03d}.ckpt\"\n",
    "            trainer.save_checkpoint(checkpoint_path)\n",
    "            print(f\"Saved periodic checkpoint: {checkpoint_path.name}\")\n",
    "        \n",
    "        # Save on improvement\n",
    "        if self.save_on_improvement and val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            checkpoint_path = self.save_dir / f\"best_model_epoch_{current_epoch:03d}_loss_{val_loss:.4f}.ckpt\"\n",
    "            trainer.save_checkpoint(checkpoint_path)\n",
    "            print(f\"Saved improvement checkpoint: {checkpoint_path.name}\")\n",
    "            \n",
    "            # Also save model metadata\n",
    "            metadata = {\n",
    "                'epoch': current_epoch,\n",
    "                'val_loss': val_loss.item() if isinstance(val_loss, torch.Tensor) else val_loss,\n",
    "                'val_acc': val_acc.item() if isinstance(val_acc, torch.Tensor) else val_acc,\n",
    "                'hyperparameters': dict(pl_module.hparams),\n",
    "                'model_class': pl_module.__class__.__name__\n",
    "            }\n",
    "            \n",
    "            import json\n",
    "            with open(self.save_dir / f\"metadata_epoch_{current_epoch:03d}.json\", 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "\n",
    "class ConditionalEarlyStopping(Callback):\n",
    "    \"\"\"Early stopping with custom conditions\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=10, min_epochs=5, improvement_threshold=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_epochs = min_epochs\n",
    "        self.improvement_threshold = improvement_threshold\n",
    "        self.wait_count = 0\n",
    "        self.best_score = float('inf')\n",
    "        \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        current_epoch = trainer.current_epoch\n",
    "        val_loss = trainer.callback_metrics.get('val_loss', float('inf'))\n",
    "        \n",
    "        # Don't stop before minimum epochs\n",
    "        if current_epoch < self.min_epochs:\n",
    "            return\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_loss < self.best_score - self.improvement_threshold:\n",
    "            self.best_score = val_loss\n",
    "            self.wait_count = 0\n",
    "        else:\n",
    "            self.wait_count += 1\n",
    "        \n",
    "        # Stop if no improvement for too long\n",
    "        if self.wait_count >= self.patience:\n",
    "            print(f\"Early stopping at epoch {current_epoch} (waited {self.wait_count} epochs)\")\n",
    "            trainer.should_stop = True\n",
    "\n",
    "# Test custom callbacks\n",
    "custom_callbacks = [\n",
    "    CustomCheckpointCallback(\n",
    "        save_dir=checkpoint_dir / \"custom\",\n",
    "        save_every_n_epochs=3,\n",
    "        save_on_improvement=True\n",
    "    ),\n",
    "    ConditionalEarlyStopping(\n",
    "        patience=8,\n",
    "        min_epochs=10,\n",
    "        improvement_threshold=0.002\n",
    "    )\n",
    "]\n",
    "\n",
    "model = CheckpointModel(learning_rate=1e-3)\n",
    "logger = TensorBoardLogger(checkpoint_dir / \"logs\", name=\"custom_callbacks\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    callbacks=custom_callbacks,\n",
    "    logger=logger,\n",
    "    enable_progress_bar=False\n",
    ")\n",
    "\n",
    "print(\"Training with custom callbacks...\")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "print(\"✓ Custom callbacks demo completed\")\n",
    "\n",
    "# Check custom checkpoint directory\n",
    "custom_checkpoints = list((checkpoint_dir / \"custom\").glob(\"*\"))\n",
    "print(f\"Custom checkpoints saved: {len([f for f in custom_checkpoints if f.suffix == '.ckpt'])}\")\n",
    "print(f\"Metadata files saved: {len([f for f in custom_checkpoints if f.suffix == '.json'])}\")\n",
    "```\n",
    "\n",
    "## Best Practices and Production Tips\n",
    "\n",
    "### Checkpoint Management Strategy\n",
    "\n",
    "```python\n",
    "def create_production_checkpoint_setup(checkpoint_base_dir, experiment_name):\n",
    "    \"\"\"Create production-ready checkpoint configuration\"\"\"\n",
    "    \n",
    "    checkpoint_dir = Path(checkpoint_base_dir) / experiment_name\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    return [\n",
    "        # Best model for inference\n",
    "        ModelCheckpoint(\n",
    "            dirpath=checkpoint_dir / \"best\",\n",
    "            filename=\"best-model-{epoch:02d}-{val_loss:.4f}\",\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "            verbose=True,\n",
    "            save_weights_only=False,  # Save full model state\n",
    "            save_last=False\n",
    "        ),\n",
    "        \n",
    "        # Top 3 models for ensemble\n",
    "        ModelCheckpoint(\n",
    "            dirpath=checkpoint_dir / \"top3\",\n",
    "            filename=\"top3-{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}\",\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=3,\n",
    "            verbose=False,\n",
    "            save_weights_only=False\n",
    "        ),\n",
    "        \n",
    "        # Periodic backups\n",
    "        ModelCheckpoint(\n",
    "            dirpath=checkpoint_dir / \"periodic\",\n",
    "            filename=\"backup-epoch-{epoch:02d}\",\n",
    "            every_n_epochs=10,\n",
    "            save_top_k=-1,\n",
    "            save_weights_only=True,  # Save space for backups\n",
    "            verbose=False\n",
    "        ),\n",
    "        \n",
    "        # Always save last checkpoint for resuming\n",
    "        ModelCheckpoint(\n",
    "            dirpath=checkpoint_dir / \"recovery\",\n",
    "            save_last=True,\n",
    "            filename=\"last-checkpoint\",\n",
    "            verbose=False\n",
    "        ),\n",
    "        \n",
    "        # Conservative early stopping\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0.0001,\n",
    "            patience=15,\n",
    "            verbose=True,\n",
    "            mode=\"min\",\n",
    "            check_finite=True,\n",
    "            strict=True  # Crash if monitored metric is not found\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# Production example\n",
    "production_callbacks = create_production_checkpoint_setup(\n",
    "    checkpoint_base_dir=checkpoint_dir,\n",
    "    experiment_name=\"production_run_v1\"\n",
    ")\n",
    "\n",
    "model = CheckpointModel(\n",
    "    input_size=20,\n",
    "    hidden_size=128,\n",
    "    num_classes=5,\n",
    "    learning_rate=1e-3\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    checkpoint_dir / \"logs\", \n",
    "    name=\"production\",\n",
    "    version=\"v1\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    callbacks=production_callbacks,\n",
    "    logger=logger,\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=10,\n",
    "    check_val_every_n_epoch=1\n",
    ")\n",
    "\n",
    "print(\"Running production training setup...\")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "print(\"✓ Production training completed\")\n",
    "\n",
    "print(\"\\n=== Production Checkpoint Summary ===\")\n",
    "prod_dir = checkpoint_dir / \"production_run_v1\"\n",
    "for subdir in [\"best\", \"top3\", \"periodic\", \"recovery\"]:\n",
    "    ckpt_files = list((prod_dir / subdir).glob(\"*.ckpt\"))\n",
    "    print(f\"{subdir}: {len(ckpt_files)} checkpoints\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered comprehensive checkpointing and early stopping strategies:\n",
    "\n",
    "1. **Basic Checkpointing**: Save best models based on validation metrics\n",
    "2. **Advanced Strategies**: Multiple checkpoint types (best, top-k, periodic, last)\n",
    "3. **Early Stopping**: Prevent overfitting with intelligent stopping criteria\n",
    "4. **Model Loading**: Resume training and load models from checkpoints\n",
    "5. **Custom Callbacks**: Build domain-specific checkpoint and stopping logic\n",
    "6. **Production Setup**: Robust checkpoint management for real-world deployment\n",
    "\n",
    "Key checkpoint strategies:\n",
    "- **Best Model**: Save the single best performing model for inference\n",
    "- **Top-K Models**: Keep multiple good models for ensembling\n",
    "- **Periodic Backups**: Regular saves to prevent data loss\n",
    "- **Last Checkpoint**: Always keep most recent state for resuming training\n",
    "- **Recovery Points**: Strategic saves at important training milestones\n",
    "\n",
    "Early stopping best practices:\n",
    "- Monitor appropriate metrics (usually validation loss)\n",
    "- Set reasonable patience (5-15 epochs typically)\n",
    "- Use minimum delta to avoid stopping on noise\n",
    "- Consider multiple stopping criteria for robust training\n",
    "- Always combine with checkpointing to save best model\n",
    "\n",
    "Production considerations:\n",
    "- Organize checkpoints by experiment and version\n",
    "- Save metadata alongside checkpoints\n",
    "- Implement checkpoint cleanup to manage disk space\n",
    "- Use descriptive filenames with key metrics\n",
    "- Plan for training resumption and model recovery\n",
    "\n",
    "Next notebook: We'll explore custom callbacks including SWA and EMA."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
