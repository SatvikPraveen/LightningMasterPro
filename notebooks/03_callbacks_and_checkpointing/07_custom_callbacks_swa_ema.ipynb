{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7591300",
   "metadata": {},
   "source": [
    "# Custom Callbacks: SWA and EMA\n",
    "\n",
    "**File Location:** `notebooks/03_callbacks_and_checkpointing/07_custom_callbacks_swa_ema.ipynb`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook explores advanced training techniques through custom callbacks, focusing on Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA). Learn to implement model ensembling techniques that improve generalization without additional inference cost.\n",
    "\n",
    "## Custom Callback Fundamentals\n",
    "\n",
    "### Building Custom Callbacks\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class TrainingMonitorCallback(Callback):\n",
    "    \"\"\"Custom callback to demonstrate callback lifecycle\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_stats = defaultdict(list)\n",
    "        \n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        print(\"ðŸš€ Training started!\")\n",
    "        \n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        print(f\"ðŸ“Š Starting epoch {trainer.current_epoch}\")\n",
    "        \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        # Log gradient norms every 50 batches\n",
    "        if batch_idx % 50 == 0:\n",
    "            total_norm = 0\n",
    "            for p in pl_module.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "            self.training_stats['grad_norm'].append(total_norm)\n",
    "            \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        val_loss = trainer.callback_metrics.get('val_loss', 0)\n",
    "        self.training_stats['val_loss'].append(val_loss.item() if isinstance(val_loss, torch.Tensor) else val_loss)\n",
    "        \n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(\"ðŸ Training completed!\")\n",
    "        print(f\"Final validation loss: {self.training_stats['val_loss'][-1]:.4f}\")\n",
    "        print(f\"Average gradient norm: {np.mean(self.training_stats['grad_norm']):.6f}\")\n",
    "\n",
    "# Simple model for callback demonstrations\n",
    "class CallbackDemoModel(pl.LightningModule):\n",
    "    def __init__(self, input_size=20, hidden_size=128, num_classes=5, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        from torchmetrics import Accuracy\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "# Create synthetic data\n",
    "def create_demo_data(num_samples=3000, input_size=20, num_classes=5):\n",
    "    torch.manual_seed(42)\n",
    "    X = torch.randn(num_samples, input_size)\n",
    "    weights = torch.randn(input_size)\n",
    "    logits = X @ weights\n",
    "    y = torch.div(logits - logits.min(), (logits.max() - logits.min()) / (num_classes - 1), rounding_mode='floor').long()\n",
    "    y = torch.clamp(y, 0, num_classes - 1)\n",
    "    return X, y\n",
    "\n",
    "X, y = create_demo_data()\n",
    "dataset = TensorDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"âœ“ Setup completed\")\n",
    "```\n",
    "\n",
    "## Stochastic Weight Averaging (SWA)\n",
    "\n",
    "### SWA Implementation\n",
    "\n",
    "```python\n",
    "class SWACallback(Callback):\n",
    "    \"\"\"Stochastic Weight Averaging callback\"\"\"\n",
    "    \n",
    "    def __init__(self, swa_epoch_start=10, swa_lrs=1e-4, annealing_epochs=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            swa_epoch_start: Epoch to start SWA\n",
    "            swa_lrs: SWA learning rate (or list of LRs for param groups)\n",
    "            annealing_epochs: Number of epochs for learning rate annealing\n",
    "        \"\"\"\n",
    "        self.swa_epoch_start = swa_epoch_start\n",
    "        self.swa_lrs = swa_lrs\n",
    "        self.annealing_epochs = annealing_epochs\n",
    "        self.swa_model = None\n",
    "        self.swa_n = 0\n",
    "        \n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.current_epoch == self.swa_epoch_start:\n",
    "            print(f\"ðŸ”„ Starting SWA at epoch {self.swa_epoch_start}\")\n",
    "            # Initialize SWA model\n",
    "            self.swa_model = copy.deepcopy(pl_module)\n",
    "            self.swa_n = 0\n",
    "            \n",
    "            # Modify optimizer for SWA learning rate\n",
    "            for param_group in trainer.optimizers[0].param_groups:\n",
    "                param_group['lr'] = self.swa_lrs\n",
    "                \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch >= self.swa_epoch_start:\n",
    "            # Update SWA model\n",
    "            self._update_swa_model(pl_module)\n",
    "            self.swa_n += 1\n",
    "            \n",
    "            if trainer.current_epoch % 5 == 0:\n",
    "                print(f\"SWA: Averaged {self.swa_n} models\")\n",
    "    \n",
    "    def _update_swa_model(self, model):\n",
    "        \"\"\"Update SWA model with current model parameters\"\"\"\n",
    "        if self.swa_model is None:\n",
    "            return\n",
    "            \n",
    "        # Running average: swa_param = (swa_param * n + current_param) / (n + 1)\n",
    "        alpha = 1.0 / (self.swa_n + 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for swa_param, current_param in zip(self.swa_model.parameters(), model.parameters()):\n",
    "                swa_param.data = (1 - alpha) * swa_param.data + alpha * current_param.data\n",
    "    \n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        if self.swa_model is not None:\n",
    "            print(\"âœ… SWA completed. Use get_swa_model() to access averaged model.\")\n",
    "    \n",
    "    def get_swa_model(self):\n",
    "        \"\"\"Get the SWA averaged model\"\"\"\n",
    "        return self.swa_model\n",
    "\n",
    "# Advanced SWA with batch normalization update\n",
    "class AdvancedSWACallback(SWACallback):\n",
    "    \"\"\"SWA with batch normalization statistics update\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, update_bn_every=5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.update_bn_every = update_bn_every\n",
    "        \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        super().on_train_epoch_end(trainer, pl_module)\n",
    "        \n",
    "        # Update BN statistics every few epochs\n",
    "        if (trainer.current_epoch >= self.swa_epoch_start and \n",
    "            trainer.current_epoch % self.update_bn_every == 0):\n",
    "            self._update_bn_stats(trainer.train_dataloader, pl_module.device)\n",
    "    \n",
    "    def _update_bn_stats(self, dataloader, device):\n",
    "        \"\"\"Update batch normalization statistics for SWA model\"\"\"\n",
    "        if self.swa_model is None:\n",
    "            return\n",
    "            \n",
    "        print(\"ðŸ”§ Updating BN statistics for SWA model...\")\n",
    "        self.swa_model.train()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(dataloader):\n",
    "                if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
    "                    x, _ = batch\n",
    "                    if isinstance(x, torch.Tensor):\n",
    "                        x = x.to(device)\n",
    "                        self.swa_model(x)\n",
    "                        \n",
    "                # Only use first few batches for BN update\n",
    "                if batch_idx >= 50:\n",
    "                    break\n",
    "        \n",
    "        print(\"âœ… BN statistics updated\")\n",
    "\n",
    "# Test SWA callback\n",
    "print(\"=== Testing SWA Callback ===\")\n",
    "\n",
    "swa_callback = AdvancedSWACallback(\n",
    "    swa_epoch_start=8,\n",
    "    swa_lrs=5e-4,\n",
    "    annealing_epochs=3,\n",
    "    update_bn_every=3\n",
    ")\n",
    "\n",
    "model = CallbackDemoModel(learning_rate=1e-3)\n",
    "monitor_callback = TrainingMonitorCallback()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    callbacks=[swa_callback, monitor_callback],\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False\n",
    ")\n",
    "\n",
    "print(\"Training with SWA...\")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Get SWA model and compare performance\n",
    "swa_model = swa_callback.get_swa_model()\n",
    "if swa_model is not None:\n",
    "    print(\"\\n=== Comparing Regular vs SWA Model ===\")\n",
    "    \n",
    "    # Evaluate both models\n",
    "    trainer.test(model, val_loader, verbose=False)\n",
    "    regular_acc = trainer.callback_metrics.get('val_acc', 0)\n",
    "    \n",
    "    trainer.test(swa_model, val_loader, verbose=False) \n",
    "    swa_acc = trainer.callback_metrics.get('val_acc', 0)\n",
    "    \n",
    "    print(f\"Regular model accuracy: {regular_acc:.4f}\")\n",
    "    print(f\"SWA model accuracy: {swa_acc:.4f}\")\n",
    "    print(f\"Improvement: {swa_acc - regular_acc:.4f}\")\n",
    "\n",
    "print(\"âœ“ SWA demo completed\")\n",
    "```\n",
    "\n",
    "## Exponential Moving Average (EMA)\n",
    "\n",
    "### EMA Implementation\n",
    "\n",
    "```python\n",
    "class EMACallback(Callback):\n",
    "    \"\"\"Exponential Moving Average callback\"\"\"\n",
    "    \n",
    "    def __init__(self, decay=0.999, update_every=1, start_epoch=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            decay: EMA decay rate (higher = more averaging)\n",
    "            update_every: Update EMA every N steps\n",
    "            start_epoch: Epoch to start EMA\n",
    "        \"\"\"\n",
    "        self.decay = decay\n",
    "        self.update_every = update_every\n",
    "        self.start_epoch = start_epoch\n",
    "        self.ema_model = None\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        # Initialize EMA model\n",
    "        self.ema_model = copy.deepcopy(pl_module)\n",
    "        # Disable gradients for EMA model\n",
    "        for param in self.ema_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(f\"ðŸ”„ EMA initialized with decay={self.decay}\")\n",
    "        \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if trainer.current_epoch >= self.start_epoch:\n",
    "            self.step_count += 1\n",
    "            \n",
    "            if self.step_count % self.update_every == 0:\n",
    "                self._update_ema_model(pl_module)\n",
    "    \n",
    "    def _update_ema_model(self, model):\n",
    "        \"\"\"Update EMA model parameters\"\"\"\n",
    "        if self.ema_model is None:\n",
    "            return\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for ema_param, current_param in zip(self.ema_model.parameters(), model.parameters()):\n",
    "                ema_param.data = self.decay * ema_param.data + (1 - self.decay) * current_param.data\n",
    "    \n",
    "    def on_validation_epoch_start(self, trainer, pl_module):\n",
    "        # Log EMA model performance\n",
    "        if self.ema_model is not None and trainer.current_epoch >= self.start_epoch:\n",
    "            self._evaluate_ema_model(trainer, pl_module)\n",
    "    \n",
    "    def _evaluate_ema_model(self, trainer, pl_module):\n",
    "        \"\"\"Evaluate EMA model and log metrics\"\"\"\n",
    "        if trainer.val_dataloaders is None:\n",
    "            return\n",
    "            \n",
    "        self.ema_model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in trainer.val_dataloaders[0]:\n",
    "                if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
    "                    x, y = batch\n",
    "                    x, y = x.to(pl_module.device), y.to(pl_module.device)\n",
    "                    \n",
    "                    logits = self.ema_model(x)\n",
    "                    loss = F.cross_entropy(logits, y)\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    correct += (preds == y).sum().item()\n",
    "                    total += y.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(trainer.val_dataloaders[0])\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        # Log EMA metrics\n",
    "        pl_module.log('ema_val_loss', avg_loss, on_epoch=True)\n",
    "        pl_module.log('ema_val_acc', accuracy, on_epoch=True)\n",
    "    \n",
    "    def get_ema_model(self):\n",
    "        \"\"\"Get the EMA model\"\"\"\n",
    "        return self.ema_model\n",
    "\n",
    "class AdaptiveEMACallback(EMACallback):\n",
    "    \"\"\"EMA with adaptive decay based on training progress\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_decay=0.999, min_decay=0.99, decay_schedule='linear', **kwargs):\n",
    "        super().__init__(decay=initial_decay, **kwargs)\n",
    "        self.initial_decay = initial_decay\n",
    "        self.min_decay = min_decay\n",
    "        self.decay_schedule = decay_schedule\n",
    "        \n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        # Adapt decay rate based on training progress\n",
    "        if trainer.max_epochs > 0:\n",
    "            progress = trainer.current_epoch / trainer.max_epochs\n",
    "            \n",
    "            if self.decay_schedule == 'linear':\n",
    "                self.decay = self.initial_decay - progress * (self.initial_decay - self.min_decay)\n",
    "            elif self.decay_schedule == 'cosine':\n",
    "                self.decay = self.min_decay + 0.5 * (self.initial_decay - self.min_decay) * (1 + np.cos(np.pi * progress))\n",
    "            \n",
    "            # Clamp decay to reasonable bounds\n",
    "            self.decay = max(self.min_decay, min(self.initial_decay, self.decay))\n",
    "\n",
    "# Test EMA callback\n",
    "print(\"\\n=== Testing EMA Callback ===\")\n",
    "\n",
    "ema_callback = AdaptiveEMACallback(\n",
    "    initial_decay=0.999,\n",
    "    min_decay=0.99,\n",
    "    decay_schedule='cosine',\n",
    "    update_every=2,\n",
    "    start_epoch=2\n",
    ")\n",
    "\n",
    "model = CallbackDemoModel(learning_rate=1e-3)\n",
    "monitor_callback = TrainingMonitorCallback()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=12,\n",
    "    callbacks=[ema_callback, monitor_callback],\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False\n",
    ")\n",
    "\n",
    "print(\"Training with EMA...\")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Get EMA model\n",
    "ema_model = ema_callback.get_ema_model()\n",
    "print(\"âœ“ EMA training completed\")\n",
    "```\n",
    "\n",
    "## Combined SWA and EMA\n",
    "\n",
    "### Hybrid Approach\n",
    "\n",
    "```python\n",
    "class HybridSWAEMACallback(Callback):\n",
    "    \"\"\"Combines SWA and EMA for robust model averaging\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        ema_decay=0.999,\n",
    "        swa_start_epoch=10,\n",
    "        swa_lr=1e-4,\n",
    "        warmup_epochs=3\n",
    "    ):\n",
    "        self.ema_decay = ema_decay\n",
    "        self.swa_start_epoch = swa_start_epoch\n",
    "        self.swa_lr = swa_lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        \n",
    "        # Models\n",
    "        self.ema_model = None\n",
    "        self.swa_model = None\n",
    "        self.swa_n = 0\n",
    "        \n",
    "        # Tracking\n",
    "        self.ema_metrics = []\n",
    "        self.swa_metrics = []\n",
    "        \n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        # Initialize EMA model\n",
    "        self.ema_model = copy.deepcopy(pl_module)\n",
    "        for param in self.ema_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"ðŸ”„ Hybrid SWA+EMA initialized\")\n",
    "        \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        # Update EMA every step after warmup\n",
    "        if trainer.current_epoch >= self.warmup_epochs:\n",
    "            self._update_ema(pl_module)\n",
    "            \n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        # Start SWA at specified epoch\n",
    "        if trainer.current_epoch == self.swa_start_epoch:\n",
    "            print(f\"ðŸ”„ Starting SWA phase at epoch {self.swa_start_epoch}\")\n",
    "            self.swa_model = copy.deepcopy(pl_module)\n",
    "            self.swa_n = 0\n",
    "            \n",
    "            # Reduce learning rate for SWA\n",
    "            for param_group in trainer.optimizers[0].param_groups:\n",
    "                param_group['lr'] = self.swa_lr\n",
    "                \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        # Update SWA if in SWA phase\n",
    "        if trainer.current_epoch >= self.swa_start_epoch and self.swa_model is not None:\n",
    "            self._update_swa(pl_module)\n",
    "            self.swa_n += 1\n",
    "            \n",
    "    def _update_ema(self, model):\n",
    "        \"\"\"Update EMA model\"\"\"\n",
    "        if self.ema_model is None:\n",
    "            return\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for ema_param, current_param in zip(self.ema_model.parameters(), model.parameters()):\n",
    "                ema_param.data = (\n",
    "                    self.ema_decay * ema_param.data + \n",
    "                    (1 - self.ema_decay) * current_param.data\n",
    "                )\n",
    "    \n",
    "    def _update_swa(self, model):\n",
    "        \"\"\"Update SWA model\"\"\"\n",
    "        if self.swa_model is None:\n",
    "            return\n",
    "            \n",
    "        alpha = 1.0 / (self.swa_n + 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for swa_param, current_param in zip(self.swa_model.parameters(), model.parameters()):\n",
    "                swa_param.data = (1 - alpha) * swa_param.data + alpha * current_param.data\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        \"\"\"Evaluate both EMA and SWA models\"\"\"\n",
    "        val_loss = trainer.callback_metrics.get('val_loss', 0)\n",
    "        val_acc = trainer.callback_metrics.get('val_acc', 0)\n",
    "        \n",
    "        # Evaluate EMA model\n",
    "        if self.ema_model is not None and trainer.current_epoch >= self.warmup_epochs:\n",
    "            ema_loss, ema_acc = self._evaluate_model(self.ema_model, trainer.val_dataloaders[0], pl_module.device)\n",
    "            pl_module.log('ema_val_loss', ema_loss)\n",
    "            pl_module.log('ema_val_acc', ema_acc)\n",
    "            self.ema_metrics.append(ema_acc)\n",
    "        \n",
    "        # Evaluate SWA model\n",
    "        if self.swa_model is not None and trainer.current_epoch >= self.swa_start_epoch:\n",
    "            swa_loss, swa_acc = self._evaluate_model(self.swa_model, trainer.val_dataloaders[0], pl_module.device)\n",
    "            pl_module.log('swa_val_loss', swa_loss)\n",
    "            pl_module.log('swa_val_acc', swa_acc)\n",
    "            self.swa_metrics.append(swa_acc)\n",
    "        \n",
    "        # Log comparison metrics\n",
    "        if len(self.ema_metrics) > 0 and len(self.swa_metrics) > 0:\n",
    "            ema_improvement = self.ema_metrics[-1] - val_acc\n",
    "            swa_improvement = self.swa_metrics[-1] - val_acc\n",
    "            pl_module.log('ema_improvement', ema_improvement)\n",
    "            pl_module.log('swa_improvement', swa_improvement)\n",
    "    \n",
    "    def _evaluate_model(self, model, dataloader, device):\n",
    "        \"\"\"Evaluate a model on validation data\"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
    "                    x, y = batch\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    \n",
    "                    logits = model(x)\n",
    "                    loss = F.cross_entropy(logits, y)\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    correct += (preds == y).sum().item()\n",
    "                    total += y.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = correct / total\n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        \"\"\"Return the best performing model (EMA, SWA, or original)\"\"\"\n",
    "        if not self.ema_metrics and not self.swa_metrics:\n",
    "            return None, \"original\"\n",
    "        \n",
    "        best_ema = max(self.ema_metrics) if self.ema_metrics else 0\n",
    "        best_swa = max(self.swa_metrics) if self.swa_metrics else 0\n",
    "        \n",
    "        if best_ema > best_swa:\n",
    "            return self.ema_model, \"ema\"\n",
    "        else:\n",
    "            return self.swa_model, \"swa\"\n",
    "\n",
    "# Test hybrid approach\n",
    "print(\"\\n=== Testing Hybrid SWA+EMA ===\")\n",
    "\n",
    "hybrid_callback = HybridSWAEMACallback(\n",
    "    ema_decay=0.999,\n",
    "    swa_start_epoch=8,\n",
    "    swa_lr=5e-4,\n",
    "    warmup_epochs=2\n",
    ")\n",
    "\n",
    "model = CallbackDemoModel(learning_rate=1e-3)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    callbacks=[hybrid_callback],\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False\n",
    ")\n",
    "\n",
    "print(\"Training with hybrid SWA+EMA...\")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Compare all models\n",
    "print(\"\\n=== Final Model Comparison ===\")\n",
    "best_model, best_type = hybrid_callback.get_best_model()\n",
    "\n",
    "# Evaluate original model\n",
    "trainer.test(model, val_loader, verbose=False)\n",
    "original_acc = trainer.callback_metrics.get('val_acc', 0)\n",
    "\n",
    "print(f\"Original model accuracy: {original_acc:.4f}\")\n",
    "print(f\"Best EMA accuracy: {max(hybrid_callback.ema_metrics) if hybrid_callback.ema_metrics else 0:.4f}\")\n",
    "print(f\"Best SWA accuracy: {max(hybrid_callback.swa_metrics) if hybrid_callback.swa_metrics else 0:.4f}\")\n",
    "print(f\"Best overall model: {best_type}\")\n",
    "\n",
    "print(\"âœ“ Hybrid approach demo completed\")\n",
    "```\n",
    "\n",
    "## Production-Ready Callback Suite\n",
    "\n",
    "### Complete Custom Callback System\n",
    "\n",
    "```python\n",
    "class ProductionCallbackSuite:\n",
    "    \"\"\"Complete callback suite for production training\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_callbacks(config):\n",
    "        \"\"\"Get production callbacks based on configuration\"\"\"\n",
    "        callbacks = []\n",
    "        \n",
    "        # Model averaging\n",
    "        if config.get('use_ema', True):\n",
    "            callbacks.append(EMACallback(\n",
    "                decay=config.get('ema_decay', 0.999),\n",
    "                update_every=config.get('ema_update_every', 1),\n",
    "                start_epoch=config.get('ema_start_epoch', 0)\n",
    "            ))\n",
    "        \n",
    "        if config.get('use_swa', True):\n",
    "            callbacks.append(AdvancedSWACallback(\n",
    "                swa_epoch_start=config.get('swa_start_epoch', 10),\n",
    "                swa_lrs=config.get('swa_lr', 1e-4),\n",
    "                annealing_epochs=config.get('swa_annealing', 5),\n",
    "                update_bn_every=config.get('swa_bn_update_every', 3)\n",
    "            ))\n",
    "        \n",
    "        # Monitoring\n",
    "        callbacks.append(TrainingMonitorCallback())\n",
    "        \n",
    "        return callbacks\n",
    "\n",
    "# Configuration for production training\n",
    "production_config = {\n",
    "    'use_ema': True,\n",
    "    'ema_decay': 0.9995,\n",
    "    'ema_update_every': 1,\n",
    "    'ema_start_epoch': 5,\n",
    "    \n",
    "    'use_swa': True,\n",
    "    'swa_start_epoch': 15,\n",
    "    'swa_lr': 1e-4,\n",
    "    'swa_annealing': 3,\n",
    "    'swa_bn_update_every': 5\n",
    "}\n",
    "\n",
    "# Get production callbacks\n",
    "production_callbacks = ProductionCallbackSuite.get_callbacks(production_config)\n",
    "\n",
    "print(\"=== Production Training with Full Callback Suite ===\")\n",
    "\n",
    "model = CallbackDemoModel(learning_rate=1e-3, hidden_size=256)  # Larger model\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=25,\n",
    "    callbacks=production_callbacks,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "print(\"âœ“ Production training completed with full callback suite\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered advanced training techniques through custom callbacks:\n",
    "\n",
    "1. **Custom Callback Basics**: Understanding callback lifecycle and building monitoring callbacks\n",
    "2. **Stochastic Weight Averaging (SWA)**: Averaging model weights from multiple epochs for better generalization\n",
    "3. **Exponential Moving Average (EMA)**: Maintaining running averages of model parameters during training\n",
    "4. **Advanced Implementations**: Adaptive decay rates, batch normalization updates, and hybrid approaches\n",
    "5. **Production Integration**: Complete callback suites for real-world training pipelines\n",
    "\n",
    "Key benefits of model averaging:\n",
    "- **Better Generalization**: Averaged models often perform better than individual checkpoints\n",
    "- **Stability**: Reduced variance in model predictions\n",
    "- **No Inference Cost**: Single model at inference time with ensemble-like benefits\n",
    "- **Robustness**: Less sensitive to learning rate and optimization noise\n",
    "\n",
    "SWA vs EMA comparison:\n",
    "- **SWA**: Averages models from different epochs, requires learning rate scheduling\n",
    "- **EMA**: Maintains running average throughout training, more computationally efficient\n",
    "- **Hybrid**: Combines both approaches for maximum robustness\n",
    "\n",
    "Best practices:\n",
    "- Start model averaging after initial training phase (warmup)\n",
    "- Use lower learning rates during SWA phase\n",
    "- Update batch normalization statistics for SWA models\n",
    "- Monitor both original and averaged model performance\n",
    "- Choose appropriate decay rates for EMA (0.995-0.9999 typically)\n",
    "\n",
    "Production considerations:\n",
    "- Save both original and averaged models\n",
    "- Implement proper callback configuration management\n",
    "- Monitor training dynamics and gradient norms\n",
    "- Use callbacks for automated hyperparameter adjustment\n",
    "- Integrate with experiment tracking and logging systems\n",
    "\n",
    "Next notebook: We'll explore mixed precision training and performance optimization."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
