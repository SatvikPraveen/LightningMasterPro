# configs/nlp/char_lm.yaml
# @package _global_

defaults:
  - /defaults.yaml

seed_everything: 42

model:
  class_path: lmpro.modules.nlp.char_lm.CharacterLM
  init_args:
    vocab_size: 128
    embed_dim: 256
    hidden_dim: 512
    num_layers: 3
    dropout_rate: 0.2
    learning_rate: 1e-3
    weight_decay: 1e-5

data:
  class_path: lmpro.datamodules.nlp_dm.NLPDataModule
  init_args:
    data_dir: data/synthetic/
    dataset_type: char_lm
    batch_size: 64
    num_workers: 4
    pin_memory: true
    sequence_length: 256
    vocab_size: 128

trainer:
  max_epochs: 25
  accelerator: auto
  devices: 1
  precision: 16-mixed
  gradient_clip_val: 1.0
  accumulate_grad_batches: 2

callbacks:
  - class_path: lightning.pytorch.callbacks.ModelCheckpoint
    init_args:
      dirpath: checkpoints/nlp/char_lm/
      filename: "char_lm-{epoch:02d}-{val_loss:.3f}"
      monitor: val_loss
      mode: min
      save_top_k: 3
      save_last: true

  - class_path: lightning.pytorch.callbacks.EarlyStopping
    init_args:
      monitor: val_loss
      mode: min
      patience: 8

  - class_path: lightning.pytorch.callbacks.LearningRateMonitor
    init_args:
      logging_interval: step

logger:
  - class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir: logs/
      name: nlp_char_lm
      version: null
